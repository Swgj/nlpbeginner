{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE.type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "BATCH_SIZE=10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_all = pd.read_csv(\"../data/sentiment-analysis-on-movie-reviews/train.tsv\",delimiter='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_data,test_data = train_test_split(data_all,test_size=0.2,random_state=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/task2/train.csv',index=False)\n",
    "test_data.to_csv('../data/task2/test.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "PAD_TOKEN='<pad>'\n",
    "TEXT = data.Field(sequential=True,batch_first=True, lower=True, pad_token=PAD_TOKEN)\n",
    "LABEL = data.Field(sequential=False, batch_first=True, unk_token=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "datafields = [(\"PhraseId\", None), # 不需要的filed设置为None\n",
    "              (\"SentenceId\", None),\n",
    "              ('Phrase', TEXT),\n",
    "              ('Sentiment', LABEL)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_data = data.TabularDataset(path='../data/task2/train.csv',format='csv',fields=datafields)\n",
    "test_data = data.TabularDataset(path='../data/task2/test.csv',format='csv',fields=datafields)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#构建词典，字符映射到embedding\n",
    "#TEXT.vocab.vectors 就是词向量\n",
    "TEXT.build_vocab(train_data,  vectors= 'glove.6B.50d',\n",
    "                 unk_init= lambda x:torch.nn.init.uniform_(x, a=-0.25, b=0.25))\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "\n",
    "#得到索引，PAD_TOKEN='<pad>'\n",
    "PAD_INDEX = TEXT.vocab.stoi[PAD_TOKEN]\n",
    "TEXT.vocab.vectors[PAD_INDEX] = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#构建迭代器\n",
    "train_iterator = data.BucketIterator(train_data, batch_size=BATCH_SIZE,train=True, shuffle=True,device=DEVICE)\n",
    "\n",
    "test_iterator = data.Iterator(test_data, batch_size=len(test_data),train=False,sort=False, device=DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16517 6\n"
     ]
    }
   ],
   "source": [
    "#部分参数设置\n",
    "embedding_choice='glove'   #  'static'    'non-static'\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "embedding_dim =50\n",
    "dropout_p=0.5\n",
    "filters_num=100\n",
    "\n",
    "vocab_size=len(TEXT.vocab)\n",
    "label_num=len(LABEL.vocab)\n",
    "print(vocab_size,label_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embedding_choice=embedding_choice\n",
    "\n",
    "        if self.embedding_choice==  'rand':\n",
    "            self.embedding=nn.Embedding(num_embeddings,embedding_dim)\n",
    "        if self.embedding_choice==  'glove':\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim,\n",
    "                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(3, embedding_dim), padding=(2,0))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(4, embedding_dim), padding=(3,0))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(5, embedding_dim), padding=(4,0))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc = nn.Linear(filters_num * 3, label_num)\n",
    "\n",
    "    def forward(self,x):      # (Batch_size, Length)\n",
    "        x=self.embedding(x).unsqueeze(1)      #(Batch_size, Length, Dimention)\n",
    "                                       #(Batch_size, 1, Length, Dimention)\n",
    "\n",
    "        x1 = F.relu(self.conv1(x)).squeeze(3)    #(Batch_size, filters_num, length+padding, 1)\n",
    "                                          #(Batch_size, filters_num, length+padding)\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)  #(Batch_size, filters_num, 1)\n",
    "                                               #(Batch_size, filters_num)\n",
    "\n",
    "        x2 = F.relu(self.conv2(x)).squeeze(3)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "\n",
    "        x3 = F.relu(self.conv3(x)).squeeze(3)\n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)  #(Batch_size, filters_num *3 )\n",
    "        x = self.dropout(x)      #(Batch_size, filters_num *3 )\n",
    "        out = self.fc(x)       #(Batch_size, label_num  )\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#构建模型\n",
    "\n",
    "model=CNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#创建优化器SGD\n",
    "criterion = nn.CrossEntropyLoss()   #损失函数\n",
    "\n",
    "if DEVICE.type=='cuda':\n",
    "    model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_0.801%:  Training average Loss: 1.440895\n",
      "Epoch 0_1.602%:  Training average Loss: 1.365857\n",
      "Epoch 0_2.403%:  Training average Loss: 1.304160\n",
      "Epoch 0_3.204%:  Training average Loss: 1.275250\n",
      "Epoch 0_4.005%:  Training average Loss: 1.247017\n",
      "Epoch 0_4.806%:  Training average Loss: 1.227093\n",
      "Epoch 0_5.607%:  Training average Loss: 1.216458\n",
      "Epoch 0_6.408%:  Training average Loss: 1.202484\n",
      "Epoch 0_7.209%:  Training average Loss: 1.194820\n",
      "Epoch 0_8.010%:  Training average Loss: 1.186466\n",
      "Epoch 0_8.811%:  Training average Loss: 1.176250\n",
      "Epoch 0_9.612%:  Training average Loss: 1.171777\n",
      "Epoch 0_10.413%:  Training average Loss: 1.164574\n",
      "Epoch 0_11.214%:  Training average Loss: 1.159322\n",
      "Epoch 0_12.015%:  Training average Loss: 1.156394\n",
      "Epoch 0_12.815%:  Training average Loss: 1.151346\n",
      "Epoch 0_13.616%:  Training average Loss: 1.146390\n",
      "Epoch 0_14.417%:  Training average Loss: 1.143681\n",
      "Epoch 0_15.218%:  Training average Loss: 1.140035\n",
      "Epoch 0_16.019%:  Training average Loss: 1.133156\n",
      "Epoch 0_16.820%:  Training average Loss: 1.131494\n",
      "Epoch 0_17.621%:  Training average Loss: 1.128796\n",
      "Epoch 0_18.422%:  Training average Loss: 1.126961\n",
      "Epoch 0_19.223%:  Training average Loss: 1.124063\n",
      "Epoch 0_20.024%:  Training average Loss: 1.122995\n",
      "Epoch 0_20.825%:  Training average Loss: 1.118478\n",
      "Epoch 0_21.626%:  Training average Loss: 1.116706\n",
      "Epoch 0_22.427%:  Training average Loss: 1.114220\n",
      "Epoch 0_23.228%:  Training average Loss: 1.113026\n",
      "Epoch 0_24.029%:  Training average Loss: 1.111335\n",
      "Epoch 0_24.830%:  Training average Loss: 1.108718\n",
      "Epoch 0_25.631%:  Training average Loss: 1.107410\n",
      "Epoch 0_26.432%:  Training average Loss: 1.105228\n",
      "Epoch 0_27.233%:  Training average Loss: 1.104047\n",
      "Epoch 0_28.034%:  Training average Loss: 1.102940\n",
      "Epoch 0_28.835%:  Training average Loss: 1.101166\n",
      "Epoch 0_29.636%:  Training average Loss: 1.099556\n",
      "Epoch 0_30.437%:  Training average Loss: 1.098160\n",
      "Epoch 0_31.238%:  Training average Loss: 1.096090\n",
      "Epoch 0_32.039%:  Training average Loss: 1.095749\n",
      "Epoch 0_32.840%:  Training average Loss: 1.093888\n",
      "Epoch 0_33.641%:  Training average Loss: 1.093538\n",
      "Epoch 0_34.442%:  Training average Loss: 1.091048\n",
      "Epoch 0_35.243%:  Training average Loss: 1.090706\n",
      "Epoch 0_36.044%:  Training average Loss: 1.090230\n",
      "Epoch 0_36.845%:  Training average Loss: 1.089942\n",
      "Epoch 0_37.645%:  Training average Loss: 1.088804\n",
      "Epoch 0_38.446%:  Training average Loss: 1.087336\n",
      "Epoch 0_39.247%:  Training average Loss: 1.086040\n",
      "Epoch 0_40.048%:  Training average Loss: 1.085360\n",
      "Epoch 0_40.849%:  Training average Loss: 1.084262\n",
      "Epoch 0_41.650%:  Training average Loss: 1.084124\n",
      "Epoch 0_42.451%:  Training average Loss: 1.082876\n",
      "Epoch 0_43.252%:  Training average Loss: 1.082097\n",
      "Epoch 0_44.053%:  Training average Loss: 1.082030\n",
      "Epoch 0_44.854%:  Training average Loss: 1.081830\n",
      "Epoch 0_45.655%:  Training average Loss: 1.080676\n",
      "Epoch 0_46.456%:  Training average Loss: 1.079783\n",
      "Epoch 0_47.257%:  Training average Loss: 1.078722\n",
      "Epoch 0_48.058%:  Training average Loss: 1.077840\n",
      "Epoch 0_48.859%:  Training average Loss: 1.076496\n",
      "Epoch 0_49.660%:  Training average Loss: 1.076002\n",
      "Epoch 0_50.461%:  Training average Loss: 1.075731\n",
      "Epoch 0_51.262%:  Training average Loss: 1.074898\n",
      "Epoch 0_52.063%:  Training average Loss: 1.074643\n",
      "Epoch 0_52.864%:  Training average Loss: 1.074429\n",
      "Epoch 0_53.665%:  Training average Loss: 1.074184\n",
      "Epoch 0_54.466%:  Training average Loss: 1.073207\n",
      "Epoch 0_55.267%:  Training average Loss: 1.073301\n",
      "Epoch 0_56.068%:  Training average Loss: 1.072363\n",
      "Epoch 0_56.869%:  Training average Loss: 1.072281\n",
      "Epoch 0_57.670%:  Training average Loss: 1.071705\n",
      "Epoch 0_58.471%:  Training average Loss: 1.071411\n",
      "Epoch 0_59.272%:  Training average Loss: 1.070965\n",
      "Epoch 0_60.073%:  Training average Loss: 1.070946\n",
      "Epoch 0_60.874%:  Training average Loss: 1.071370\n",
      "Epoch 0_61.675%:  Training average Loss: 1.071122\n",
      "Epoch 0_62.475%:  Training average Loss: 1.070506\n",
      "Epoch 0_63.276%:  Training average Loss: 1.069969\n",
      "Epoch 0_64.077%:  Training average Loss: 1.070602\n",
      "Epoch 0_64.878%:  Training average Loss: 1.070311\n",
      "Epoch 0_65.679%:  Training average Loss: 1.069944\n",
      "Epoch 0_66.480%:  Training average Loss: 1.069568\n",
      "Epoch 0_67.281%:  Training average Loss: 1.068610\n",
      "Epoch 0_68.082%:  Training average Loss: 1.068040\n",
      "Epoch 0_68.883%:  Training average Loss: 1.067295\n",
      "Epoch 0_69.684%:  Training average Loss: 1.067203\n",
      "Epoch 0_70.485%:  Training average Loss: 1.066762\n",
      "Epoch 0_71.286%:  Training average Loss: 1.066480\n",
      "Epoch 0_72.087%:  Training average Loss: 1.066288\n",
      "Epoch 0_72.888%:  Training average Loss: 1.065926\n",
      "Epoch 0_73.689%:  Training average Loss: 1.066043\n",
      "Epoch 0_74.490%:  Training average Loss: 1.066025\n",
      "Epoch 0_75.291%:  Training average Loss: 1.065984\n",
      "Epoch 0_76.092%:  Training average Loss: 1.066003\n",
      "Epoch 0_76.893%:  Training average Loss: 1.065660\n",
      "Epoch 0_77.694%:  Training average Loss: 1.065346\n",
      "Epoch 0_78.495%:  Training average Loss: 1.065037\n",
      "Epoch 0_79.296%:  Training average Loss: 1.064618\n",
      "Epoch 0_80.097%:  Training average Loss: 1.064217\n",
      "Epoch 0_80.898%:  Training average Loss: 1.063874\n",
      "Epoch 0_81.699%:  Training average Loss: 1.063540\n",
      "Epoch 0_82.500%:  Training average Loss: 1.063036\n",
      "Epoch 0_83.301%:  Training average Loss: 1.062413\n",
      "Epoch 0_84.102%:  Training average Loss: 1.062031\n",
      "Epoch 0_84.903%:  Training average Loss: 1.062140\n",
      "Epoch 0_85.704%:  Training average Loss: 1.061313\n",
      "Epoch 0_86.504%:  Training average Loss: 1.061039\n",
      "Epoch 0_87.305%:  Training average Loss: 1.060984\n",
      "Epoch 0_88.106%:  Training average Loss: 1.060639\n",
      "Epoch 0_88.907%:  Training average Loss: 1.060306\n",
      "Epoch 0_89.708%:  Training average Loss: 1.059878\n",
      "Epoch 0_90.509%:  Training average Loss: 1.059762\n",
      "Epoch 0_91.310%:  Training average Loss: 1.059336\n",
      "Epoch 0_92.111%:  Training average Loss: 1.058960\n",
      "Epoch 0_92.912%:  Training average Loss: 1.058459\n",
      "Epoch 0_93.713%:  Training average Loss: 1.057938\n",
      "Epoch 0_94.514%:  Training average Loss: 1.057662\n",
      "Epoch 0_95.315%:  Training average Loss: 1.057665\n",
      "Epoch 0_96.116%:  Training average Loss: 1.057418\n",
      "Epoch 0_96.917%:  Training average Loss: 1.057089\n",
      "Epoch 0_97.718%:  Training average Loss: 1.056943\n",
      "Epoch 0_98.519%:  Training average Loss: 1.056325\n",
      "Epoch 0_99.320%:  Training average Loss: 1.056350\n",
      "Epoch 0 :  Verification average Loss: 0.986733, Verification accuracy: 59.414347%,Total Time:78.289264\n",
      "Model is saved in model_dict/model_glove/epoch_0_accuracy_0.594143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\torch102\\lib\\site-packages\\torch\\serialization.py:401: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "import time\n",
    "epoch=100\n",
    "best_accuracy=0.0\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    #训练\n",
    "    for batch in train_iterator:\n",
    "        steps+=1\n",
    "        #print(steps)\n",
    "        optimizer.zero_grad() #  梯度缓存清零\n",
    "\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)    #[batch_size, label_num]\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1]  #get the indices\n",
    "                   .view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f\"\n",
    "                      %(i, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps))\n",
    "\n",
    "    #每个epoch都验证一下\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(test_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    for batch in test_iterator:\n",
    "        steps+=1\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "          %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))\n",
    "\n",
    "        if best_accuracy < total_correct/total_data_num :\n",
    "            best_accuracy =total_correct/total_data_num\n",
    "            torch.save(model,'model_dict/model_glove/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            print('Model is saved in model_dict/model_glove/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            #torch.cuda.empty_cache()\n",
    "    break #运行时去除break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16517 6\n"
     ]
    }
   ],
   "source": [
    "embedding_choice='glove'   #  'static'    'non-static'\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "embedding_dim =50\n",
    "dropout_p=0.5\n",
    "hidden_size=50  #隐藏单元数\n",
    "num_layers=2  #层数\n",
    "\n",
    "vocab_size=len(TEXT.vocab)\n",
    "label_num=len(LABEL.vocab)\n",
    "print(vocab_size,label_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "\n",
    "        self.embedding_choice=embedding_choice\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "\n",
    "        if self.embedding_choice==  'rand':\n",
    "            self.embedding=nn.Embedding(num_embeddings,embedding_dim)\n",
    "        if self.embedding_choice==  'glove':\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim,\n",
    "                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "        #input_size (输入的特征维度),hidden_size ,num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
    "                            batch_first=True,dropout=dropout_p,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size * 2, label_num)  # 2 for bidirection\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):      # (Batch_size, Length)\n",
    "        # Set initial hidden and cell states\n",
    "        # h_n (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "        # c_n (num_layers * num_directions, batch, hidden_size):\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "\n",
    "        if DEVICE.type=='cuda':\n",
    "            h0=h0.cuda()\n",
    "            c0=c0.cuda()\n",
    "\n",
    "        x=self.embedding(x)     #(Batch_size, Length)\n",
    "                                       #(Batch_size,  Length, Dimention)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))   #(Batch_size, Length，Dimention)\n",
    "                                        # (batch_size, Length, hidden_size)\n",
    "        out=self.dropout(out)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])   # (batch_size, Length, hidden_size)\n",
    "                           # (batch_size, label_num)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#构建模型\n",
    "\n",
    "model=LSTM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#创建优化器SGD\n",
    "criterion = nn.CrossEntropyLoss()   #损失函数\n",
    "\n",
    "if DEVICE.type=='cuda':\n",
    "    model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_0.801%:  Training average Loss: 0.975373\n",
      "Epoch 0_1.602%:  Training average Loss: 1.001408\n",
      "Epoch 0_2.403%:  Training average Loss: 1.004385\n",
      "Epoch 0_3.204%:  Training average Loss: 0.989824\n",
      "Epoch 0_4.005%:  Training average Loss: 0.994228\n",
      "Epoch 0_4.806%:  Training average Loss: 0.997428\n",
      "Epoch 0_5.607%:  Training average Loss: 0.999208\n",
      "Epoch 0_6.408%:  Training average Loss: 0.997163\n",
      "Epoch 0_7.209%:  Training average Loss: 0.993184\n",
      "Epoch 0_8.010%:  Training average Loss: 0.991081\n",
      "Epoch 0_8.811%:  Training average Loss: 0.990124\n",
      "Epoch 0_9.612%:  Training average Loss: 0.993935\n",
      "Epoch 0_10.413%:  Training average Loss: 0.992488\n",
      "Epoch 0_11.214%:  Training average Loss: 0.992907\n",
      "Epoch 0_12.015%:  Training average Loss: 0.991923\n",
      "Epoch 0_12.815%:  Training average Loss: 0.991285\n",
      "Epoch 0_13.616%:  Training average Loss: 0.988973\n",
      "Epoch 0_14.417%:  Training average Loss: 0.989150\n",
      "Epoch 0_15.218%:  Training average Loss: 0.987931\n",
      "Epoch 0_16.019%:  Training average Loss: 0.989645\n",
      "Epoch 0_16.820%:  Training average Loss: 0.989928\n",
      "Epoch 0_17.621%:  Training average Loss: 0.991313\n",
      "Epoch 0_18.422%:  Training average Loss: 0.990914\n",
      "Epoch 0_19.223%:  Training average Loss: 0.991058\n",
      "Epoch 0_20.024%:  Training average Loss: 0.991252\n",
      "Epoch 0_20.825%:  Training average Loss: 0.990510\n",
      "Epoch 0_21.626%:  Training average Loss: 0.989137\n",
      "Epoch 0_22.427%:  Training average Loss: 0.988853\n",
      "Epoch 0_23.228%:  Training average Loss: 0.989830\n",
      "Epoch 0_24.029%:  Training average Loss: 0.988696\n",
      "Epoch 0_24.830%:  Training average Loss: 0.988158\n",
      "Epoch 0_25.631%:  Training average Loss: 0.988331\n",
      "Epoch 0_26.432%:  Training average Loss: 0.988004\n",
      "Epoch 0_27.233%:  Training average Loss: 0.988156\n",
      "Epoch 0_28.034%:  Training average Loss: 0.987549\n",
      "Epoch 0_28.835%:  Training average Loss: 0.987953\n",
      "Epoch 0_29.636%:  Training average Loss: 0.987777\n",
      "Epoch 0_30.437%:  Training average Loss: 0.986972\n",
      "Epoch 0_31.238%:  Training average Loss: 0.987050\n",
      "Epoch 0_32.039%:  Training average Loss: 0.987871\n",
      "Epoch 0_32.840%:  Training average Loss: 0.988671\n",
      "Epoch 0_33.641%:  Training average Loss: 0.988316\n",
      "Epoch 0_34.442%:  Training average Loss: 0.987012\n",
      "Epoch 0_35.243%:  Training average Loss: 0.986618\n",
      "Epoch 0_36.044%:  Training average Loss: 0.987308\n",
      "Epoch 0_36.845%:  Training average Loss: 0.987472\n",
      "Epoch 0_37.645%:  Training average Loss: 0.987245\n",
      "Epoch 0_38.446%:  Training average Loss: 0.987144\n",
      "Epoch 0_39.247%:  Training average Loss: 0.986059\n",
      "Epoch 0_40.048%:  Training average Loss: 0.987058\n",
      "Epoch 0_40.849%:  Training average Loss: 0.987565\n",
      "Epoch 0_41.650%:  Training average Loss: 0.987631\n",
      "Epoch 0_42.451%:  Training average Loss: 0.987248\n",
      "Epoch 0_43.252%:  Training average Loss: 0.986041\n",
      "Epoch 0_44.053%:  Training average Loss: 0.985998\n",
      "Epoch 0_44.854%:  Training average Loss: 0.985953\n",
      "Epoch 0_45.655%:  Training average Loss: 0.986043\n",
      "Epoch 0_46.456%:  Training average Loss: 0.985999\n",
      "Epoch 0_47.257%:  Training average Loss: 0.986111\n",
      "Epoch 0_48.058%:  Training average Loss: 0.985596\n",
      "Epoch 0_48.859%:  Training average Loss: 0.984818\n",
      "Epoch 0_49.660%:  Training average Loss: 0.984694\n",
      "Epoch 0_50.461%:  Training average Loss: 0.984628\n",
      "Epoch 0_51.262%:  Training average Loss: 0.983705\n",
      "Epoch 0_52.063%:  Training average Loss: 0.983829\n",
      "Epoch 0_52.864%:  Training average Loss: 0.983924\n",
      "Epoch 0_53.665%:  Training average Loss: 0.984189\n",
      "Epoch 0_54.466%:  Training average Loss: 0.983978\n",
      "Epoch 0_55.267%:  Training average Loss: 0.983593\n",
      "Epoch 0_56.068%:  Training average Loss: 0.983194\n",
      "Epoch 0_56.869%:  Training average Loss: 0.982364\n",
      "Epoch 0_57.670%:  Training average Loss: 0.982799\n",
      "Epoch 0_58.471%:  Training average Loss: 0.983481\n",
      "Epoch 0_59.272%:  Training average Loss: 0.983009\n",
      "Epoch 0_60.073%:  Training average Loss: 0.983105\n",
      "Epoch 0_60.874%:  Training average Loss: 0.982941\n",
      "Epoch 0_61.675%:  Training average Loss: 0.982882\n",
      "Epoch 0_62.475%:  Training average Loss: 0.982376\n",
      "Epoch 0_63.276%:  Training average Loss: 0.981840\n",
      "Epoch 0_64.077%:  Training average Loss: 0.981844\n",
      "Epoch 0_64.878%:  Training average Loss: 0.981518\n",
      "Epoch 0_65.679%:  Training average Loss: 0.981536\n",
      "Epoch 0_66.480%:  Training average Loss: 0.981607\n",
      "Epoch 0_67.281%:  Training average Loss: 0.981668\n",
      "Epoch 0_68.082%:  Training average Loss: 0.981351\n",
      "Epoch 0_68.883%:  Training average Loss: 0.981200\n",
      "Epoch 0_69.684%:  Training average Loss: 0.981425\n",
      "Epoch 0_70.485%:  Training average Loss: 0.981472\n",
      "Epoch 0_71.286%:  Training average Loss: 0.981616\n",
      "Epoch 0_72.087%:  Training average Loss: 0.981432\n",
      "Epoch 0_72.888%:  Training average Loss: 0.981488\n",
      "Epoch 0_73.689%:  Training average Loss: 0.980807\n",
      "Epoch 0_74.490%:  Training average Loss: 0.980677\n",
      "Epoch 0_75.291%:  Training average Loss: 0.980936\n",
      "Epoch 0_76.092%:  Training average Loss: 0.980591\n",
      "Epoch 0_76.893%:  Training average Loss: 0.980276\n",
      "Epoch 0_77.694%:  Training average Loss: 0.980200\n",
      "Epoch 0_78.495%:  Training average Loss: 0.979464\n",
      "Epoch 0_79.296%:  Training average Loss: 0.979236\n",
      "Epoch 0_80.097%:  Training average Loss: 0.979184\n",
      "Epoch 0_80.898%:  Training average Loss: 0.979159\n",
      "Epoch 0_81.699%:  Training average Loss: 0.979394\n",
      "Epoch 0_82.500%:  Training average Loss: 0.979221\n",
      "Epoch 0_83.301%:  Training average Loss: 0.979091\n",
      "Epoch 0_84.102%:  Training average Loss: 0.979041\n",
      "Epoch 0_84.903%:  Training average Loss: 0.979195\n",
      "Epoch 0_85.704%:  Training average Loss: 0.978892\n",
      "Epoch 0_86.504%:  Training average Loss: 0.978497\n",
      "Epoch 0_87.305%:  Training average Loss: 0.978739\n",
      "Epoch 0_88.106%:  Training average Loss: 0.978357\n",
      "Epoch 0_88.907%:  Training average Loss: 0.978182\n",
      "Epoch 0_89.708%:  Training average Loss: 0.978184\n",
      "Epoch 0_90.509%:  Training average Loss: 0.977906\n",
      "Epoch 0_91.310%:  Training average Loss: 0.977693\n",
      "Epoch 0_92.111%:  Training average Loss: 0.977684\n",
      "Epoch 0_92.912%:  Training average Loss: 0.977309\n",
      "Epoch 0_93.713%:  Training average Loss: 0.976835\n",
      "Epoch 0_94.514%:  Training average Loss: 0.976459\n",
      "Epoch 0_95.315%:  Training average Loss: 0.975997\n",
      "Epoch 0_96.116%:  Training average Loss: 0.975827\n",
      "Epoch 0_96.917%:  Training average Loss: 0.975573\n",
      "Epoch 0_97.718%:  Training average Loss: 0.975527\n",
      "Epoch 0_98.519%:  Training average Loss: 0.975244\n",
      "Epoch 0_99.320%:  Training average Loss: 0.975600\n",
      "Epoch 0 :  Verification average Loss: 0.941907, Verification accuracy: 61.214878%,Total Time:161.333676\n",
      "Model is saved in model_dict/model_lstm/epoch_0_accuracy_0.612149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\torch102\\lib\\site-packages\\torch\\serialization.py:401: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1_0.801%:  Training average Loss: 0.933823\n",
      "Epoch 1_1.602%:  Training average Loss: 0.934040\n",
      "Epoch 1_2.403%:  Training average Loss: 0.955319\n",
      "Epoch 1_3.204%:  Training average Loss: 0.937806\n",
      "Epoch 1_4.005%:  Training average Loss: 0.928102\n",
      "Epoch 1_4.806%:  Training average Loss: 0.925860\n",
      "Epoch 1_5.607%:  Training average Loss: 0.932861\n",
      "Epoch 1_6.408%:  Training average Loss: 0.937918\n",
      "Epoch 1_7.209%:  Training average Loss: 0.941890\n",
      "Epoch 1_8.010%:  Training average Loss: 0.943666\n",
      "Epoch 1_8.811%:  Training average Loss: 0.936575\n",
      "Epoch 1_9.612%:  Training average Loss: 0.937170\n",
      "Epoch 1_10.413%:  Training average Loss: 0.937787\n",
      "Epoch 1_11.214%:  Training average Loss: 0.938969\n",
      "Epoch 1_12.015%:  Training average Loss: 0.940052\n",
      "Epoch 1_12.815%:  Training average Loss: 0.938778\n",
      "Epoch 1_13.616%:  Training average Loss: 0.940761\n",
      "Epoch 1_14.417%:  Training average Loss: 0.941858\n",
      "Epoch 1_15.218%:  Training average Loss: 0.941621\n",
      "Epoch 1_16.019%:  Training average Loss: 0.943671\n",
      "Epoch 1_16.820%:  Training average Loss: 0.944129\n",
      "Epoch 1_17.621%:  Training average Loss: 0.943621\n",
      "Epoch 1_18.422%:  Training average Loss: 0.944715\n",
      "Epoch 1_19.223%:  Training average Loss: 0.944061\n",
      "Epoch 1_20.024%:  Training average Loss: 0.944632\n",
      "Epoch 1_20.825%:  Training average Loss: 0.944420\n",
      "Epoch 1_21.626%:  Training average Loss: 0.944130\n",
      "Epoch 1_22.427%:  Training average Loss: 0.943569\n",
      "Epoch 1_23.228%:  Training average Loss: 0.941401\n",
      "Epoch 1_24.029%:  Training average Loss: 0.940698\n",
      "Epoch 1_24.830%:  Training average Loss: 0.940589\n",
      "Epoch 1_25.631%:  Training average Loss: 0.942440\n",
      "Epoch 1_26.432%:  Training average Loss: 0.942099\n",
      "Epoch 1_27.233%:  Training average Loss: 0.940563\n",
      "Epoch 1_28.034%:  Training average Loss: 0.939975\n",
      "Epoch 1_28.835%:  Training average Loss: 0.940068\n",
      "Epoch 1_29.636%:  Training average Loss: 0.941098\n",
      "Epoch 1_30.437%:  Training average Loss: 0.940455\n",
      "Epoch 1_31.238%:  Training average Loss: 0.940303\n",
      "Epoch 1_32.039%:  Training average Loss: 0.940354\n",
      "Epoch 1_32.840%:  Training average Loss: 0.940442\n",
      "Epoch 1_33.641%:  Training average Loss: 0.940494\n",
      "Epoch 1_34.442%:  Training average Loss: 0.939894\n",
      "Epoch 1_35.243%:  Training average Loss: 0.939463\n",
      "Epoch 1_36.044%:  Training average Loss: 0.939799\n",
      "Epoch 1_36.845%:  Training average Loss: 0.939673\n",
      "Epoch 1_37.645%:  Training average Loss: 0.939734\n",
      "Epoch 1_38.446%:  Training average Loss: 0.939131\n",
      "Epoch 1_39.247%:  Training average Loss: 0.939303\n",
      "Epoch 1_40.048%:  Training average Loss: 0.938652\n",
      "Epoch 1_40.849%:  Training average Loss: 0.938305\n",
      "Epoch 1_41.650%:  Training average Loss: 0.937766\n",
      "Epoch 1_42.451%:  Training average Loss: 0.938543\n",
      "Epoch 1_43.252%:  Training average Loss: 0.937674\n",
      "Epoch 1_44.053%:  Training average Loss: 0.936863\n",
      "Epoch 1_44.854%:  Training average Loss: 0.936248\n",
      "Epoch 1_45.655%:  Training average Loss: 0.936321\n",
      "Epoch 1_46.456%:  Training average Loss: 0.935833\n",
      "Epoch 1_47.257%:  Training average Loss: 0.935778\n",
      "Epoch 1_48.058%:  Training average Loss: 0.936178\n",
      "Epoch 1_48.859%:  Training average Loss: 0.935402\n",
      "Epoch 1_49.660%:  Training average Loss: 0.935467\n",
      "Epoch 1_50.461%:  Training average Loss: 0.935719\n",
      "Epoch 1_51.262%:  Training average Loss: 0.935903\n",
      "Epoch 1_52.063%:  Training average Loss: 0.935777\n",
      "Epoch 1_52.864%:  Training average Loss: 0.936187\n",
      "Epoch 1_53.665%:  Training average Loss: 0.935730\n",
      "Epoch 1_54.466%:  Training average Loss: 0.935951\n",
      "Epoch 1_55.267%:  Training average Loss: 0.935486\n",
      "Epoch 1_56.068%:  Training average Loss: 0.935819\n",
      "Epoch 1_56.869%:  Training average Loss: 0.935551\n",
      "Epoch 1_57.670%:  Training average Loss: 0.935761\n",
      "Epoch 1_58.471%:  Training average Loss: 0.935048\n",
      "Epoch 1_59.272%:  Training average Loss: 0.934912\n",
      "Epoch 1_60.073%:  Training average Loss: 0.934752\n",
      "Epoch 1_60.874%:  Training average Loss: 0.935026\n",
      "Epoch 1_61.675%:  Training average Loss: 0.934788\n",
      "Epoch 1_62.475%:  Training average Loss: 0.934597\n",
      "Epoch 1_63.276%:  Training average Loss: 0.934647\n",
      "Epoch 1_64.077%:  Training average Loss: 0.934548\n",
      "Epoch 1_64.878%:  Training average Loss: 0.934753\n",
      "Epoch 1_65.679%:  Training average Loss: 0.934532\n",
      "Epoch 1_66.480%:  Training average Loss: 0.934879\n",
      "Epoch 1_67.281%:  Training average Loss: 0.935879\n",
      "Epoch 1_68.082%:  Training average Loss: 0.936309\n",
      "Epoch 1_68.883%:  Training average Loss: 0.936208\n",
      "Epoch 1_69.684%:  Training average Loss: 0.935958\n",
      "Epoch 1_70.485%:  Training average Loss: 0.936405\n",
      "Epoch 1_71.286%:  Training average Loss: 0.935659\n",
      "Epoch 1_72.087%:  Training average Loss: 0.935763\n",
      "Epoch 1_72.888%:  Training average Loss: 0.935216\n",
      "Epoch 1_73.689%:  Training average Loss: 0.935125\n",
      "Epoch 1_74.490%:  Training average Loss: 0.934456\n",
      "Epoch 1_75.291%:  Training average Loss: 0.934561\n",
      "Epoch 1_76.092%:  Training average Loss: 0.934491\n",
      "Epoch 1_76.893%:  Training average Loss: 0.934088\n",
      "Epoch 1_77.694%:  Training average Loss: 0.934150\n",
      "Epoch 1_78.495%:  Training average Loss: 0.934248\n",
      "Epoch 1_79.296%:  Training average Loss: 0.934332\n",
      "Epoch 1_80.097%:  Training average Loss: 0.934177\n",
      "Epoch 1_80.898%:  Training average Loss: 0.934249\n",
      "Epoch 1_81.699%:  Training average Loss: 0.934360\n",
      "Epoch 1_82.500%:  Training average Loss: 0.934396\n",
      "Epoch 1_83.301%:  Training average Loss: 0.933933\n",
      "Epoch 1_84.102%:  Training average Loss: 0.933311\n",
      "Epoch 1_84.903%:  Training average Loss: 0.933006\n",
      "Epoch 1_85.704%:  Training average Loss: 0.932915\n",
      "Epoch 1_86.504%:  Training average Loss: 0.932649\n",
      "Epoch 1_87.305%:  Training average Loss: 0.932319\n",
      "Epoch 1_88.106%:  Training average Loss: 0.932341\n",
      "Epoch 1_88.907%:  Training average Loss: 0.932075\n",
      "Epoch 1_89.708%:  Training average Loss: 0.932069\n",
      "Epoch 1_90.509%:  Training average Loss: 0.932144\n",
      "Epoch 1_91.310%:  Training average Loss: 0.932134\n",
      "Epoch 1_92.111%:  Training average Loss: 0.932032\n",
      "Epoch 1_92.912%:  Training average Loss: 0.931784\n",
      "Epoch 1_93.713%:  Training average Loss: 0.931597\n",
      "Epoch 1_94.514%:  Training average Loss: 0.931326\n",
      "Epoch 1_95.315%:  Training average Loss: 0.931158\n",
      "Epoch 1_96.116%:  Training average Loss: 0.931473\n",
      "Epoch 1_96.917%:  Training average Loss: 0.930906\n",
      "Epoch 1_97.718%:  Training average Loss: 0.930914\n",
      "Epoch 1_98.519%:  Training average Loss: 0.930783\n",
      "Epoch 1_99.320%:  Training average Loss: 0.930621\n",
      "Epoch 1 :  Verification average Loss: 0.931634, Verification accuracy: 61.721078%,Total Time:326.230502\n",
      "Model is saved in model_dict/model_lstm/epoch_1_accuracy_0.617211\n",
      "Epoch 2_0.801%:  Training average Loss: 0.869472\n",
      "Epoch 2_1.602%:  Training average Loss: 0.883301\n",
      "Epoch 2_2.403%:  Training average Loss: 0.878312\n",
      "Epoch 2_3.204%:  Training average Loss: 0.879976\n",
      "Epoch 2_4.005%:  Training average Loss: 0.888703\n",
      "Epoch 2_4.806%:  Training average Loss: 0.886976\n",
      "Epoch 2_5.607%:  Training average Loss: 0.889581\n",
      "Epoch 2_6.408%:  Training average Loss: 0.891015\n",
      "Epoch 2_7.209%:  Training average Loss: 0.895578\n",
      "Epoch 2_8.010%:  Training average Loss: 0.896067\n",
      "Epoch 2_8.811%:  Training average Loss: 0.895229\n",
      "Epoch 2_9.612%:  Training average Loss: 0.897774\n",
      "Epoch 2_10.413%:  Training average Loss: 0.897215\n",
      "Epoch 2_11.214%:  Training average Loss: 0.894688\n",
      "Epoch 2_12.015%:  Training average Loss: 0.895294\n",
      "Epoch 2_12.815%:  Training average Loss: 0.892731\n",
      "Epoch 2_13.616%:  Training average Loss: 0.892710\n",
      "Epoch 2_14.417%:  Training average Loss: 0.891261\n",
      "Epoch 2_15.218%:  Training average Loss: 0.890567\n",
      "Epoch 2_16.019%:  Training average Loss: 0.891742\n",
      "Epoch 2_16.820%:  Training average Loss: 0.892544\n",
      "Epoch 2_17.621%:  Training average Loss: 0.892230\n",
      "Epoch 2_18.422%:  Training average Loss: 0.893128\n",
      "Epoch 2_19.223%:  Training average Loss: 0.892910\n",
      "Epoch 2_20.024%:  Training average Loss: 0.892891\n",
      "Epoch 2_20.825%:  Training average Loss: 0.893796\n",
      "Epoch 2_21.626%:  Training average Loss: 0.893434\n",
      "Epoch 2_22.427%:  Training average Loss: 0.893101\n",
      "Epoch 2_23.228%:  Training average Loss: 0.892671\n",
      "Epoch 2_24.029%:  Training average Loss: 0.894253\n",
      "Epoch 2_24.830%:  Training average Loss: 0.894677\n",
      "Epoch 2_25.631%:  Training average Loss: 0.894952\n",
      "Epoch 2_26.432%:  Training average Loss: 0.895904\n",
      "Epoch 2_27.233%:  Training average Loss: 0.896192\n",
      "Epoch 2_28.034%:  Training average Loss: 0.896741\n",
      "Epoch 2_28.835%:  Training average Loss: 0.896984\n",
      "Epoch 2_29.636%:  Training average Loss: 0.897748\n",
      "Epoch 2_30.437%:  Training average Loss: 0.897294\n",
      "Epoch 2_31.238%:  Training average Loss: 0.898529\n",
      "Epoch 2_32.039%:  Training average Loss: 0.897884\n",
      "Epoch 2_32.840%:  Training average Loss: 0.899152\n",
      "Epoch 2_33.641%:  Training average Loss: 0.899511\n",
      "Epoch 2_34.442%:  Training average Loss: 0.899298\n",
      "Epoch 2_35.243%:  Training average Loss: 0.899898\n",
      "Epoch 2_36.044%:  Training average Loss: 0.899576\n",
      "Epoch 2_36.845%:  Training average Loss: 0.899978\n",
      "Epoch 2_37.645%:  Training average Loss: 0.899911\n",
      "Epoch 2_38.446%:  Training average Loss: 0.900391\n",
      "Epoch 2_39.247%:  Training average Loss: 0.900059\n",
      "Epoch 2_40.048%:  Training average Loss: 0.900848\n",
      "Epoch 2_40.849%:  Training average Loss: 0.901378\n",
      "Epoch 2_41.650%:  Training average Loss: 0.901632\n",
      "Epoch 2_42.451%:  Training average Loss: 0.901442\n",
      "Epoch 2_43.252%:  Training average Loss: 0.901692\n",
      "Epoch 2_44.053%:  Training average Loss: 0.901966\n",
      "Epoch 2_44.854%:  Training average Loss: 0.902136\n",
      "Epoch 2_45.655%:  Training average Loss: 0.902254\n",
      "Epoch 2_46.456%:  Training average Loss: 0.902100\n",
      "Epoch 2_47.257%:  Training average Loss: 0.901637\n",
      "Epoch 2_48.058%:  Training average Loss: 0.900886\n",
      "Epoch 2_48.859%:  Training average Loss: 0.900620\n",
      "Epoch 2_49.660%:  Training average Loss: 0.900078\n",
      "Epoch 2_50.461%:  Training average Loss: 0.899821\n",
      "Epoch 2_51.262%:  Training average Loss: 0.899669\n",
      "Epoch 2_52.063%:  Training average Loss: 0.899754\n",
      "Epoch 2_52.864%:  Training average Loss: 0.899736\n",
      "Epoch 2_53.665%:  Training average Loss: 0.899595\n",
      "Epoch 2_54.466%:  Training average Loss: 0.900241\n",
      "Epoch 2_55.267%:  Training average Loss: 0.900623\n",
      "Epoch 2_56.068%:  Training average Loss: 0.900129\n",
      "Epoch 2_56.869%:  Training average Loss: 0.899740\n",
      "Epoch 2_57.670%:  Training average Loss: 0.899532\n",
      "Epoch 2_58.471%:  Training average Loss: 0.899044\n",
      "Epoch 2_59.272%:  Training average Loss: 0.899012\n",
      "Epoch 2_60.073%:  Training average Loss: 0.899453\n",
      "Epoch 2_60.874%:  Training average Loss: 0.899489\n",
      "Epoch 2_61.675%:  Training average Loss: 0.899523\n",
      "Epoch 2_62.475%:  Training average Loss: 0.899340\n",
      "Epoch 2_63.276%:  Training average Loss: 0.899316\n",
      "Epoch 2_64.077%:  Training average Loss: 0.898831\n",
      "Epoch 2_64.878%:  Training average Loss: 0.898837\n",
      "Epoch 2_65.679%:  Training average Loss: 0.898807\n",
      "Epoch 2_66.480%:  Training average Loss: 0.899012\n",
      "Epoch 2_67.281%:  Training average Loss: 0.898759\n",
      "Epoch 2_68.082%:  Training average Loss: 0.898570\n",
      "Epoch 2_68.883%:  Training average Loss: 0.898336\n",
      "Epoch 2_69.684%:  Training average Loss: 0.898649\n",
      "Epoch 2_70.485%:  Training average Loss: 0.898607\n",
      "Epoch 2_71.286%:  Training average Loss: 0.898559\n",
      "Epoch 2_72.087%:  Training average Loss: 0.898363\n",
      "Epoch 2_72.888%:  Training average Loss: 0.898066\n",
      "Epoch 2_73.689%:  Training average Loss: 0.898319\n",
      "Epoch 2_74.490%:  Training average Loss: 0.898649\n",
      "Epoch 2_75.291%:  Training average Loss: 0.899038\n",
      "Epoch 2_76.092%:  Training average Loss: 0.899522\n",
      "Epoch 2_76.893%:  Training average Loss: 0.899709\n",
      "Epoch 2_77.694%:  Training average Loss: 0.899602\n",
      "Epoch 2_78.495%:  Training average Loss: 0.899114\n",
      "Epoch 2_79.296%:  Training average Loss: 0.899039\n",
      "Epoch 2_80.097%:  Training average Loss: 0.898801\n",
      "Epoch 2_80.898%:  Training average Loss: 0.898719\n",
      "Epoch 2_81.699%:  Training average Loss: 0.898902\n",
      "Epoch 2_82.500%:  Training average Loss: 0.898687\n",
      "Epoch 2_83.301%:  Training average Loss: 0.898915\n",
      "Epoch 2_84.102%:  Training average Loss: 0.898517\n",
      "Epoch 2_84.903%:  Training average Loss: 0.898534\n",
      "Epoch 2_85.704%:  Training average Loss: 0.898631\n",
      "Epoch 2_86.504%:  Training average Loss: 0.898951\n",
      "Epoch 2_87.305%:  Training average Loss: 0.898745\n",
      "Epoch 2_88.106%:  Training average Loss: 0.898756\n",
      "Epoch 2_88.907%:  Training average Loss: 0.898459\n",
      "Epoch 2_89.708%:  Training average Loss: 0.898360\n",
      "Epoch 2_90.509%:  Training average Loss: 0.898813\n",
      "Epoch 2_91.310%:  Training average Loss: 0.899074\n",
      "Epoch 2_92.111%:  Training average Loss: 0.899267\n",
      "Epoch 2_92.912%:  Training average Loss: 0.899108\n",
      "Epoch 2_93.713%:  Training average Loss: 0.898872\n",
      "Epoch 2_94.514%:  Training average Loss: 0.898962\n",
      "Epoch 2_95.315%:  Training average Loss: 0.898961\n",
      "Epoch 2_96.116%:  Training average Loss: 0.898866\n",
      "Epoch 2_96.917%:  Training average Loss: 0.898836\n",
      "Epoch 2_97.718%:  Training average Loss: 0.898899\n",
      "Epoch 2_98.519%:  Training average Loss: 0.898945\n",
      "Epoch 2_99.320%:  Training average Loss: 0.898831\n",
      "Epoch 2 :  Verification average Loss: 0.916992, Verification accuracy: 61.512831%,Total Time:486.444560\n",
      "Epoch 3_0.801%:  Training average Loss: 0.891662\n",
      "Epoch 3_1.602%:  Training average Loss: 0.900431\n",
      "Epoch 3_2.403%:  Training average Loss: 0.894342\n",
      "Epoch 3_3.204%:  Training average Loss: 0.878659\n",
      "Epoch 3_4.005%:  Training average Loss: 0.870094\n",
      "Epoch 3_4.806%:  Training average Loss: 0.868771\n",
      "Epoch 3_5.607%:  Training average Loss: 0.872411\n",
      "Epoch 3_6.408%:  Training average Loss: 0.879945\n",
      "Epoch 3_7.209%:  Training average Loss: 0.879471\n",
      "Epoch 3_8.010%:  Training average Loss: 0.881319\n",
      "Epoch 3_8.811%:  Training average Loss: 0.881578\n",
      "Epoch 3_9.612%:  Training average Loss: 0.883126\n",
      "Epoch 3_10.413%:  Training average Loss: 0.879014\n",
      "Epoch 3_11.214%:  Training average Loss: 0.880788\n",
      "Epoch 3_12.015%:  Training average Loss: 0.881961\n",
      "Epoch 3_12.815%:  Training average Loss: 0.882367\n",
      "Epoch 3_13.616%:  Training average Loss: 0.882859\n",
      "Epoch 3_14.417%:  Training average Loss: 0.882484\n",
      "Epoch 3_15.218%:  Training average Loss: 0.881228\n",
      "Epoch 3_16.019%:  Training average Loss: 0.881114\n",
      "Epoch 3_16.820%:  Training average Loss: 0.884724\n",
      "Epoch 3_17.621%:  Training average Loss: 0.884371\n",
      "Epoch 3_18.422%:  Training average Loss: 0.885266\n",
      "Epoch 3_19.223%:  Training average Loss: 0.885023\n",
      "Epoch 3_20.024%:  Training average Loss: 0.887099\n",
      "Epoch 3_20.825%:  Training average Loss: 0.885759\n",
      "Epoch 3_21.626%:  Training average Loss: 0.884692\n",
      "Epoch 3_22.427%:  Training average Loss: 0.884595\n",
      "Epoch 3_23.228%:  Training average Loss: 0.884748\n",
      "Epoch 3_24.029%:  Training average Loss: 0.885578\n",
      "Epoch 3_24.830%:  Training average Loss: 0.886519\n",
      "Epoch 3_25.631%:  Training average Loss: 0.885278\n",
      "Epoch 3_26.432%:  Training average Loss: 0.884789\n",
      "Epoch 3_27.233%:  Training average Loss: 0.882770\n",
      "Epoch 3_28.034%:  Training average Loss: 0.882900\n",
      "Epoch 3_28.835%:  Training average Loss: 0.883177\n",
      "Epoch 3_29.636%:  Training average Loss: 0.883031\n",
      "Epoch 3_30.437%:  Training average Loss: 0.882935\n",
      "Epoch 3_31.238%:  Training average Loss: 0.882834\n",
      "Epoch 3_32.039%:  Training average Loss: 0.882431\n",
      "Epoch 3_32.840%:  Training average Loss: 0.882972\n",
      "Epoch 3_33.641%:  Training average Loss: 0.882495\n",
      "Epoch 3_34.442%:  Training average Loss: 0.881573\n",
      "Epoch 3_35.243%:  Training average Loss: 0.882113\n",
      "Epoch 3_36.044%:  Training average Loss: 0.881877\n",
      "Epoch 3_36.845%:  Training average Loss: 0.881656\n",
      "Epoch 3_37.645%:  Training average Loss: 0.881292\n",
      "Epoch 3_38.446%:  Training average Loss: 0.881213\n",
      "Epoch 3_39.247%:  Training average Loss: 0.880620\n",
      "Epoch 3_40.048%:  Training average Loss: 0.880685\n",
      "Epoch 3_40.849%:  Training average Loss: 0.880639\n",
      "Epoch 3_41.650%:  Training average Loss: 0.879934\n",
      "Epoch 3_42.451%:  Training average Loss: 0.880230\n",
      "Epoch 3_43.252%:  Training average Loss: 0.879971\n",
      "Epoch 3_44.053%:  Training average Loss: 0.878932\n",
      "Epoch 3_44.854%:  Training average Loss: 0.878694\n",
      "Epoch 3_45.655%:  Training average Loss: 0.878691\n",
      "Epoch 3_46.456%:  Training average Loss: 0.878910\n",
      "Epoch 3_47.257%:  Training average Loss: 0.879495\n",
      "Epoch 3_48.058%:  Training average Loss: 0.878734\n",
      "Epoch 3_48.859%:  Training average Loss: 0.878754\n",
      "Epoch 3_49.660%:  Training average Loss: 0.878357\n",
      "Epoch 3_50.461%:  Training average Loss: 0.878003\n",
      "Epoch 3_51.262%:  Training average Loss: 0.877314\n",
      "Epoch 3_52.063%:  Training average Loss: 0.877733\n",
      "Epoch 3_52.864%:  Training average Loss: 0.877658\n",
      "Epoch 3_53.665%:  Training average Loss: 0.877141\n",
      "Epoch 3_54.466%:  Training average Loss: 0.877536\n",
      "Epoch 3_55.267%:  Training average Loss: 0.877252\n",
      "Epoch 3_56.068%:  Training average Loss: 0.877973\n",
      "Epoch 3_56.869%:  Training average Loss: 0.877800\n",
      "Epoch 3_57.670%:  Training average Loss: 0.877885\n",
      "Epoch 3_58.471%:  Training average Loss: 0.877901\n",
      "Epoch 3_59.272%:  Training average Loss: 0.877382\n",
      "Epoch 3_60.073%:  Training average Loss: 0.877705\n",
      "Epoch 3_60.874%:  Training average Loss: 0.877148\n",
      "Epoch 3_61.675%:  Training average Loss: 0.877175\n",
      "Epoch 3_62.475%:  Training average Loss: 0.877232\n",
      "Epoch 3_63.276%:  Training average Loss: 0.876964\n",
      "Epoch 3_64.077%:  Training average Loss: 0.877037\n",
      "Epoch 3_64.878%:  Training average Loss: 0.876985\n",
      "Epoch 3_65.679%:  Training average Loss: 0.876382\n",
      "Epoch 3_66.480%:  Training average Loss: 0.876051\n",
      "Epoch 3_67.281%:  Training average Loss: 0.875949\n",
      "Epoch 3_68.082%:  Training average Loss: 0.875931\n",
      "Epoch 3_68.883%:  Training average Loss: 0.876530\n",
      "Epoch 3_69.684%:  Training average Loss: 0.875528\n",
      "Epoch 3_70.485%:  Training average Loss: 0.874852\n",
      "Epoch 3_71.286%:  Training average Loss: 0.875375\n",
      "Epoch 3_72.087%:  Training average Loss: 0.875632\n",
      "Epoch 3_72.888%:  Training average Loss: 0.874940\n",
      "Epoch 3_73.689%:  Training average Loss: 0.875326\n",
      "Epoch 3_74.490%:  Training average Loss: 0.875426\n",
      "Epoch 3_75.291%:  Training average Loss: 0.875474\n",
      "Epoch 3_76.092%:  Training average Loss: 0.875160\n",
      "Epoch 3_76.893%:  Training average Loss: 0.875733\n",
      "Epoch 3_77.694%:  Training average Loss: 0.875675\n",
      "Epoch 3_78.495%:  Training average Loss: 0.875056\n",
      "Epoch 3_79.296%:  Training average Loss: 0.874998\n",
      "Epoch 3_80.097%:  Training average Loss: 0.875064\n",
      "Epoch 3_80.898%:  Training average Loss: 0.875045\n",
      "Epoch 3_81.699%:  Training average Loss: 0.874964\n",
      "Epoch 3_82.500%:  Training average Loss: 0.875034\n",
      "Epoch 3_83.301%:  Training average Loss: 0.874968\n",
      "Epoch 3_84.102%:  Training average Loss: 0.875279\n",
      "Epoch 3_84.903%:  Training average Loss: 0.874799\n",
      "Epoch 3_85.704%:  Training average Loss: 0.874701\n",
      "Epoch 3_86.504%:  Training average Loss: 0.874569\n",
      "Epoch 3_87.305%:  Training average Loss: 0.874184\n",
      "Epoch 3_88.106%:  Training average Loss: 0.873761\n",
      "Epoch 3_88.907%:  Training average Loss: 0.873686\n",
      "Epoch 3_89.708%:  Training average Loss: 0.873757\n",
      "Epoch 3_90.509%:  Training average Loss: 0.873506\n",
      "Epoch 3_91.310%:  Training average Loss: 0.873727\n",
      "Epoch 3_92.111%:  Training average Loss: 0.873536\n",
      "Epoch 3_92.912%:  Training average Loss: 0.873452\n",
      "Epoch 3_93.713%:  Training average Loss: 0.873595\n",
      "Epoch 3_94.514%:  Training average Loss: 0.873307\n",
      "Epoch 3_95.315%:  Training average Loss: 0.873838\n",
      "Epoch 3_96.116%:  Training average Loss: 0.873554\n",
      "Epoch 3_96.917%:  Training average Loss: 0.873819\n",
      "Epoch 3_97.718%:  Training average Loss: 0.873682\n",
      "Epoch 3_98.519%:  Training average Loss: 0.873639\n",
      "Epoch 3_99.320%:  Training average Loss: 0.873693\n",
      "Epoch 3 :  Verification average Loss: 0.893841, Verification accuracy: 63.012206%,Total Time:646.946574\n",
      "Model is saved in model_dict/model_lstm/epoch_3_accuracy_0.630122\n",
      "Epoch 4_0.801%:  Training average Loss: 0.845776\n",
      "Epoch 4_1.602%:  Training average Loss: 0.847717\n",
      "Epoch 4_2.403%:  Training average Loss: 0.837441\n",
      "Epoch 4_3.204%:  Training average Loss: 0.838968\n",
      "Epoch 4_4.005%:  Training average Loss: 0.846637\n",
      "Epoch 4_4.806%:  Training average Loss: 0.848264\n",
      "Epoch 4_5.607%:  Training average Loss: 0.856870\n",
      "Epoch 4_6.408%:  Training average Loss: 0.854487\n",
      "Epoch 4_7.209%:  Training average Loss: 0.848208\n",
      "Epoch 4_8.010%:  Training average Loss: 0.848293\n",
      "Epoch 4_8.811%:  Training average Loss: 0.847841\n",
      "Epoch 4_9.612%:  Training average Loss: 0.845908\n",
      "Epoch 4_10.413%:  Training average Loss: 0.849241\n",
      "Epoch 4_11.214%:  Training average Loss: 0.849277\n",
      "Epoch 4_12.015%:  Training average Loss: 0.849702\n",
      "Epoch 4_12.815%:  Training average Loss: 0.849291\n",
      "Epoch 4_13.616%:  Training average Loss: 0.849784\n",
      "Epoch 4_14.417%:  Training average Loss: 0.849258\n",
      "Epoch 4_15.218%:  Training average Loss: 0.848823\n",
      "Epoch 4_16.019%:  Training average Loss: 0.848640\n",
      "Epoch 4_16.820%:  Training average Loss: 0.847416\n",
      "Epoch 4_17.621%:  Training average Loss: 0.847528\n",
      "Epoch 4_18.422%:  Training average Loss: 0.849564\n",
      "Epoch 4_19.223%:  Training average Loss: 0.850091\n",
      "Epoch 4_20.024%:  Training average Loss: 0.848295\n",
      "Epoch 4_20.825%:  Training average Loss: 0.848110\n",
      "Epoch 4_21.626%:  Training average Loss: 0.847432\n",
      "Epoch 4_22.427%:  Training average Loss: 0.846959\n",
      "Epoch 4_23.228%:  Training average Loss: 0.845189\n",
      "Epoch 4_24.029%:  Training average Loss: 0.844857\n",
      "Epoch 4_24.830%:  Training average Loss: 0.846066\n",
      "Epoch 4_25.631%:  Training average Loss: 0.846230\n",
      "Epoch 4_26.432%:  Training average Loss: 0.848227\n",
      "Epoch 4_27.233%:  Training average Loss: 0.847747\n",
      "Epoch 4_28.034%:  Training average Loss: 0.847709\n",
      "Epoch 4_28.835%:  Training average Loss: 0.848986\n",
      "Epoch 4_29.636%:  Training average Loss: 0.850394\n",
      "Epoch 4_30.437%:  Training average Loss: 0.848611\n",
      "Epoch 4_31.238%:  Training average Loss: 0.847898\n",
      "Epoch 4_32.039%:  Training average Loss: 0.847059\n",
      "Epoch 4_32.840%:  Training average Loss: 0.846201\n",
      "Epoch 4_33.641%:  Training average Loss: 0.846676\n",
      "Epoch 4_34.442%:  Training average Loss: 0.848017\n",
      "Epoch 4_35.243%:  Training average Loss: 0.847863\n",
      "Epoch 4_36.044%:  Training average Loss: 0.848311\n",
      "Epoch 4_36.845%:  Training average Loss: 0.847967\n",
      "Epoch 4_37.645%:  Training average Loss: 0.847873\n",
      "Epoch 4_38.446%:  Training average Loss: 0.848111\n",
      "Epoch 4_39.247%:  Training average Loss: 0.848607\n",
      "Epoch 4_40.048%:  Training average Loss: 0.847884\n",
      "Epoch 4_40.849%:  Training average Loss: 0.847268\n",
      "Epoch 4_41.650%:  Training average Loss: 0.847313\n",
      "Epoch 4_42.451%:  Training average Loss: 0.847091\n",
      "Epoch 4_43.252%:  Training average Loss: 0.846992\n",
      "Epoch 4_44.053%:  Training average Loss: 0.846364\n",
      "Epoch 4_44.854%:  Training average Loss: 0.847345\n",
      "Epoch 4_45.655%:  Training average Loss: 0.847382\n",
      "Epoch 4_46.456%:  Training average Loss: 0.847185\n",
      "Epoch 4_47.257%:  Training average Loss: 0.847121\n",
      "Epoch 4_48.058%:  Training average Loss: 0.846590\n",
      "Epoch 4_48.859%:  Training average Loss: 0.846695\n",
      "Epoch 4_49.660%:  Training average Loss: 0.846922\n",
      "Epoch 4_50.461%:  Training average Loss: 0.846698\n",
      "Epoch 4_51.262%:  Training average Loss: 0.846923\n",
      "Epoch 4_52.063%:  Training average Loss: 0.847803\n",
      "Epoch 4_52.864%:  Training average Loss: 0.848056\n",
      "Epoch 4_53.665%:  Training average Loss: 0.847646\n",
      "Epoch 4_54.466%:  Training average Loss: 0.847641\n",
      "Epoch 4_55.267%:  Training average Loss: 0.848169\n",
      "Epoch 4_56.068%:  Training average Loss: 0.848617\n",
      "Epoch 4_56.869%:  Training average Loss: 0.848484\n",
      "Epoch 4_57.670%:  Training average Loss: 0.849108\n",
      "Epoch 4_58.471%:  Training average Loss: 0.849102\n",
      "Epoch 4_59.272%:  Training average Loss: 0.849480\n",
      "Epoch 4_60.073%:  Training average Loss: 0.849631\n",
      "Epoch 4_60.874%:  Training average Loss: 0.849359\n",
      "Epoch 4_61.675%:  Training average Loss: 0.849401\n",
      "Epoch 4_62.475%:  Training average Loss: 0.849621\n",
      "Epoch 4_63.276%:  Training average Loss: 0.849594\n",
      "Epoch 4_64.077%:  Training average Loss: 0.849721\n",
      "Epoch 4_64.878%:  Training average Loss: 0.849719\n",
      "Epoch 4_65.679%:  Training average Loss: 0.849902\n",
      "Epoch 4_66.480%:  Training average Loss: 0.850049\n",
      "Epoch 4_67.281%:  Training average Loss: 0.850050\n",
      "Epoch 4_68.082%:  Training average Loss: 0.849954\n",
      "Epoch 4_68.883%:  Training average Loss: 0.849867\n",
      "Epoch 4_69.684%:  Training average Loss: 0.849554\n",
      "Epoch 4_70.485%:  Training average Loss: 0.849995\n",
      "Epoch 4_71.286%:  Training average Loss: 0.850255\n",
      "Epoch 4_72.087%:  Training average Loss: 0.850309\n",
      "Epoch 4_72.888%:  Training average Loss: 0.850796\n",
      "Epoch 4_73.689%:  Training average Loss: 0.850624\n",
      "Epoch 4_74.490%:  Training average Loss: 0.850304\n",
      "Epoch 4_75.291%:  Training average Loss: 0.850374\n",
      "Epoch 4_76.092%:  Training average Loss: 0.850625\n",
      "Epoch 4_76.893%:  Training average Loss: 0.850720\n",
      "Epoch 4_77.694%:  Training average Loss: 0.851035\n",
      "Epoch 4_78.495%:  Training average Loss: 0.851060\n",
      "Epoch 4_79.296%:  Training average Loss: 0.851013\n",
      "Epoch 4_80.097%:  Training average Loss: 0.851270\n",
      "Epoch 4_80.898%:  Training average Loss: 0.851560\n",
      "Epoch 4_81.699%:  Training average Loss: 0.851455\n",
      "Epoch 4_82.500%:  Training average Loss: 0.851702\n",
      "Epoch 4_83.301%:  Training average Loss: 0.851832\n",
      "Epoch 4_84.102%:  Training average Loss: 0.851740\n",
      "Epoch 4_84.903%:  Training average Loss: 0.851402\n",
      "Epoch 4_85.704%:  Training average Loss: 0.851506\n",
      "Epoch 4_86.504%:  Training average Loss: 0.851795\n",
      "Epoch 4_87.305%:  Training average Loss: 0.852072\n",
      "Epoch 4_88.106%:  Training average Loss: 0.852080\n",
      "Epoch 4_88.907%:  Training average Loss: 0.851913\n",
      "Epoch 4_89.708%:  Training average Loss: 0.851781\n",
      "Epoch 4_90.509%:  Training average Loss: 0.852146\n",
      "Epoch 4_91.310%:  Training average Loss: 0.852200\n",
      "Epoch 4_92.111%:  Training average Loss: 0.852646\n",
      "Epoch 4_92.912%:  Training average Loss: 0.852620\n",
      "Epoch 4_93.713%:  Training average Loss: 0.852571\n",
      "Epoch 4_94.514%:  Training average Loss: 0.852825\n",
      "Epoch 4_95.315%:  Training average Loss: 0.852693\n",
      "Epoch 4_96.116%:  Training average Loss: 0.852836\n",
      "Epoch 4_96.917%:  Training average Loss: 0.853095\n",
      "Epoch 4_97.718%:  Training average Loss: 0.853303\n",
      "Epoch 4_98.519%:  Training average Loss: 0.853623\n",
      "Epoch 4_99.320%:  Training average Loss: 0.853151\n",
      "Epoch 4 :  Verification average Loss: 0.887325, Verification accuracy: 63.329382%,Total Time:806.690923\n",
      "Model is saved in model_dict/model_lstm/epoch_4_accuracy_0.633294\n",
      "Epoch 5_0.801%:  Training average Loss: 0.838442\n",
      "Epoch 5_1.602%:  Training average Loss: 0.834444\n",
      "Epoch 5_2.403%:  Training average Loss: 0.842931\n",
      "Epoch 5_3.204%:  Training average Loss: 0.841832\n",
      "Epoch 5_4.005%:  Training average Loss: 0.837190\n",
      "Epoch 5_4.806%:  Training average Loss: 0.824470\n",
      "Epoch 5_5.607%:  Training average Loss: 0.830257\n",
      "Epoch 5_6.408%:  Training average Loss: 0.828697\n",
      "Epoch 5_7.209%:  Training average Loss: 0.826320\n",
      "Epoch 5_8.010%:  Training average Loss: 0.826732\n",
      "Epoch 5_8.811%:  Training average Loss: 0.830515\n",
      "Epoch 5_9.612%:  Training average Loss: 0.830177\n",
      "Epoch 5_10.413%:  Training average Loss: 0.831514\n",
      "Epoch 5_11.214%:  Training average Loss: 0.832583\n",
      "Epoch 5_12.015%:  Training average Loss: 0.834076\n",
      "Epoch 5_12.815%:  Training average Loss: 0.833957\n",
      "Epoch 5_13.616%:  Training average Loss: 0.832873\n",
      "Epoch 5_14.417%:  Training average Loss: 0.831684\n",
      "Epoch 5_15.218%:  Training average Loss: 0.832890\n",
      "Epoch 5_16.019%:  Training average Loss: 0.833599\n",
      "Epoch 5_16.820%:  Training average Loss: 0.833389\n",
      "Epoch 5_17.621%:  Training average Loss: 0.834027\n",
      "Epoch 5_18.422%:  Training average Loss: 0.832406\n",
      "Epoch 5_19.223%:  Training average Loss: 0.833225\n",
      "Epoch 5_20.024%:  Training average Loss: 0.831830\n",
      "Epoch 5_20.825%:  Training average Loss: 0.833689\n",
      "Epoch 5_21.626%:  Training average Loss: 0.833462\n",
      "Epoch 5_22.427%:  Training average Loss: 0.836064\n",
      "Epoch 5_23.228%:  Training average Loss: 0.837141\n",
      "Epoch 5_24.029%:  Training average Loss: 0.836353\n",
      "Epoch 5_24.830%:  Training average Loss: 0.835759\n",
      "Epoch 5_25.631%:  Training average Loss: 0.836702\n",
      "Epoch 5_26.432%:  Training average Loss: 0.836518\n",
      "Epoch 5_27.233%:  Training average Loss: 0.836751\n",
      "Epoch 5_28.034%:  Training average Loss: 0.837321\n",
      "Epoch 5_28.835%:  Training average Loss: 0.837506\n",
      "Epoch 5_29.636%:  Training average Loss: 0.837964\n",
      "Epoch 5_30.437%:  Training average Loss: 0.837779\n",
      "Epoch 5_31.238%:  Training average Loss: 0.836742\n",
      "Epoch 5_32.039%:  Training average Loss: 0.835888\n",
      "Epoch 5_32.840%:  Training average Loss: 0.835633\n",
      "Epoch 5_33.641%:  Training average Loss: 0.836923\n",
      "Epoch 5_34.442%:  Training average Loss: 0.836668\n",
      "Epoch 5_35.243%:  Training average Loss: 0.836635\n",
      "Epoch 5_36.044%:  Training average Loss: 0.836216\n",
      "Epoch 5_36.845%:  Training average Loss: 0.835449\n",
      "Epoch 5_37.645%:  Training average Loss: 0.835304\n",
      "Epoch 5_38.446%:  Training average Loss: 0.834733\n",
      "Epoch 5_39.247%:  Training average Loss: 0.835057\n",
      "Epoch 5_40.048%:  Training average Loss: 0.834421\n",
      "Epoch 5_40.849%:  Training average Loss: 0.834876\n",
      "Epoch 5_41.650%:  Training average Loss: 0.834816\n",
      "Epoch 5_42.451%:  Training average Loss: 0.834343\n",
      "Epoch 5_43.252%:  Training average Loss: 0.834500\n",
      "Epoch 5_44.053%:  Training average Loss: 0.834684\n",
      "Epoch 5_44.854%:  Training average Loss: 0.835245\n",
      "Epoch 5_45.655%:  Training average Loss: 0.835383\n",
      "Epoch 5_46.456%:  Training average Loss: 0.835330\n",
      "Epoch 5_47.257%:  Training average Loss: 0.835260\n",
      "Epoch 5_48.058%:  Training average Loss: 0.835968\n",
      "Epoch 5_48.859%:  Training average Loss: 0.836193\n",
      "Epoch 5_49.660%:  Training average Loss: 0.835612\n",
      "Epoch 5_50.461%:  Training average Loss: 0.835620\n",
      "Epoch 5_51.262%:  Training average Loss: 0.835299\n",
      "Epoch 5_52.063%:  Training average Loss: 0.835603\n",
      "Epoch 5_52.864%:  Training average Loss: 0.835628\n",
      "Epoch 5_53.665%:  Training average Loss: 0.835403\n",
      "Epoch 5_54.466%:  Training average Loss: 0.835527\n",
      "Epoch 5_55.267%:  Training average Loss: 0.835728\n",
      "Epoch 5_56.068%:  Training average Loss: 0.835669\n",
      "Epoch 5_56.869%:  Training average Loss: 0.835353\n",
      "Epoch 5_57.670%:  Training average Loss: 0.834885\n",
      "Epoch 5_58.471%:  Training average Loss: 0.835169\n",
      "Epoch 5_59.272%:  Training average Loss: 0.834925\n",
      "Epoch 5_60.073%:  Training average Loss: 0.834714\n",
      "Epoch 5_60.874%:  Training average Loss: 0.834753\n",
      "Epoch 5_61.675%:  Training average Loss: 0.834583\n",
      "Epoch 5_62.475%:  Training average Loss: 0.835101\n",
      "Epoch 5_63.276%:  Training average Loss: 0.835491\n",
      "Epoch 5_64.077%:  Training average Loss: 0.835765\n",
      "Epoch 5_64.878%:  Training average Loss: 0.835854\n",
      "Epoch 5_65.679%:  Training average Loss: 0.835356\n",
      "Epoch 5_66.480%:  Training average Loss: 0.835259\n",
      "Epoch 5_67.281%:  Training average Loss: 0.834687\n",
      "Epoch 5_68.082%:  Training average Loss: 0.834480\n",
      "Epoch 5_68.883%:  Training average Loss: 0.834658\n",
      "Epoch 5_69.684%:  Training average Loss: 0.834940\n",
      "Epoch 5_70.485%:  Training average Loss: 0.835059\n",
      "Epoch 5_71.286%:  Training average Loss: 0.835129\n",
      "Epoch 5_72.087%:  Training average Loss: 0.835730\n",
      "Epoch 5_72.888%:  Training average Loss: 0.836233\n",
      "Epoch 5_73.689%:  Training average Loss: 0.836575\n",
      "Epoch 5_74.490%:  Training average Loss: 0.836238\n",
      "Epoch 5_75.291%:  Training average Loss: 0.836546\n",
      "Epoch 5_76.092%:  Training average Loss: 0.836350\n",
      "Epoch 5_76.893%:  Training average Loss: 0.836277\n",
      "Epoch 5_77.694%:  Training average Loss: 0.836347\n",
      "Epoch 5_78.495%:  Training average Loss: 0.836108\n",
      "Epoch 5_79.296%:  Training average Loss: 0.835882\n",
      "Epoch 5_80.097%:  Training average Loss: 0.835931\n",
      "Epoch 5_80.898%:  Training average Loss: 0.836008\n",
      "Epoch 5_81.699%:  Training average Loss: 0.836263\n",
      "Epoch 5_82.500%:  Training average Loss: 0.835958\n",
      "Epoch 5_83.301%:  Training average Loss: 0.835938\n",
      "Epoch 5_84.102%:  Training average Loss: 0.835909\n",
      "Epoch 5_84.903%:  Training average Loss: 0.836070\n",
      "Epoch 5_85.704%:  Training average Loss: 0.836023\n",
      "Epoch 5_86.504%:  Training average Loss: 0.835998\n",
      "Epoch 5_87.305%:  Training average Loss: 0.835685\n",
      "Epoch 5_88.106%:  Training average Loss: 0.836134\n",
      "Epoch 5_88.907%:  Training average Loss: 0.836085\n",
      "Epoch 5_89.708%:  Training average Loss: 0.836104\n",
      "Epoch 5_90.509%:  Training average Loss: 0.836047\n",
      "Epoch 5_91.310%:  Training average Loss: 0.835811\n",
      "Epoch 5_92.111%:  Training average Loss: 0.835914\n",
      "Epoch 5_92.912%:  Training average Loss: 0.835519\n",
      "Epoch 5_93.713%:  Training average Loss: 0.835329\n",
      "Epoch 5_94.514%:  Training average Loss: 0.835582\n",
      "Epoch 5_95.315%:  Training average Loss: 0.835418\n",
      "Epoch 5_96.116%:  Training average Loss: 0.835419\n",
      "Epoch 5_96.917%:  Training average Loss: 0.835586\n",
      "Epoch 5_97.718%:  Training average Loss: 0.835516\n",
      "Epoch 5_98.519%:  Training average Loss: 0.835693\n",
      "Epoch 5_99.320%:  Training average Loss: 0.835864\n",
      "Epoch 5 :  Verification average Loss: 0.884499, Verification accuracy: 64.127127%,Total Time:964.008236\n",
      "Model is saved in model_dict/model_lstm/epoch_5_accuracy_0.641271\n",
      "Epoch 6_0.801%:  Training average Loss: 0.756644\n",
      "Epoch 6_1.602%:  Training average Loss: 0.795945\n",
      "Epoch 6_2.403%:  Training average Loss: 0.801452\n",
      "Epoch 6_3.204%:  Training average Loss: 0.818281\n",
      "Epoch 6_4.005%:  Training average Loss: 0.807420\n",
      "Epoch 6_4.806%:  Training average Loss: 0.802761\n",
      "Epoch 6_5.607%:  Training average Loss: 0.811721\n",
      "Epoch 6_6.408%:  Training average Loss: 0.811483\n",
      "Epoch 6_7.209%:  Training average Loss: 0.811298\n",
      "Epoch 6_8.010%:  Training average Loss: 0.811425\n",
      "Epoch 6_8.811%:  Training average Loss: 0.812542\n",
      "Epoch 6_9.612%:  Training average Loss: 0.808424\n",
      "Epoch 6_10.413%:  Training average Loss: 0.807579\n",
      "Epoch 6_11.214%:  Training average Loss: 0.810407\n",
      "Epoch 6_12.015%:  Training average Loss: 0.808183\n",
      "Epoch 6_12.815%:  Training average Loss: 0.810534\n",
      "Epoch 6_13.616%:  Training average Loss: 0.810943\n",
      "Epoch 6_14.417%:  Training average Loss: 0.813503\n",
      "Epoch 6_15.218%:  Training average Loss: 0.814880\n",
      "Epoch 6_16.019%:  Training average Loss: 0.816548\n",
      "Epoch 6_16.820%:  Training average Loss: 0.815109\n",
      "Epoch 6_17.621%:  Training average Loss: 0.816099\n",
      "Epoch 6_18.422%:  Training average Loss: 0.815276\n",
      "Epoch 6_19.223%:  Training average Loss: 0.815484\n",
      "Epoch 6_20.024%:  Training average Loss: 0.816795\n",
      "Epoch 6_20.825%:  Training average Loss: 0.815896\n",
      "Epoch 6_21.626%:  Training average Loss: 0.817607\n",
      "Epoch 6_22.427%:  Training average Loss: 0.815637\n",
      "Epoch 6_23.228%:  Training average Loss: 0.815014\n",
      "Epoch 6_24.029%:  Training average Loss: 0.815244\n",
      "Epoch 6_24.830%:  Training average Loss: 0.815233\n",
      "Epoch 6_25.631%:  Training average Loss: 0.814857\n",
      "Epoch 6_26.432%:  Training average Loss: 0.813879\n",
      "Epoch 6_27.233%:  Training average Loss: 0.814061\n",
      "Epoch 6_28.034%:  Training average Loss: 0.813309\n",
      "Epoch 6_28.835%:  Training average Loss: 0.813754\n",
      "Epoch 6_29.636%:  Training average Loss: 0.814032\n",
      "Epoch 6_30.437%:  Training average Loss: 0.814373\n",
      "Epoch 6_31.238%:  Training average Loss: 0.814639\n",
      "Epoch 6_32.039%:  Training average Loss: 0.813811\n",
      "Epoch 6_32.840%:  Training average Loss: 0.814157\n",
      "Epoch 6_33.641%:  Training average Loss: 0.815281\n",
      "Epoch 6_34.442%:  Training average Loss: 0.814489\n",
      "Epoch 6_35.243%:  Training average Loss: 0.813717\n",
      "Epoch 6_36.044%:  Training average Loss: 0.813946\n",
      "Epoch 6_36.845%:  Training average Loss: 0.814231\n",
      "Epoch 6_37.645%:  Training average Loss: 0.814933\n",
      "Epoch 6_38.446%:  Training average Loss: 0.815224\n",
      "Epoch 6_39.247%:  Training average Loss: 0.815560\n",
      "Epoch 6_40.048%:  Training average Loss: 0.816494\n",
      "Epoch 6_40.849%:  Training average Loss: 0.817085\n",
      "Epoch 6_41.650%:  Training average Loss: 0.816409\n",
      "Epoch 6_42.451%:  Training average Loss: 0.816311\n",
      "Epoch 6_43.252%:  Training average Loss: 0.816557\n",
      "Epoch 6_44.053%:  Training average Loss: 0.816222\n",
      "Epoch 6_44.854%:  Training average Loss: 0.815611\n",
      "Epoch 6_45.655%:  Training average Loss: 0.815865\n",
      "Epoch 6_46.456%:  Training average Loss: 0.816163\n",
      "Epoch 6_47.257%:  Training average Loss: 0.817066\n",
      "Epoch 6_48.058%:  Training average Loss: 0.817175\n",
      "Epoch 6_48.859%:  Training average Loss: 0.817473\n",
      "Epoch 6_49.660%:  Training average Loss: 0.817362\n",
      "Epoch 6_50.461%:  Training average Loss: 0.817033\n",
      "Epoch 6_51.262%:  Training average Loss: 0.817667\n",
      "Epoch 6_52.063%:  Training average Loss: 0.817719\n",
      "Epoch 6_52.864%:  Training average Loss: 0.817514\n",
      "Epoch 6_53.665%:  Training average Loss: 0.817977\n",
      "Epoch 6_54.466%:  Training average Loss: 0.817647\n",
      "Epoch 6_55.267%:  Training average Loss: 0.817130\n",
      "Epoch 6_56.068%:  Training average Loss: 0.817257\n",
      "Epoch 6_56.869%:  Training average Loss: 0.818132\n",
      "Epoch 6_57.670%:  Training average Loss: 0.818518\n",
      "Epoch 6_58.471%:  Training average Loss: 0.818540\n",
      "Epoch 6_59.272%:  Training average Loss: 0.818719\n",
      "Epoch 6_60.073%:  Training average Loss: 0.819224\n",
      "Epoch 6_60.874%:  Training average Loss: 0.819079\n",
      "Epoch 6_61.675%:  Training average Loss: 0.819016\n",
      "Epoch 6_62.475%:  Training average Loss: 0.818551\n",
      "Epoch 6_63.276%:  Training average Loss: 0.818572\n",
      "Epoch 6_64.077%:  Training average Loss: 0.819108\n",
      "Epoch 6_64.878%:  Training average Loss: 0.819288\n",
      "Epoch 6_65.679%:  Training average Loss: 0.818985\n",
      "Epoch 6_66.480%:  Training average Loss: 0.818798\n",
      "Epoch 6_67.281%:  Training average Loss: 0.818743\n",
      "Epoch 6_68.082%:  Training average Loss: 0.819366\n",
      "Epoch 6_68.883%:  Training average Loss: 0.818859\n",
      "Epoch 6_69.684%:  Training average Loss: 0.818242\n",
      "Epoch 6_70.485%:  Training average Loss: 0.818322\n",
      "Epoch 6_71.286%:  Training average Loss: 0.817996\n",
      "Epoch 6_72.087%:  Training average Loss: 0.817895\n",
      "Epoch 6_72.888%:  Training average Loss: 0.817749\n",
      "Epoch 6_73.689%:  Training average Loss: 0.817763\n",
      "Epoch 6_74.490%:  Training average Loss: 0.817378\n",
      "Epoch 6_75.291%:  Training average Loss: 0.817184\n",
      "Epoch 6_76.092%:  Training average Loss: 0.817910\n",
      "Epoch 6_76.893%:  Training average Loss: 0.817479\n",
      "Epoch 6_77.694%:  Training average Loss: 0.817630\n",
      "Epoch 6_78.495%:  Training average Loss: 0.818000\n",
      "Epoch 6_79.296%:  Training average Loss: 0.818409\n",
      "Epoch 6_80.097%:  Training average Loss: 0.818329\n",
      "Epoch 6_80.898%:  Training average Loss: 0.818410\n",
      "Epoch 6_81.699%:  Training average Loss: 0.818175\n",
      "Epoch 6_82.500%:  Training average Loss: 0.817902\n",
      "Epoch 6_83.301%:  Training average Loss: 0.817778\n",
      "Epoch 6_84.102%:  Training average Loss: 0.817852\n",
      "Epoch 6_84.903%:  Training average Loss: 0.818022\n",
      "Epoch 6_85.704%:  Training average Loss: 0.818209\n",
      "Epoch 6_86.504%:  Training average Loss: 0.818191\n",
      "Epoch 6_87.305%:  Training average Loss: 0.818516\n",
      "Epoch 6_88.106%:  Training average Loss: 0.818552\n",
      "Epoch 6_88.907%:  Training average Loss: 0.818395\n",
      "Epoch 6_89.708%:  Training average Loss: 0.818648\n",
      "Epoch 6_90.509%:  Training average Loss: 0.818778\n",
      "Epoch 6_91.310%:  Training average Loss: 0.818697\n",
      "Epoch 6_92.111%:  Training average Loss: 0.819070\n",
      "Epoch 6_92.912%:  Training average Loss: 0.819124\n",
      "Epoch 6_93.713%:  Training average Loss: 0.819092\n",
      "Epoch 6_94.514%:  Training average Loss: 0.819266\n",
      "Epoch 6_95.315%:  Training average Loss: 0.819111\n",
      "Epoch 6_96.116%:  Training average Loss: 0.818768\n",
      "Epoch 6_96.917%:  Training average Loss: 0.818907\n",
      "Epoch 6_97.718%:  Training average Loss: 0.818903\n",
      "Epoch 6_98.519%:  Training average Loss: 0.819233\n",
      "Epoch 6_99.320%:  Training average Loss: 0.819401\n",
      "Epoch 6 :  Verification average Loss: 0.865527, Verification accuracy: 64.572454%,Total Time:1121.720754\n",
      "Model is saved in model_dict/model_lstm/epoch_6_accuracy_0.645725\n",
      "Epoch 7_0.801%:  Training average Loss: 0.777567\n",
      "Epoch 7_1.602%:  Training average Loss: 0.775538\n",
      "Epoch 7_2.403%:  Training average Loss: 0.778613\n",
      "Epoch 7_3.204%:  Training average Loss: 0.776370\n",
      "Epoch 7_4.005%:  Training average Loss: 0.781243\n",
      "Epoch 7_4.806%:  Training average Loss: 0.784043\n",
      "Epoch 7_5.607%:  Training average Loss: 0.783028\n",
      "Epoch 7_6.408%:  Training average Loss: 0.785717\n",
      "Epoch 7_7.209%:  Training average Loss: 0.783542\n",
      "Epoch 7_8.010%:  Training average Loss: 0.784470\n",
      "Epoch 7_8.811%:  Training average Loss: 0.789395\n",
      "Epoch 7_9.612%:  Training average Loss: 0.783703\n",
      "Epoch 7_10.413%:  Training average Loss: 0.786477\n",
      "Epoch 7_11.214%:  Training average Loss: 0.790406\n",
      "Epoch 7_12.015%:  Training average Loss: 0.790021\n",
      "Epoch 7_12.815%:  Training average Loss: 0.790446\n",
      "Epoch 7_13.616%:  Training average Loss: 0.791448\n",
      "Epoch 7_14.417%:  Training average Loss: 0.793573\n",
      "Epoch 7_15.218%:  Training average Loss: 0.795145\n",
      "Epoch 7_16.019%:  Training average Loss: 0.793597\n",
      "Epoch 7_16.820%:  Training average Loss: 0.795818\n",
      "Epoch 7_17.621%:  Training average Loss: 0.795545\n",
      "Epoch 7_18.422%:  Training average Loss: 0.795920\n",
      "Epoch 7_19.223%:  Training average Loss: 0.797157\n",
      "Epoch 7_20.024%:  Training average Loss: 0.797809\n",
      "Epoch 7_20.825%:  Training average Loss: 0.797882\n",
      "Epoch 7_21.626%:  Training average Loss: 0.796883\n",
      "Epoch 7_22.427%:  Training average Loss: 0.795417\n",
      "Epoch 7_23.228%:  Training average Loss: 0.797227\n",
      "Epoch 7_24.029%:  Training average Loss: 0.797144\n",
      "Epoch 7_24.830%:  Training average Loss: 0.796634\n",
      "Epoch 7_25.631%:  Training average Loss: 0.796631\n",
      "Epoch 7_26.432%:  Training average Loss: 0.796489\n",
      "Epoch 7_27.233%:  Training average Loss: 0.797043\n",
      "Epoch 7_28.034%:  Training average Loss: 0.796820\n",
      "Epoch 7_28.835%:  Training average Loss: 0.796474\n",
      "Epoch 7_29.636%:  Training average Loss: 0.797148\n",
      "Epoch 7_30.437%:  Training average Loss: 0.798456\n",
      "Epoch 7_31.238%:  Training average Loss: 0.798389\n",
      "Epoch 7_32.039%:  Training average Loss: 0.798242\n",
      "Epoch 7_32.840%:  Training average Loss: 0.797671\n",
      "Epoch 7_33.641%:  Training average Loss: 0.797901\n",
      "Epoch 7_34.442%:  Training average Loss: 0.797967\n",
      "Epoch 7_35.243%:  Training average Loss: 0.798167\n",
      "Epoch 7_36.044%:  Training average Loss: 0.797693\n",
      "Epoch 7_36.845%:  Training average Loss: 0.798542\n",
      "Epoch 7_37.645%:  Training average Loss: 0.799411\n",
      "Epoch 7_38.446%:  Training average Loss: 0.799223\n",
      "Epoch 7_39.247%:  Training average Loss: 0.799532\n",
      "Epoch 7_40.048%:  Training average Loss: 0.798425\n",
      "Epoch 7_40.849%:  Training average Loss: 0.799045\n",
      "Epoch 7_41.650%:  Training average Loss: 0.799814\n",
      "Epoch 7_42.451%:  Training average Loss: 0.799975\n",
      "Epoch 7_43.252%:  Training average Loss: 0.800043\n",
      "Epoch 7_44.053%:  Training average Loss: 0.800033\n",
      "Epoch 7_44.854%:  Training average Loss: 0.799417\n",
      "Epoch 7_45.655%:  Training average Loss: 0.799879\n",
      "Epoch 7_46.456%:  Training average Loss: 0.799472\n",
      "Epoch 7_47.257%:  Training average Loss: 0.799829\n",
      "Epoch 7_48.058%:  Training average Loss: 0.800951\n",
      "Epoch 7_48.859%:  Training average Loss: 0.800576\n",
      "Epoch 7_49.660%:  Training average Loss: 0.799935\n",
      "Epoch 7_50.461%:  Training average Loss: 0.800059\n",
      "Epoch 7_51.262%:  Training average Loss: 0.800671\n",
      "Epoch 7_52.063%:  Training average Loss: 0.800849\n",
      "Epoch 7_52.864%:  Training average Loss: 0.800535\n",
      "Epoch 7_53.665%:  Training average Loss: 0.800699\n",
      "Epoch 7_54.466%:  Training average Loss: 0.801295\n",
      "Epoch 7_55.267%:  Training average Loss: 0.801684\n",
      "Epoch 7_56.068%:  Training average Loss: 0.801917\n",
      "Epoch 7_56.869%:  Training average Loss: 0.802583\n",
      "Epoch 7_57.670%:  Training average Loss: 0.802950\n",
      "Epoch 7_58.471%:  Training average Loss: 0.802964\n",
      "Epoch 7_59.272%:  Training average Loss: 0.803219\n",
      "Epoch 7_60.073%:  Training average Loss: 0.803289\n",
      "Epoch 7_60.874%:  Training average Loss: 0.803299\n",
      "Epoch 7_61.675%:  Training average Loss: 0.803146\n",
      "Epoch 7_62.475%:  Training average Loss: 0.803000\n",
      "Epoch 7_63.276%:  Training average Loss: 0.803112\n",
      "Epoch 7_64.077%:  Training average Loss: 0.803170\n",
      "Epoch 7_64.878%:  Training average Loss: 0.802819\n",
      "Epoch 7_65.679%:  Training average Loss: 0.803192\n",
      "Epoch 7_66.480%:  Training average Loss: 0.802882\n",
      "Epoch 7_67.281%:  Training average Loss: 0.802322\n",
      "Epoch 7_68.082%:  Training average Loss: 0.802656\n",
      "Epoch 7_68.883%:  Training average Loss: 0.802616\n",
      "Epoch 7_69.684%:  Training average Loss: 0.802664\n",
      "Epoch 7_70.485%:  Training average Loss: 0.802639\n",
      "Epoch 7_71.286%:  Training average Loss: 0.802464\n",
      "Epoch 7_72.087%:  Training average Loss: 0.802256\n",
      "Epoch 7_72.888%:  Training average Loss: 0.802590\n",
      "Epoch 7_73.689%:  Training average Loss: 0.802701\n",
      "Epoch 7_74.490%:  Training average Loss: 0.802636\n",
      "Epoch 7_75.291%:  Training average Loss: 0.802330\n",
      "Epoch 7_76.092%:  Training average Loss: 0.802705\n",
      "Epoch 7_76.893%:  Training average Loss: 0.802686\n",
      "Epoch 7_77.694%:  Training average Loss: 0.802905\n",
      "Epoch 7_78.495%:  Training average Loss: 0.803008\n",
      "Epoch 7_79.296%:  Training average Loss: 0.802691\n",
      "Epoch 7_80.097%:  Training average Loss: 0.802932\n",
      "Epoch 7_80.898%:  Training average Loss: 0.803342\n",
      "Epoch 7_81.699%:  Training average Loss: 0.803789\n",
      "Epoch 7_82.500%:  Training average Loss: 0.803803\n",
      "Epoch 7_83.301%:  Training average Loss: 0.803329\n",
      "Epoch 7_84.102%:  Training average Loss: 0.803670\n",
      "Epoch 7_84.903%:  Training average Loss: 0.803934\n",
      "Epoch 7_85.704%:  Training average Loss: 0.804228\n",
      "Epoch 7_86.504%:  Training average Loss: 0.804322\n",
      "Epoch 7_87.305%:  Training average Loss: 0.804539\n",
      "Epoch 7_88.106%:  Training average Loss: 0.804643\n",
      "Epoch 7_88.907%:  Training average Loss: 0.804818\n",
      "Epoch 7_89.708%:  Training average Loss: 0.805061\n",
      "Epoch 7_90.509%:  Training average Loss: 0.805262\n",
      "Epoch 7_91.310%:  Training average Loss: 0.805483\n",
      "Epoch 7_92.111%:  Training average Loss: 0.805881\n",
      "Epoch 7_92.912%:  Training average Loss: 0.805933\n",
      "Epoch 7_93.713%:  Training average Loss: 0.805887\n",
      "Epoch 7_94.514%:  Training average Loss: 0.806077\n",
      "Epoch 7_95.315%:  Training average Loss: 0.806252\n",
      "Epoch 7_96.116%:  Training average Loss: 0.806708\n",
      "Epoch 7_96.917%:  Training average Loss: 0.806801\n",
      "Epoch 7_97.718%:  Training average Loss: 0.806717\n",
      "Epoch 7_98.519%:  Training average Loss: 0.806845\n",
      "Epoch 7_99.320%:  Training average Loss: 0.806827\n",
      "Epoch 7 :  Verification average Loss: 0.849142, Verification accuracy: 64.976132%,Total Time:1279.071574\n",
      "Model is saved in model_dict/model_lstm/epoch_7_accuracy_0.649761\n",
      "Epoch 8_0.801%:  Training average Loss: 0.790567\n",
      "Epoch 8_1.602%:  Training average Loss: 0.784393\n",
      "Epoch 8_2.403%:  Training average Loss: 0.784107\n",
      "Epoch 8_3.204%:  Training average Loss: 0.779786\n",
      "Epoch 8_4.005%:  Training average Loss: 0.774370\n",
      "Epoch 8_4.806%:  Training average Loss: 0.782475\n",
      "Epoch 8_5.607%:  Training average Loss: 0.783365\n",
      "Epoch 8_6.408%:  Training average Loss: 0.786123\n",
      "Epoch 8_7.209%:  Training average Loss: 0.784905\n",
      "Epoch 8_8.010%:  Training average Loss: 0.786965\n",
      "Epoch 8_8.811%:  Training average Loss: 0.784662\n",
      "Epoch 8_9.612%:  Training average Loss: 0.786406\n",
      "Epoch 8_10.413%:  Training average Loss: 0.783664\n",
      "Epoch 8_11.214%:  Training average Loss: 0.782462\n",
      "Epoch 8_12.015%:  Training average Loss: 0.782000\n",
      "Epoch 8_12.815%:  Training average Loss: 0.783813\n",
      "Epoch 8_13.616%:  Training average Loss: 0.783612\n",
      "Epoch 8_14.417%:  Training average Loss: 0.783885\n",
      "Epoch 8_15.218%:  Training average Loss: 0.783882\n",
      "Epoch 8_16.019%:  Training average Loss: 0.784113\n",
      "Epoch 8_16.820%:  Training average Loss: 0.784009\n",
      "Epoch 8_17.621%:  Training average Loss: 0.786122\n",
      "Epoch 8_18.422%:  Training average Loss: 0.784710\n",
      "Epoch 8_19.223%:  Training average Loss: 0.786553\n",
      "Epoch 8_20.024%:  Training average Loss: 0.786707\n",
      "Epoch 8_20.825%:  Training average Loss: 0.787282\n",
      "Epoch 8_21.626%:  Training average Loss: 0.787804\n",
      "Epoch 8_22.427%:  Training average Loss: 0.787002\n",
      "Epoch 8_23.228%:  Training average Loss: 0.787658\n",
      "Epoch 8_24.029%:  Training average Loss: 0.789440\n",
      "Epoch 8_24.830%:  Training average Loss: 0.790357\n",
      "Epoch 8_25.631%:  Training average Loss: 0.790466\n",
      "Epoch 8_26.432%:  Training average Loss: 0.791838\n",
      "Epoch 8_27.233%:  Training average Loss: 0.792272\n",
      "Epoch 8_28.034%:  Training average Loss: 0.791813\n",
      "Epoch 8_28.835%:  Training average Loss: 0.790846\n",
      "Epoch 8_29.636%:  Training average Loss: 0.790133\n",
      "Epoch 8_30.437%:  Training average Loss: 0.790626\n",
      "Epoch 8_31.238%:  Training average Loss: 0.790831\n",
      "Epoch 8_32.039%:  Training average Loss: 0.790757\n",
      "Epoch 8_32.840%:  Training average Loss: 0.792237\n",
      "Epoch 8_33.641%:  Training average Loss: 0.792277\n",
      "Epoch 8_34.442%:  Training average Loss: 0.793121\n",
      "Epoch 8_35.243%:  Training average Loss: 0.793634\n",
      "Epoch 8_36.044%:  Training average Loss: 0.793594\n",
      "Epoch 8_36.845%:  Training average Loss: 0.793556\n",
      "Epoch 8_37.645%:  Training average Loss: 0.792791\n",
      "Epoch 8_38.446%:  Training average Loss: 0.792848\n",
      "Epoch 8_39.247%:  Training average Loss: 0.793092\n",
      "Epoch 8_40.048%:  Training average Loss: 0.793235\n",
      "Epoch 8_40.849%:  Training average Loss: 0.792503\n",
      "Epoch 8_41.650%:  Training average Loss: 0.792610\n",
      "Epoch 8_42.451%:  Training average Loss: 0.792416\n",
      "Epoch 8_43.252%:  Training average Loss: 0.792322\n",
      "Epoch 8_44.053%:  Training average Loss: 0.792409\n",
      "Epoch 8_44.854%:  Training average Loss: 0.792077\n",
      "Epoch 8_45.655%:  Training average Loss: 0.792488\n",
      "Epoch 8_46.456%:  Training average Loss: 0.793017\n",
      "Epoch 8_47.257%:  Training average Loss: 0.792997\n",
      "Epoch 8_48.058%:  Training average Loss: 0.792827\n",
      "Epoch 8_48.859%:  Training average Loss: 0.792950\n",
      "Epoch 8_49.660%:  Training average Loss: 0.793175\n",
      "Epoch 8_50.461%:  Training average Loss: 0.793354\n",
      "Epoch 8_51.262%:  Training average Loss: 0.793274\n",
      "Epoch 8_52.063%:  Training average Loss: 0.792905\n",
      "Epoch 8_52.864%:  Training average Loss: 0.793241\n",
      "Epoch 8_53.665%:  Training average Loss: 0.793301\n",
      "Epoch 8_54.466%:  Training average Loss: 0.793642\n",
      "Epoch 8_55.267%:  Training average Loss: 0.792465\n",
      "Epoch 8_56.068%:  Training average Loss: 0.792518\n",
      "Epoch 8_56.869%:  Training average Loss: 0.792845\n",
      "Epoch 8_57.670%:  Training average Loss: 0.792889\n",
      "Epoch 8_58.471%:  Training average Loss: 0.793198\n",
      "Epoch 8_59.272%:  Training average Loss: 0.792732\n",
      "Epoch 8_60.073%:  Training average Loss: 0.792658\n",
      "Epoch 8_60.874%:  Training average Loss: 0.792854\n",
      "Epoch 8_61.675%:  Training average Loss: 0.793820\n",
      "Epoch 8_62.475%:  Training average Loss: 0.794249\n",
      "Epoch 8_63.276%:  Training average Loss: 0.794717\n",
      "Epoch 8_64.077%:  Training average Loss: 0.794851\n",
      "Epoch 8_64.878%:  Training average Loss: 0.795135\n",
      "Epoch 8_65.679%:  Training average Loss: 0.794534\n",
      "Epoch 8_66.480%:  Training average Loss: 0.794806\n",
      "Epoch 8_67.281%:  Training average Loss: 0.795202\n",
      "Epoch 8_68.082%:  Training average Loss: 0.795088\n",
      "Epoch 8_68.883%:  Training average Loss: 0.794888\n",
      "Epoch 8_69.684%:  Training average Loss: 0.794901\n",
      "Epoch 8_70.485%:  Training average Loss: 0.795057\n",
      "Epoch 8_71.286%:  Training average Loss: 0.794807\n",
      "Epoch 8_72.087%:  Training average Loss: 0.794829\n",
      "Epoch 8_72.888%:  Training average Loss: 0.794637\n",
      "Epoch 8_73.689%:  Training average Loss: 0.794287\n",
      "Epoch 8_74.490%:  Training average Loss: 0.794506\n",
      "Epoch 8_75.291%:  Training average Loss: 0.794691\n",
      "Epoch 8_76.092%:  Training average Loss: 0.794736\n",
      "Epoch 8_76.893%:  Training average Loss: 0.795109\n",
      "Epoch 8_77.694%:  Training average Loss: 0.795147\n",
      "Epoch 8_78.495%:  Training average Loss: 0.794562\n",
      "Epoch 8_79.296%:  Training average Loss: 0.794717\n",
      "Epoch 8_80.097%:  Training average Loss: 0.794413\n",
      "Epoch 8_80.898%:  Training average Loss: 0.794017\n",
      "Epoch 8_81.699%:  Training average Loss: 0.794167\n",
      "Epoch 8_82.500%:  Training average Loss: 0.793826\n",
      "Epoch 8_83.301%:  Training average Loss: 0.793749\n",
      "Epoch 8_84.102%:  Training average Loss: 0.794045\n",
      "Epoch 8_84.903%:  Training average Loss: 0.793779\n",
      "Epoch 8_85.704%:  Training average Loss: 0.793573\n",
      "Epoch 8_86.504%:  Training average Loss: 0.793501\n",
      "Epoch 8_87.305%:  Training average Loss: 0.793303\n",
      "Epoch 8_88.106%:  Training average Loss: 0.793176\n",
      "Epoch 8_88.907%:  Training average Loss: 0.793243\n",
      "Epoch 8_89.708%:  Training average Loss: 0.793405\n",
      "Epoch 8_90.509%:  Training average Loss: 0.793408\n",
      "Epoch 8_91.310%:  Training average Loss: 0.793747\n",
      "Epoch 8_92.111%:  Training average Loss: 0.793920\n",
      "Epoch 8_92.912%:  Training average Loss: 0.793702\n",
      "Epoch 8_93.713%:  Training average Loss: 0.793323\n",
      "Epoch 8_94.514%:  Training average Loss: 0.793020\n",
      "Epoch 8_95.315%:  Training average Loss: 0.792981\n",
      "Epoch 8_96.116%:  Training average Loss: 0.792969\n",
      "Epoch 8_96.917%:  Training average Loss: 0.793329\n",
      "Epoch 8_97.718%:  Training average Loss: 0.793744\n",
      "Epoch 8_98.519%:  Training average Loss: 0.793927\n",
      "Epoch 8_99.320%:  Training average Loss: 0.794033\n",
      "Epoch 8 :  Verification average Loss: 0.852959, Verification accuracy: 64.719828%,Total Time:1437.752704\n",
      "Epoch 9_0.801%:  Training average Loss: 0.814706\n",
      "Epoch 9_1.602%:  Training average Loss: 0.783652\n",
      "Epoch 9_2.403%:  Training average Loss: 0.770311\n",
      "Epoch 9_3.204%:  Training average Loss: 0.766057\n",
      "Epoch 9_4.005%:  Training average Loss: 0.760814\n",
      "Epoch 9_4.806%:  Training average Loss: 0.768479\n",
      "Epoch 9_5.607%:  Training average Loss: 0.766225\n",
      "Epoch 9_6.408%:  Training average Loss: 0.772599\n",
      "Epoch 9_7.209%:  Training average Loss: 0.772062\n",
      "Epoch 9_8.010%:  Training average Loss: 0.771289\n",
      "Epoch 9_8.811%:  Training average Loss: 0.772688\n",
      "Epoch 9_9.612%:  Training average Loss: 0.772406\n",
      "Epoch 9_10.413%:  Training average Loss: 0.771684\n",
      "Epoch 9_11.214%:  Training average Loss: 0.772184\n",
      "Epoch 9_12.015%:  Training average Loss: 0.774423\n",
      "Epoch 9_12.815%:  Training average Loss: 0.777300\n",
      "Epoch 9_13.616%:  Training average Loss: 0.777412\n",
      "Epoch 9_14.417%:  Training average Loss: 0.779407\n",
      "Epoch 9_15.218%:  Training average Loss: 0.779183\n",
      "Epoch 9_16.019%:  Training average Loss: 0.778936\n",
      "Epoch 9_16.820%:  Training average Loss: 0.779841\n",
      "Epoch 9_17.621%:  Training average Loss: 0.779593\n",
      "Epoch 9_18.422%:  Training average Loss: 0.779373\n",
      "Epoch 9_19.223%:  Training average Loss: 0.777761\n",
      "Epoch 9_20.024%:  Training average Loss: 0.778691\n",
      "Epoch 9_20.825%:  Training average Loss: 0.781376\n",
      "Epoch 9_21.626%:  Training average Loss: 0.781777\n",
      "Epoch 9_22.427%:  Training average Loss: 0.781554\n",
      "Epoch 9_23.228%:  Training average Loss: 0.781577\n",
      "Epoch 9_24.029%:  Training average Loss: 0.781652\n",
      "Epoch 9_24.830%:  Training average Loss: 0.781732\n",
      "Epoch 9_25.631%:  Training average Loss: 0.780852\n",
      "Epoch 9_26.432%:  Training average Loss: 0.780683\n",
      "Epoch 9_27.233%:  Training average Loss: 0.778780\n",
      "Epoch 9_28.034%:  Training average Loss: 0.778766\n",
      "Epoch 9_28.835%:  Training average Loss: 0.779262\n",
      "Epoch 9_29.636%:  Training average Loss: 0.779201\n",
      "Epoch 9_30.437%:  Training average Loss: 0.778402\n",
      "Epoch 9_31.238%:  Training average Loss: 0.777882\n",
      "Epoch 9_32.039%:  Training average Loss: 0.778346\n",
      "Epoch 9_32.840%:  Training average Loss: 0.778602\n",
      "Epoch 9_33.641%:  Training average Loss: 0.778901\n",
      "Epoch 9_34.442%:  Training average Loss: 0.778227\n",
      "Epoch 9_35.243%:  Training average Loss: 0.778387\n",
      "Epoch 9_36.044%:  Training average Loss: 0.779031\n",
      "Epoch 9_36.845%:  Training average Loss: 0.779382\n",
      "Epoch 9_37.645%:  Training average Loss: 0.779449\n",
      "Epoch 9_38.446%:  Training average Loss: 0.779845\n",
      "Epoch 9_39.247%:  Training average Loss: 0.779322\n",
      "Epoch 9_40.048%:  Training average Loss: 0.779899\n",
      "Epoch 9_40.849%:  Training average Loss: 0.779621\n",
      "Epoch 9_41.650%:  Training average Loss: 0.780509\n",
      "Epoch 9_42.451%:  Training average Loss: 0.779589\n",
      "Epoch 9_43.252%:  Training average Loss: 0.779732\n",
      "Epoch 9_44.053%:  Training average Loss: 0.780205\n",
      "Epoch 9_44.854%:  Training average Loss: 0.779919\n",
      "Epoch 9_45.655%:  Training average Loss: 0.781010\n",
      "Epoch 9_46.456%:  Training average Loss: 0.780674\n",
      "Epoch 9_47.257%:  Training average Loss: 0.780579\n",
      "Epoch 9_48.058%:  Training average Loss: 0.780438\n",
      "Epoch 9_48.859%:  Training average Loss: 0.780449\n",
      "Epoch 9_49.660%:  Training average Loss: 0.780389\n",
      "Epoch 9_50.461%:  Training average Loss: 0.780093\n",
      "Epoch 9_51.262%:  Training average Loss: 0.779706\n",
      "Epoch 9_52.063%:  Training average Loss: 0.779148\n",
      "Epoch 9_52.864%:  Training average Loss: 0.779175\n",
      "Epoch 9_53.665%:  Training average Loss: 0.779160\n",
      "Epoch 9_54.466%:  Training average Loss: 0.778635\n",
      "Epoch 9_55.267%:  Training average Loss: 0.778366\n",
      "Epoch 9_56.068%:  Training average Loss: 0.778361\n",
      "Epoch 9_56.869%:  Training average Loss: 0.778283\n",
      "Epoch 9_57.670%:  Training average Loss: 0.778473\n",
      "Epoch 9_58.471%:  Training average Loss: 0.778163\n",
      "Epoch 9_59.272%:  Training average Loss: 0.777998\n",
      "Epoch 9_60.073%:  Training average Loss: 0.777682\n",
      "Epoch 9_60.874%:  Training average Loss: 0.777648\n",
      "Epoch 9_61.675%:  Training average Loss: 0.777320\n",
      "Epoch 9_62.475%:  Training average Loss: 0.777043\n",
      "Epoch 9_63.276%:  Training average Loss: 0.776978\n",
      "Epoch 9_64.077%:  Training average Loss: 0.776783\n",
      "Epoch 9_64.878%:  Training average Loss: 0.776680\n",
      "Epoch 9_65.679%:  Training average Loss: 0.777181\n",
      "Epoch 9_66.480%:  Training average Loss: 0.777505\n",
      "Epoch 9_67.281%:  Training average Loss: 0.778134\n",
      "Epoch 9_68.082%:  Training average Loss: 0.778014\n",
      "Epoch 9_68.883%:  Training average Loss: 0.778023\n",
      "Epoch 9_69.684%:  Training average Loss: 0.778070\n",
      "Epoch 9_70.485%:  Training average Loss: 0.777708\n",
      "Epoch 9_71.286%:  Training average Loss: 0.777870\n",
      "Epoch 9_72.087%:  Training average Loss: 0.778252\n",
      "Epoch 9_72.888%:  Training average Loss: 0.778346\n",
      "Epoch 9_73.689%:  Training average Loss: 0.778397\n",
      "Epoch 9_74.490%:  Training average Loss: 0.778406\n",
      "Epoch 9_75.291%:  Training average Loss: 0.778215\n",
      "Epoch 9_76.092%:  Training average Loss: 0.777655\n",
      "Epoch 9_76.893%:  Training average Loss: 0.778166\n",
      "Epoch 9_77.694%:  Training average Loss: 0.778548\n",
      "Epoch 9_78.495%:  Training average Loss: 0.778643\n",
      "Epoch 9_79.296%:  Training average Loss: 0.778665\n",
      "Epoch 9_80.097%:  Training average Loss: 0.778589\n",
      "Epoch 9_80.898%:  Training average Loss: 0.779052\n",
      "Epoch 9_81.699%:  Training average Loss: 0.779456\n",
      "Epoch 9_82.500%:  Training average Loss: 0.779131\n",
      "Epoch 9_83.301%:  Training average Loss: 0.779451\n",
      "Epoch 9_84.102%:  Training average Loss: 0.779330\n",
      "Epoch 9_84.903%:  Training average Loss: 0.780077\n",
      "Epoch 9_85.704%:  Training average Loss: 0.780415\n",
      "Epoch 9_86.504%:  Training average Loss: 0.781236\n",
      "Epoch 9_87.305%:  Training average Loss: 0.781065\n",
      "Epoch 9_88.106%:  Training average Loss: 0.780959\n",
      "Epoch 9_88.907%:  Training average Loss: 0.781373\n",
      "Epoch 9_89.708%:  Training average Loss: 0.781307\n",
      "Epoch 9_90.509%:  Training average Loss: 0.781106\n",
      "Epoch 9_91.310%:  Training average Loss: 0.781082\n",
      "Epoch 9_92.111%:  Training average Loss: 0.781237\n",
      "Epoch 9_92.912%:  Training average Loss: 0.781231\n",
      "Epoch 9_93.713%:  Training average Loss: 0.781284\n",
      "Epoch 9_94.514%:  Training average Loss: 0.781261\n",
      "Epoch 9_95.315%:  Training average Loss: 0.781307\n",
      "Epoch 9_96.116%:  Training average Loss: 0.781941\n",
      "Epoch 9_96.917%:  Training average Loss: 0.782308\n",
      "Epoch 9_97.718%:  Training average Loss: 0.782796\n",
      "Epoch 9_98.519%:  Training average Loss: 0.782812\n",
      "Epoch 9_99.320%:  Training average Loss: 0.782966\n",
      "Epoch 9 :  Verification average Loss: 0.856316, Verification accuracy: 65.331753%,Total Time:1601.100205\n",
      "Model is saved in model_dict/model_lstm/epoch_9_accuracy_0.653318\n",
      "Epoch 10_0.801%:  Training average Loss: 0.783012\n",
      "Epoch 10_1.602%:  Training average Loss: 0.779065\n",
      "Epoch 10_2.403%:  Training average Loss: 0.778120\n",
      "Epoch 10_3.204%:  Training average Loss: 0.766972\n",
      "Epoch 10_4.005%:  Training average Loss: 0.773457\n",
      "Epoch 10_4.806%:  Training average Loss: 0.773763\n",
      "Epoch 10_5.607%:  Training average Loss: 0.768267\n",
      "Epoch 10_6.408%:  Training average Loss: 0.769620\n",
      "Epoch 10_7.209%:  Training average Loss: 0.772667\n",
      "Epoch 10_8.010%:  Training average Loss: 0.775787\n",
      "Epoch 10_8.811%:  Training average Loss: 0.777354\n",
      "Epoch 10_9.612%:  Training average Loss: 0.774691\n",
      "Epoch 10_10.413%:  Training average Loss: 0.774555\n",
      "Epoch 10_11.214%:  Training average Loss: 0.776884\n",
      "Epoch 10_12.015%:  Training average Loss: 0.778609\n",
      "Epoch 10_12.815%:  Training average Loss: 0.776782\n",
      "Epoch 10_13.616%:  Training average Loss: 0.773822\n",
      "Epoch 10_14.417%:  Training average Loss: 0.774130\n",
      "Epoch 10_15.218%:  Training average Loss: 0.775139\n",
      "Epoch 10_16.019%:  Training average Loss: 0.774488\n",
      "Epoch 10_16.820%:  Training average Loss: 0.776147\n",
      "Epoch 10_17.621%:  Training average Loss: 0.775897\n",
      "Epoch 10_18.422%:  Training average Loss: 0.776549\n",
      "Epoch 10_19.223%:  Training average Loss: 0.777185\n",
      "Epoch 10_20.024%:  Training average Loss: 0.778207\n",
      "Epoch 10_20.825%:  Training average Loss: 0.778040\n",
      "Epoch 10_21.626%:  Training average Loss: 0.776422\n",
      "Epoch 10_22.427%:  Training average Loss: 0.775054\n",
      "Epoch 10_23.228%:  Training average Loss: 0.774089\n",
      "Epoch 10_24.029%:  Training average Loss: 0.773470\n",
      "Epoch 10_24.830%:  Training average Loss: 0.772336\n",
      "Epoch 10_25.631%:  Training average Loss: 0.771937\n",
      "Epoch 10_26.432%:  Training average Loss: 0.771224\n",
      "Epoch 10_27.233%:  Training average Loss: 0.772770\n",
      "Epoch 10_28.034%:  Training average Loss: 0.772846\n",
      "Epoch 10_28.835%:  Training average Loss: 0.774289\n",
      "Epoch 10_29.636%:  Training average Loss: 0.774570\n",
      "Epoch 10_30.437%:  Training average Loss: 0.773788\n",
      "Epoch 10_31.238%:  Training average Loss: 0.773382\n",
      "Epoch 10_32.039%:  Training average Loss: 0.774532\n",
      "Epoch 10_32.840%:  Training average Loss: 0.773724\n",
      "Epoch 10_33.641%:  Training average Loss: 0.773005\n",
      "Epoch 10_34.442%:  Training average Loss: 0.772825\n",
      "Epoch 10_35.243%:  Training average Loss: 0.773109\n",
      "Epoch 10_36.044%:  Training average Loss: 0.773170\n",
      "Epoch 10_36.845%:  Training average Loss: 0.773107\n",
      "Epoch 10_37.645%:  Training average Loss: 0.772425\n",
      "Epoch 10_38.446%:  Training average Loss: 0.772652\n",
      "Epoch 10_39.247%:  Training average Loss: 0.773375\n",
      "Epoch 10_40.048%:  Training average Loss: 0.773478\n",
      "Epoch 10_40.849%:  Training average Loss: 0.774674\n",
      "Epoch 10_41.650%:  Training average Loss: 0.774354\n",
      "Epoch 10_42.451%:  Training average Loss: 0.774483\n",
      "Epoch 10_43.252%:  Training average Loss: 0.773607\n",
      "Epoch 10_44.053%:  Training average Loss: 0.774472\n",
      "Epoch 10_44.854%:  Training average Loss: 0.774201\n",
      "Epoch 10_45.655%:  Training average Loss: 0.774498\n",
      "Epoch 10_46.456%:  Training average Loss: 0.774311\n",
      "Epoch 10_47.257%:  Training average Loss: 0.773633\n",
      "Epoch 10_48.058%:  Training average Loss: 0.772883\n",
      "Epoch 10_48.859%:  Training average Loss: 0.772638\n",
      "Epoch 10_49.660%:  Training average Loss: 0.772983\n",
      "Epoch 10_50.461%:  Training average Loss: 0.773256\n",
      "Epoch 10_51.262%:  Training average Loss: 0.772679\n",
      "Epoch 10_52.063%:  Training average Loss: 0.772166\n",
      "Epoch 10_52.864%:  Training average Loss: 0.772109\n",
      "Epoch 10_53.665%:  Training average Loss: 0.772139\n",
      "Epoch 10_54.466%:  Training average Loss: 0.772058\n",
      "Epoch 10_55.267%:  Training average Loss: 0.772002\n",
      "Epoch 10_56.068%:  Training average Loss: 0.771824\n",
      "Epoch 10_56.869%:  Training average Loss: 0.771289\n",
      "Epoch 10_57.670%:  Training average Loss: 0.771030\n",
      "Epoch 10_58.471%:  Training average Loss: 0.771130\n",
      "Epoch 10_59.272%:  Training average Loss: 0.770891\n",
      "Epoch 10_60.073%:  Training average Loss: 0.770929\n",
      "Epoch 10_60.874%:  Training average Loss: 0.770743\n",
      "Epoch 10_61.675%:  Training average Loss: 0.770998\n",
      "Epoch 10_62.475%:  Training average Loss: 0.771371\n",
      "Epoch 10_63.276%:  Training average Loss: 0.771235\n",
      "Epoch 10_64.077%:  Training average Loss: 0.771391\n",
      "Epoch 10_64.878%:  Training average Loss: 0.771531\n",
      "Epoch 10_65.679%:  Training average Loss: 0.771243\n",
      "Epoch 10_66.480%:  Training average Loss: 0.771500\n",
      "Epoch 10_67.281%:  Training average Loss: 0.771117\n",
      "Epoch 10_68.082%:  Training average Loss: 0.771401\n",
      "Epoch 10_68.883%:  Training average Loss: 0.772185\n",
      "Epoch 10_69.684%:  Training average Loss: 0.772558\n",
      "Epoch 10_70.485%:  Training average Loss: 0.772735\n",
      "Epoch 10_71.286%:  Training average Loss: 0.772899\n",
      "Epoch 10_72.087%:  Training average Loss: 0.773231\n",
      "Epoch 10_72.888%:  Training average Loss: 0.773509\n",
      "Epoch 10_73.689%:  Training average Loss: 0.773010\n",
      "Epoch 10_74.490%:  Training average Loss: 0.773351\n",
      "Epoch 10_75.291%:  Training average Loss: 0.773336\n",
      "Epoch 10_76.092%:  Training average Loss: 0.773456\n",
      "Epoch 10_76.893%:  Training average Loss: 0.773711\n",
      "Epoch 10_77.694%:  Training average Loss: 0.773887\n",
      "Epoch 10_78.495%:  Training average Loss: 0.773987\n",
      "Epoch 10_79.296%:  Training average Loss: 0.774166\n",
      "Epoch 10_80.097%:  Training average Loss: 0.774000\n",
      "Epoch 10_80.898%:  Training average Loss: 0.774484\n",
      "Epoch 10_81.699%:  Training average Loss: 0.774351\n",
      "Epoch 10_82.500%:  Training average Loss: 0.774170\n",
      "Epoch 10_83.301%:  Training average Loss: 0.774368\n",
      "Epoch 10_84.102%:  Training average Loss: 0.774261\n",
      "Epoch 10_84.903%:  Training average Loss: 0.774620\n",
      "Epoch 10_85.704%:  Training average Loss: 0.774482\n",
      "Epoch 10_86.504%:  Training average Loss: 0.774298\n",
      "Epoch 10_87.305%:  Training average Loss: 0.774264\n",
      "Epoch 10_88.106%:  Training average Loss: 0.773924\n",
      "Epoch 10_88.907%:  Training average Loss: 0.774141\n",
      "Epoch 10_89.708%:  Training average Loss: 0.774440\n",
      "Epoch 10_90.509%:  Training average Loss: 0.774511\n",
      "Epoch 10_91.310%:  Training average Loss: 0.774763\n",
      "Epoch 10_92.111%:  Training average Loss: 0.774877\n",
      "Epoch 10_92.912%:  Training average Loss: 0.774935\n",
      "Epoch 10_93.713%:  Training average Loss: 0.774828\n",
      "Epoch 10_94.514%:  Training average Loss: 0.774490\n",
      "Epoch 10_95.315%:  Training average Loss: 0.774603\n",
      "Epoch 10_96.116%:  Training average Loss: 0.774407\n",
      "Epoch 10_96.917%:  Training average Loss: 0.774671\n",
      "Epoch 10_97.718%:  Training average Loss: 0.774781\n",
      "Epoch 10_98.519%:  Training average Loss: 0.774724\n",
      "Epoch 10_99.320%:  Training average Loss: 0.775157\n",
      "Epoch 10 :  Verification average Loss: 0.871140, Verification accuracy: 64.207221%,Total Time:1763.666522\n",
      "Epoch 11_0.801%:  Training average Loss: 0.772131\n",
      "Epoch 11_1.602%:  Training average Loss: 0.764115\n",
      "Epoch 11_2.403%:  Training average Loss: 0.753000\n",
      "Epoch 11_3.204%:  Training average Loss: 0.744936\n",
      "Epoch 11_4.005%:  Training average Loss: 0.746053\n",
      "Epoch 11_4.806%:  Training average Loss: 0.747665\n",
      "Epoch 11_5.607%:  Training average Loss: 0.749893\n",
      "Epoch 11_6.408%:  Training average Loss: 0.747197\n",
      "Epoch 11_7.209%:  Training average Loss: 0.749454\n",
      "Epoch 11_8.010%:  Training average Loss: 0.751189\n",
      "Epoch 11_8.811%:  Training average Loss: 0.754612\n",
      "Epoch 11_9.612%:  Training average Loss: 0.753217\n",
      "Epoch 11_10.413%:  Training average Loss: 0.754658\n",
      "Epoch 11_11.214%:  Training average Loss: 0.753008\n",
      "Epoch 11_12.015%:  Training average Loss: 0.753617\n",
      "Epoch 11_12.815%:  Training average Loss: 0.752979\n",
      "Epoch 11_13.616%:  Training average Loss: 0.753319\n",
      "Epoch 11_14.417%:  Training average Loss: 0.751002\n",
      "Epoch 11_15.218%:  Training average Loss: 0.749156\n",
      "Epoch 11_16.019%:  Training average Loss: 0.750137\n",
      "Epoch 11_16.820%:  Training average Loss: 0.749900\n",
      "Epoch 11_17.621%:  Training average Loss: 0.750009\n",
      "Epoch 11_18.422%:  Training average Loss: 0.749182\n",
      "Epoch 11_19.223%:  Training average Loss: 0.749582\n",
      "Epoch 11_20.024%:  Training average Loss: 0.749173\n",
      "Epoch 11_20.825%:  Training average Loss: 0.750860\n",
      "Epoch 11_21.626%:  Training average Loss: 0.753122\n",
      "Epoch 11_22.427%:  Training average Loss: 0.751647\n",
      "Epoch 11_23.228%:  Training average Loss: 0.754130\n",
      "Epoch 11_24.029%:  Training average Loss: 0.753953\n",
      "Epoch 11_24.830%:  Training average Loss: 0.753151\n",
      "Epoch 11_25.631%:  Training average Loss: 0.754565\n",
      "Epoch 11_26.432%:  Training average Loss: 0.755814\n",
      "Epoch 11_27.233%:  Training average Loss: 0.755159\n",
      "Epoch 11_28.034%:  Training average Loss: 0.755302\n",
      "Epoch 11_28.835%:  Training average Loss: 0.756270\n",
      "Epoch 11_29.636%:  Training average Loss: 0.755640\n",
      "Epoch 11_30.437%:  Training average Loss: 0.755768\n",
      "Epoch 11_31.238%:  Training average Loss: 0.756598\n",
      "Epoch 11_32.039%:  Training average Loss: 0.757083\n",
      "Epoch 11_32.840%:  Training average Loss: 0.757132\n",
      "Epoch 11_33.641%:  Training average Loss: 0.756780\n",
      "Epoch 11_34.442%:  Training average Loss: 0.756875\n",
      "Epoch 11_35.243%:  Training average Loss: 0.756342\n",
      "Epoch 11_36.044%:  Training average Loss: 0.757278\n",
      "Epoch 11_36.845%:  Training average Loss: 0.756959\n",
      "Epoch 11_37.645%:  Training average Loss: 0.756951\n",
      "Epoch 11_38.446%:  Training average Loss: 0.757403\n",
      "Epoch 11_39.247%:  Training average Loss: 0.757744\n",
      "Epoch 11_40.048%:  Training average Loss: 0.758228\n",
      "Epoch 11_40.849%:  Training average Loss: 0.758102\n",
      "Epoch 11_41.650%:  Training average Loss: 0.757957\n",
      "Epoch 11_42.451%:  Training average Loss: 0.758562\n",
      "Epoch 11_43.252%:  Training average Loss: 0.758538\n",
      "Epoch 11_44.053%:  Training average Loss: 0.757969\n",
      "Epoch 11_44.854%:  Training average Loss: 0.758038\n",
      "Epoch 11_45.655%:  Training average Loss: 0.759559\n",
      "Epoch 11_46.456%:  Training average Loss: 0.759955\n",
      "Epoch 11_47.257%:  Training average Loss: 0.760361\n",
      "Epoch 11_48.058%:  Training average Loss: 0.760742\n",
      "Epoch 11_48.859%:  Training average Loss: 0.760706\n",
      "Epoch 11_49.660%:  Training average Loss: 0.760967\n",
      "Epoch 11_50.461%:  Training average Loss: 0.760687\n",
      "Epoch 11_51.262%:  Training average Loss: 0.760833\n",
      "Epoch 11_52.063%:  Training average Loss: 0.761744\n",
      "Epoch 11_52.864%:  Training average Loss: 0.761473\n",
      "Epoch 11_53.665%:  Training average Loss: 0.762234\n",
      "Epoch 11_54.466%:  Training average Loss: 0.762481\n",
      "Epoch 11_55.267%:  Training average Loss: 0.762495\n",
      "Epoch 11_56.068%:  Training average Loss: 0.762439\n",
      "Epoch 11_56.869%:  Training average Loss: 0.762581\n",
      "Epoch 11_57.670%:  Training average Loss: 0.762833\n",
      "Epoch 11_58.471%:  Training average Loss: 0.762833\n",
      "Epoch 11_59.272%:  Training average Loss: 0.762922\n",
      "Epoch 11_60.073%:  Training average Loss: 0.763154\n",
      "Epoch 11_60.874%:  Training average Loss: 0.763338\n",
      "Epoch 11_61.675%:  Training average Loss: 0.763260\n",
      "Epoch 11_62.475%:  Training average Loss: 0.763503\n",
      "Epoch 11_63.276%:  Training average Loss: 0.763345\n",
      "Epoch 11_64.077%:  Training average Loss: 0.763874\n",
      "Epoch 11_64.878%:  Training average Loss: 0.763988\n",
      "Epoch 11_65.679%:  Training average Loss: 0.764020\n",
      "Epoch 11_66.480%:  Training average Loss: 0.763817\n",
      "Epoch 11_67.281%:  Training average Loss: 0.763837\n",
      "Epoch 11_68.082%:  Training average Loss: 0.763480\n",
      "Epoch 11_68.883%:  Training average Loss: 0.763591\n",
      "Epoch 11_69.684%:  Training average Loss: 0.764011\n",
      "Epoch 11_70.485%:  Training average Loss: 0.764347\n",
      "Epoch 11_71.286%:  Training average Loss: 0.764121\n",
      "Epoch 11_72.087%:  Training average Loss: 0.763769\n",
      "Epoch 11_72.888%:  Training average Loss: 0.763781\n",
      "Epoch 11_73.689%:  Training average Loss: 0.763732\n",
      "Epoch 11_74.490%:  Training average Loss: 0.763608\n",
      "Epoch 11_75.291%:  Training average Loss: 0.763877\n",
      "Epoch 11_76.092%:  Training average Loss: 0.763546\n",
      "Epoch 11_76.893%:  Training average Loss: 0.763226\n",
      "Epoch 11_77.694%:  Training average Loss: 0.763390\n",
      "Epoch 11_78.495%:  Training average Loss: 0.763716\n",
      "Epoch 11_79.296%:  Training average Loss: 0.763671\n",
      "Epoch 11_80.097%:  Training average Loss: 0.763561\n",
      "Epoch 11_80.898%:  Training average Loss: 0.763431\n",
      "Epoch 11_81.699%:  Training average Loss: 0.764168\n",
      "Epoch 11_82.500%:  Training average Loss: 0.763825\n",
      "Epoch 11_83.301%:  Training average Loss: 0.764174\n",
      "Epoch 11_84.102%:  Training average Loss: 0.764237\n",
      "Epoch 11_84.903%:  Training average Loss: 0.764434\n",
      "Epoch 11_85.704%:  Training average Loss: 0.764562\n",
      "Epoch 11_86.504%:  Training average Loss: 0.764344\n",
      "Epoch 11_87.305%:  Training average Loss: 0.764655\n",
      "Epoch 11_88.106%:  Training average Loss: 0.764570\n",
      "Epoch 11_88.907%:  Training average Loss: 0.764783\n",
      "Epoch 11_89.708%:  Training average Loss: 0.765177\n",
      "Epoch 11_90.509%:  Training average Loss: 0.765547\n",
      "Epoch 11_91.310%:  Training average Loss: 0.765545\n",
      "Epoch 11_92.111%:  Training average Loss: 0.765733\n",
      "Epoch 11_92.912%:  Training average Loss: 0.765867\n",
      "Epoch 11_93.713%:  Training average Loss: 0.766239\n",
      "Epoch 11_94.514%:  Training average Loss: 0.766045\n",
      "Epoch 11_95.315%:  Training average Loss: 0.766252\n",
      "Epoch 11_96.116%:  Training average Loss: 0.766255\n",
      "Epoch 11_96.917%:  Training average Loss: 0.766458\n",
      "Epoch 11_97.718%:  Training average Loss: 0.766075\n",
      "Epoch 11_98.519%:  Training average Loss: 0.765722\n",
      "Epoch 11_99.320%:  Training average Loss: 0.765742\n",
      "Epoch 11 :  Verification average Loss: 0.845089, Verification accuracy: 65.142729%,Total Time:1930.068768\n",
      "Epoch 12_0.801%:  Training average Loss: 0.737944\n",
      "Epoch 12_1.602%:  Training average Loss: 0.730434\n",
      "Epoch 12_2.403%:  Training average Loss: 0.738306\n",
      "Epoch 12_3.204%:  Training average Loss: 0.741839\n",
      "Epoch 12_4.005%:  Training average Loss: 0.746615\n",
      "Epoch 12_4.806%:  Training average Loss: 0.749754\n",
      "Epoch 12_5.607%:  Training average Loss: 0.751594\n",
      "Epoch 12_6.408%:  Training average Loss: 0.751332\n",
      "Epoch 12_7.209%:  Training average Loss: 0.752574\n",
      "Epoch 12_8.010%:  Training average Loss: 0.757245\n",
      "Epoch 12_8.811%:  Training average Loss: 0.759035\n",
      "Epoch 12_9.612%:  Training average Loss: 0.758102\n",
      "Epoch 12_10.413%:  Training average Loss: 0.758563\n",
      "Epoch 12_11.214%:  Training average Loss: 0.758096\n",
      "Epoch 12_12.015%:  Training average Loss: 0.761431\n",
      "Epoch 12_12.815%:  Training average Loss: 0.757611\n",
      "Epoch 12_13.616%:  Training average Loss: 0.757075\n",
      "Epoch 12_14.417%:  Training average Loss: 0.759362\n",
      "Epoch 12_15.218%:  Training average Loss: 0.759783\n",
      "Epoch 12_16.019%:  Training average Loss: 0.758191\n",
      "Epoch 12_16.820%:  Training average Loss: 0.757066\n",
      "Epoch 12_17.621%:  Training average Loss: 0.757073\n",
      "Epoch 12_18.422%:  Training average Loss: 0.755802\n",
      "Epoch 12_19.223%:  Training average Loss: 0.755148\n",
      "Epoch 12_20.024%:  Training average Loss: 0.754715\n",
      "Epoch 12_20.825%:  Training average Loss: 0.755448\n",
      "Epoch 12_21.626%:  Training average Loss: 0.754036\n",
      "Epoch 12_22.427%:  Training average Loss: 0.754431\n",
      "Epoch 12_23.228%:  Training average Loss: 0.756149\n",
      "Epoch 12_24.029%:  Training average Loss: 0.757681\n",
      "Epoch 12_24.830%:  Training average Loss: 0.757660\n",
      "Epoch 12_25.631%:  Training average Loss: 0.758559\n",
      "Epoch 12_26.432%:  Training average Loss: 0.757722\n",
      "Epoch 12_27.233%:  Training average Loss: 0.756117\n",
      "Epoch 12_28.034%:  Training average Loss: 0.756108\n",
      "Epoch 12_28.835%:  Training average Loss: 0.757017\n",
      "Epoch 12_29.636%:  Training average Loss: 0.757237\n",
      "Epoch 12_30.437%:  Training average Loss: 0.757342\n",
      "Epoch 12_31.238%:  Training average Loss: 0.759035\n",
      "Epoch 12_32.039%:  Training average Loss: 0.758224\n",
      "Epoch 12_32.840%:  Training average Loss: 0.756829\n",
      "Epoch 12_33.641%:  Training average Loss: 0.757920\n",
      "Epoch 12_34.442%:  Training average Loss: 0.759636\n",
      "Epoch 12_35.243%:  Training average Loss: 0.759293\n",
      "Epoch 12_36.044%:  Training average Loss: 0.759381\n",
      "Epoch 12_36.845%:  Training average Loss: 0.759154\n",
      "Epoch 12_37.645%:  Training average Loss: 0.758025\n",
      "Epoch 12_38.446%:  Training average Loss: 0.757273\n",
      "Epoch 12_39.247%:  Training average Loss: 0.757621\n",
      "Epoch 12_40.048%:  Training average Loss: 0.757652\n",
      "Epoch 12_40.849%:  Training average Loss: 0.757149\n",
      "Epoch 12_41.650%:  Training average Loss: 0.757527\n",
      "Epoch 12_42.451%:  Training average Loss: 0.757239\n",
      "Epoch 12_43.252%:  Training average Loss: 0.756477\n",
      "Epoch 12_44.053%:  Training average Loss: 0.755784\n",
      "Epoch 12_44.854%:  Training average Loss: 0.756083\n",
      "Epoch 12_45.655%:  Training average Loss: 0.755804\n",
      "Epoch 12_46.456%:  Training average Loss: 0.756474\n",
      "Epoch 12_47.257%:  Training average Loss: 0.756295\n",
      "Epoch 12_48.058%:  Training average Loss: 0.756251\n",
      "Epoch 12_48.859%:  Training average Loss: 0.755905\n",
      "Epoch 12_49.660%:  Training average Loss: 0.756501\n",
      "Epoch 12_50.461%:  Training average Loss: 0.756465\n",
      "Epoch 12_51.262%:  Training average Loss: 0.756662\n",
      "Epoch 12_52.063%:  Training average Loss: 0.756840\n",
      "Epoch 12_52.864%:  Training average Loss: 0.756329\n",
      "Epoch 12_53.665%:  Training average Loss: 0.755843\n",
      "Epoch 12_54.466%:  Training average Loss: 0.756503\n",
      "Epoch 12_55.267%:  Training average Loss: 0.756184\n",
      "Epoch 12_56.068%:  Training average Loss: 0.756077\n",
      "Epoch 12_56.869%:  Training average Loss: 0.756358\n",
      "Epoch 12_57.670%:  Training average Loss: 0.756323\n",
      "Epoch 12_58.471%:  Training average Loss: 0.755600\n",
      "Epoch 12_59.272%:  Training average Loss: 0.755051\n",
      "Epoch 12_60.073%:  Training average Loss: 0.755679\n",
      "Epoch 12_60.874%:  Training average Loss: 0.756198\n",
      "Epoch 12_61.675%:  Training average Loss: 0.756320\n",
      "Epoch 12_62.475%:  Training average Loss: 0.756428\n",
      "Epoch 12_63.276%:  Training average Loss: 0.757073\n",
      "Epoch 12_64.077%:  Training average Loss: 0.757210\n",
      "Epoch 12_64.878%:  Training average Loss: 0.757054\n",
      "Epoch 12_65.679%:  Training average Loss: 0.756950\n",
      "Epoch 12_66.480%:  Training average Loss: 0.757310\n",
      "Epoch 12_67.281%:  Training average Loss: 0.757768\n",
      "Epoch 12_68.082%:  Training average Loss: 0.758360\n",
      "Epoch 12_68.883%:  Training average Loss: 0.758365\n",
      "Epoch 12_69.684%:  Training average Loss: 0.758266\n",
      "Epoch 12_70.485%:  Training average Loss: 0.758506\n",
      "Epoch 12_71.286%:  Training average Loss: 0.758797\n",
      "Epoch 12_72.087%:  Training average Loss: 0.759022\n",
      "Epoch 12_72.888%:  Training average Loss: 0.759064\n",
      "Epoch 12_73.689%:  Training average Loss: 0.758832\n",
      "Epoch 12_74.490%:  Training average Loss: 0.758571\n",
      "Epoch 12_75.291%:  Training average Loss: 0.758258\n",
      "Epoch 12_76.092%:  Training average Loss: 0.758230\n",
      "Epoch 12_76.893%:  Training average Loss: 0.758007\n",
      "Epoch 12_77.694%:  Training average Loss: 0.758433\n",
      "Epoch 12_78.495%:  Training average Loss: 0.758505\n",
      "Epoch 12_79.296%:  Training average Loss: 0.757746\n",
      "Epoch 12_80.097%:  Training average Loss: 0.757757\n",
      "Epoch 12_80.898%:  Training average Loss: 0.757505\n",
      "Epoch 12_81.699%:  Training average Loss: 0.757261\n",
      "Epoch 12_82.500%:  Training average Loss: 0.757494\n",
      "Epoch 12_83.301%:  Training average Loss: 0.757857\n",
      "Epoch 12_84.102%:  Training average Loss: 0.758056\n",
      "Epoch 12_84.903%:  Training average Loss: 0.758207\n",
      "Epoch 12_85.704%:  Training average Loss: 0.758547\n",
      "Epoch 12_86.504%:  Training average Loss: 0.758375\n",
      "Epoch 12_87.305%:  Training average Loss: 0.758317\n",
      "Epoch 12_88.106%:  Training average Loss: 0.758695\n",
      "Epoch 12_88.907%:  Training average Loss: 0.759198\n",
      "Epoch 12_89.708%:  Training average Loss: 0.759335\n",
      "Epoch 12_90.509%:  Training average Loss: 0.759458\n",
      "Epoch 12_91.310%:  Training average Loss: 0.759292\n",
      "Epoch 12_92.111%:  Training average Loss: 0.759400\n",
      "Epoch 12_92.912%:  Training average Loss: 0.759756\n",
      "Epoch 12_93.713%:  Training average Loss: 0.760109\n",
      "Epoch 12_94.514%:  Training average Loss: 0.760158\n",
      "Epoch 12_95.315%:  Training average Loss: 0.760283\n",
      "Epoch 12_96.116%:  Training average Loss: 0.760145\n",
      "Epoch 12_96.917%:  Training average Loss: 0.760249\n",
      "Epoch 12_97.718%:  Training average Loss: 0.760447\n",
      "Epoch 12_98.519%:  Training average Loss: 0.760221\n",
      "Epoch 12_99.320%:  Training average Loss: 0.760032\n",
      "Epoch 12 :  Verification average Loss: 0.853218, Verification accuracy: 65.350976%,Total Time:2098.098927\n",
      "Model is saved in model_dict/model_lstm/epoch_12_accuracy_0.653510\n",
      "Epoch 13_0.801%:  Training average Loss: 0.731017\n",
      "Epoch 13_1.602%:  Training average Loss: 0.710183\n",
      "Epoch 13_2.403%:  Training average Loss: 0.723950\n",
      "Epoch 13_3.204%:  Training average Loss: 0.724712\n",
      "Epoch 13_4.005%:  Training average Loss: 0.723425\n",
      "Epoch 13_4.806%:  Training average Loss: 0.723558\n",
      "Epoch 13_5.607%:  Training average Loss: 0.719622\n",
      "Epoch 13_6.408%:  Training average Loss: 0.720908\n",
      "Epoch 13_7.209%:  Training average Loss: 0.720647\n",
      "Epoch 13_8.010%:  Training average Loss: 0.722861\n",
      "Epoch 13_8.811%:  Training average Loss: 0.723066\n",
      "Epoch 13_9.612%:  Training average Loss: 0.723214\n",
      "Epoch 13_10.413%:  Training average Loss: 0.722528\n",
      "Epoch 13_11.214%:  Training average Loss: 0.725179\n",
      "Epoch 13_12.015%:  Training average Loss: 0.725227\n",
      "Epoch 13_12.815%:  Training average Loss: 0.726804\n",
      "Epoch 13_13.616%:  Training average Loss: 0.727184\n",
      "Epoch 13_14.417%:  Training average Loss: 0.727248\n",
      "Epoch 13_15.218%:  Training average Loss: 0.731078\n",
      "Epoch 13_16.019%:  Training average Loss: 0.731373\n",
      "Epoch 13_16.820%:  Training average Loss: 0.732477\n",
      "Epoch 13_17.621%:  Training average Loss: 0.732904\n",
      "Epoch 13_18.422%:  Training average Loss: 0.736315\n",
      "Epoch 13_19.223%:  Training average Loss: 0.738011\n",
      "Epoch 13_20.024%:  Training average Loss: 0.738563\n",
      "Epoch 13_20.825%:  Training average Loss: 0.740268\n",
      "Epoch 13_21.626%:  Training average Loss: 0.740341\n",
      "Epoch 13_22.427%:  Training average Loss: 0.741466\n",
      "Epoch 13_23.228%:  Training average Loss: 0.741064\n",
      "Epoch 13_24.029%:  Training average Loss: 0.741485\n",
      "Epoch 13_24.830%:  Training average Loss: 0.741162\n",
      "Epoch 13_25.631%:  Training average Loss: 0.741252\n",
      "Epoch 13_26.432%:  Training average Loss: 0.740809\n",
      "Epoch 13_27.233%:  Training average Loss: 0.740464\n",
      "Epoch 13_28.034%:  Training average Loss: 0.740690\n",
      "Epoch 13_28.835%:  Training average Loss: 0.740644\n",
      "Epoch 13_29.636%:  Training average Loss: 0.740221\n",
      "Epoch 13_30.437%:  Training average Loss: 0.740641\n",
      "Epoch 13_31.238%:  Training average Loss: 0.740756\n",
      "Epoch 13_32.039%:  Training average Loss: 0.741857\n",
      "Epoch 13_32.840%:  Training average Loss: 0.742754\n",
      "Epoch 13_33.641%:  Training average Loss: 0.742132\n",
      "Epoch 13_34.442%:  Training average Loss: 0.742029\n",
      "Epoch 13_35.243%:  Training average Loss: 0.743349\n",
      "Epoch 13_36.044%:  Training average Loss: 0.742416\n",
      "Epoch 13_36.845%:  Training average Loss: 0.742209\n",
      "Epoch 13_37.645%:  Training average Loss: 0.742420\n",
      "Epoch 13_38.446%:  Training average Loss: 0.742594\n",
      "Epoch 13_39.247%:  Training average Loss: 0.743306\n",
      "Epoch 13_40.048%:  Training average Loss: 0.743657\n",
      "Epoch 13_40.849%:  Training average Loss: 0.743910\n",
      "Epoch 13_41.650%:  Training average Loss: 0.744899\n",
      "Epoch 13_42.451%:  Training average Loss: 0.744767\n",
      "Epoch 13_43.252%:  Training average Loss: 0.745273\n",
      "Epoch 13_44.053%:  Training average Loss: 0.746083\n",
      "Epoch 13_44.854%:  Training average Loss: 0.745993\n",
      "Epoch 13_45.655%:  Training average Loss: 0.746280\n",
      "Epoch 13_46.456%:  Training average Loss: 0.746253\n",
      "Epoch 13_47.257%:  Training average Loss: 0.746465\n",
      "Epoch 13_48.058%:  Training average Loss: 0.746648\n",
      "Epoch 13_48.859%:  Training average Loss: 0.745861\n",
      "Epoch 13_49.660%:  Training average Loss: 0.745275\n",
      "Epoch 13_50.461%:  Training average Loss: 0.744931\n",
      "Epoch 13_51.262%:  Training average Loss: 0.745157\n",
      "Epoch 13_52.063%:  Training average Loss: 0.745289\n",
      "Epoch 13_52.864%:  Training average Loss: 0.745261\n",
      "Epoch 13_53.665%:  Training average Loss: 0.745421\n",
      "Epoch 13_54.466%:  Training average Loss: 0.745899\n",
      "Epoch 13_55.267%:  Training average Loss: 0.746173\n",
      "Epoch 13_56.068%:  Training average Loss: 0.746116\n",
      "Epoch 13_56.869%:  Training average Loss: 0.746070\n",
      "Epoch 13_57.670%:  Training average Loss: 0.746149\n",
      "Epoch 13_58.471%:  Training average Loss: 0.745780\n",
      "Epoch 13_59.272%:  Training average Loss: 0.745789\n",
      "Epoch 13_60.073%:  Training average Loss: 0.745959\n",
      "Epoch 13_60.874%:  Training average Loss: 0.746087\n",
      "Epoch 13_61.675%:  Training average Loss: 0.746154\n",
      "Epoch 13_62.475%:  Training average Loss: 0.746156\n",
      "Epoch 13_63.276%:  Training average Loss: 0.746386\n",
      "Epoch 13_64.077%:  Training average Loss: 0.746685\n",
      "Epoch 13_64.878%:  Training average Loss: 0.746761\n",
      "Epoch 13_65.679%:  Training average Loss: 0.747030\n",
      "Epoch 13_66.480%:  Training average Loss: 0.747222\n",
      "Epoch 13_67.281%:  Training average Loss: 0.747374\n",
      "Epoch 13_68.082%:  Training average Loss: 0.747215\n",
      "Epoch 13_68.883%:  Training average Loss: 0.747454\n",
      "Epoch 13_69.684%:  Training average Loss: 0.747776\n",
      "Epoch 13_70.485%:  Training average Loss: 0.748214\n",
      "Epoch 13_71.286%:  Training average Loss: 0.748404\n",
      "Epoch 13_72.087%:  Training average Loss: 0.748293\n",
      "Epoch 13_72.888%:  Training average Loss: 0.748488\n",
      "Epoch 13_73.689%:  Training average Loss: 0.748713\n",
      "Epoch 13_74.490%:  Training average Loss: 0.748655\n",
      "Epoch 13_75.291%:  Training average Loss: 0.748449\n",
      "Epoch 13_76.092%:  Training average Loss: 0.748788\n",
      "Epoch 13_76.893%:  Training average Loss: 0.748854\n",
      "Epoch 13_77.694%:  Training average Loss: 0.749161\n",
      "Epoch 13_78.495%:  Training average Loss: 0.749000\n",
      "Epoch 13_79.296%:  Training average Loss: 0.749151\n",
      "Epoch 13_80.097%:  Training average Loss: 0.749377\n",
      "Epoch 13_80.898%:  Training average Loss: 0.749742\n",
      "Epoch 13_81.699%:  Training average Loss: 0.749954\n",
      "Epoch 13_82.500%:  Training average Loss: 0.750139\n",
      "Epoch 13_83.301%:  Training average Loss: 0.750034\n",
      "Epoch 13_84.102%:  Training average Loss: 0.750251\n",
      "Epoch 13_84.903%:  Training average Loss: 0.749833\n",
      "Epoch 13_85.704%:  Training average Loss: 0.750210\n",
      "Epoch 13_86.504%:  Training average Loss: 0.750293\n",
      "Epoch 13_87.305%:  Training average Loss: 0.750477\n",
      "Epoch 13_88.106%:  Training average Loss: 0.750693\n",
      "Epoch 13_88.907%:  Training average Loss: 0.750358\n",
      "Epoch 13_89.708%:  Training average Loss: 0.750399\n",
      "Epoch 13_90.509%:  Training average Loss: 0.750521\n",
      "Epoch 13_91.310%:  Training average Loss: 0.750398\n",
      "Epoch 13_92.111%:  Training average Loss: 0.750622\n",
      "Epoch 13_92.912%:  Training average Loss: 0.750964\n",
      "Epoch 13_93.713%:  Training average Loss: 0.751098\n",
      "Epoch 13_94.514%:  Training average Loss: 0.751587\n",
      "Epoch 13_95.315%:  Training average Loss: 0.751424\n",
      "Epoch 13_96.116%:  Training average Loss: 0.751787\n",
      "Epoch 13_96.917%:  Training average Loss: 0.751895\n",
      "Epoch 13_97.718%:  Training average Loss: 0.751864\n",
      "Epoch 13_98.519%:  Training average Loss: 0.751932\n",
      "Epoch 13_99.320%:  Training average Loss: 0.752060\n",
      "Epoch 13 :  Verification average Loss: 0.843477, Verification accuracy: 65.386217%,Total Time:2261.413419\n",
      "Model is saved in model_dict/model_lstm/epoch_13_accuracy_0.653862\n",
      "Epoch 14_0.801%:  Training average Loss: 0.736629\n",
      "Epoch 14_1.602%:  Training average Loss: 0.707034\n",
      "Epoch 14_2.403%:  Training average Loss: 0.723590\n",
      "Epoch 14_3.204%:  Training average Loss: 0.730368\n",
      "Epoch 14_4.005%:  Training average Loss: 0.724816\n",
      "Epoch 14_4.806%:  Training average Loss: 0.725699\n",
      "Epoch 14_5.607%:  Training average Loss: 0.726890\n",
      "Epoch 14_6.408%:  Training average Loss: 0.734184\n",
      "Epoch 14_7.209%:  Training average Loss: 0.730747\n",
      "Epoch 14_8.010%:  Training average Loss: 0.731635\n",
      "Epoch 14_8.811%:  Training average Loss: 0.733991\n",
      "Epoch 14_9.612%:  Training average Loss: 0.733605\n",
      "Epoch 14_10.413%:  Training average Loss: 0.733474\n",
      "Epoch 14_11.214%:  Training average Loss: 0.739090\n",
      "Epoch 14_12.015%:  Training average Loss: 0.738745\n",
      "Epoch 14_12.815%:  Training average Loss: 0.739699\n",
      "Epoch 14_13.616%:  Training average Loss: 0.739614\n",
      "Epoch 14_14.417%:  Training average Loss: 0.738948\n",
      "Epoch 14_15.218%:  Training average Loss: 0.741348\n",
      "Epoch 14_16.019%:  Training average Loss: 0.740244\n",
      "Epoch 14_16.820%:  Training average Loss: 0.740851\n",
      "Epoch 14_17.621%:  Training average Loss: 0.741206\n",
      "Epoch 14_18.422%:  Training average Loss: 0.742366\n",
      "Epoch 14_19.223%:  Training average Loss: 0.741624\n",
      "Epoch 14_20.024%:  Training average Loss: 0.740806\n",
      "Epoch 14_20.825%:  Training average Loss: 0.740013\n",
      "Epoch 14_21.626%:  Training average Loss: 0.739029\n",
      "Epoch 14_22.427%:  Training average Loss: 0.738846\n",
      "Epoch 14_23.228%:  Training average Loss: 0.740592\n",
      "Epoch 14_24.029%:  Training average Loss: 0.739761\n",
      "Epoch 14_24.830%:  Training average Loss: 0.739450\n",
      "Epoch 14_25.631%:  Training average Loss: 0.738407\n",
      "Epoch 14_26.432%:  Training average Loss: 0.738743\n",
      "Epoch 14_27.233%:  Training average Loss: 0.739595\n",
      "Epoch 14_28.034%:  Training average Loss: 0.740247\n",
      "Epoch 14_28.835%:  Training average Loss: 0.739737\n",
      "Epoch 14_29.636%:  Training average Loss: 0.739292\n",
      "Epoch 14_30.437%:  Training average Loss: 0.739491\n",
      "Epoch 14_31.238%:  Training average Loss: 0.738165\n",
      "Epoch 14_32.039%:  Training average Loss: 0.738582\n",
      "Epoch 14_32.840%:  Training average Loss: 0.739035\n",
      "Epoch 14_33.641%:  Training average Loss: 0.739039\n",
      "Epoch 14_34.442%:  Training average Loss: 0.738904\n",
      "Epoch 14_35.243%:  Training average Loss: 0.737812\n",
      "Epoch 14_36.044%:  Training average Loss: 0.738141\n",
      "Epoch 14_36.845%:  Training average Loss: 0.738436\n",
      "Epoch 14_37.645%:  Training average Loss: 0.738523\n",
      "Epoch 14_38.446%:  Training average Loss: 0.738476\n",
      "Epoch 14_39.247%:  Training average Loss: 0.739406\n",
      "Epoch 14_40.048%:  Training average Loss: 0.738557\n",
      "Epoch 14_40.849%:  Training average Loss: 0.738630\n",
      "Epoch 14_41.650%:  Training average Loss: 0.738393\n",
      "Epoch 14_42.451%:  Training average Loss: 0.737569\n",
      "Epoch 14_43.252%:  Training average Loss: 0.737171\n",
      "Epoch 14_44.053%:  Training average Loss: 0.737187\n",
      "Epoch 14_44.854%:  Training average Loss: 0.737269\n",
      "Epoch 14_45.655%:  Training average Loss: 0.737817\n",
      "Epoch 14_46.456%:  Training average Loss: 0.738050\n",
      "Epoch 14_47.257%:  Training average Loss: 0.738715\n",
      "Epoch 14_48.058%:  Training average Loss: 0.738980\n",
      "Epoch 14_48.859%:  Training average Loss: 0.738979\n",
      "Epoch 14_49.660%:  Training average Loss: 0.739073\n",
      "Epoch 14_50.461%:  Training average Loss: 0.738766\n",
      "Epoch 14_51.262%:  Training average Loss: 0.739008\n",
      "Epoch 14_52.063%:  Training average Loss: 0.739159\n",
      "Epoch 14_52.864%:  Training average Loss: 0.738737\n",
      "Epoch 14_53.665%:  Training average Loss: 0.738823\n",
      "Epoch 14_54.466%:  Training average Loss: 0.739054\n",
      "Epoch 14_55.267%:  Training average Loss: 0.739001\n",
      "Epoch 14_56.068%:  Training average Loss: 0.739320\n",
      "Epoch 14_56.869%:  Training average Loss: 0.739783\n",
      "Epoch 14_57.670%:  Training average Loss: 0.739709\n",
      "Epoch 14_58.471%:  Training average Loss: 0.739383\n",
      "Epoch 14_59.272%:  Training average Loss: 0.739423\n",
      "Epoch 14_60.073%:  Training average Loss: 0.739821\n",
      "Epoch 14_60.874%:  Training average Loss: 0.739709\n",
      "Epoch 14_61.675%:  Training average Loss: 0.739711\n",
      "Epoch 14_62.475%:  Training average Loss: 0.739565\n",
      "Epoch 14_63.276%:  Training average Loss: 0.739750\n",
      "Epoch 14_64.077%:  Training average Loss: 0.740368\n",
      "Epoch 14_64.878%:  Training average Loss: 0.740199\n",
      "Epoch 14_65.679%:  Training average Loss: 0.740203\n",
      "Epoch 14_66.480%:  Training average Loss: 0.739984\n",
      "Epoch 14_67.281%:  Training average Loss: 0.740022\n",
      "Epoch 14_68.082%:  Training average Loss: 0.739654\n",
      "Epoch 14_68.883%:  Training average Loss: 0.739967\n",
      "Epoch 14_69.684%:  Training average Loss: 0.740356\n",
      "Epoch 14_70.485%:  Training average Loss: 0.740671\n",
      "Epoch 14_71.286%:  Training average Loss: 0.740843\n",
      "Epoch 14_72.087%:  Training average Loss: 0.740756\n",
      "Epoch 14_72.888%:  Training average Loss: 0.741125\n",
      "Epoch 14_73.689%:  Training average Loss: 0.741365\n",
      "Epoch 14_74.490%:  Training average Loss: 0.741261\n",
      "Epoch 14_75.291%:  Training average Loss: 0.741484\n",
      "Epoch 14_76.092%:  Training average Loss: 0.741662\n",
      "Epoch 14_76.893%:  Training average Loss: 0.742201\n",
      "Epoch 14_77.694%:  Training average Loss: 0.742287\n",
      "Epoch 14_78.495%:  Training average Loss: 0.742428\n",
      "Epoch 14_79.296%:  Training average Loss: 0.742539\n",
      "Epoch 14_80.097%:  Training average Loss: 0.742352\n",
      "Epoch 14_80.898%:  Training average Loss: 0.742432\n",
      "Epoch 14_81.699%:  Training average Loss: 0.742604\n",
      "Epoch 14_82.500%:  Training average Loss: 0.742658\n",
      "Epoch 14_83.301%:  Training average Loss: 0.742288\n",
      "Epoch 14_84.102%:  Training average Loss: 0.742354\n",
      "Epoch 14_84.903%:  Training average Loss: 0.742579\n",
      "Epoch 14_85.704%:  Training average Loss: 0.742828\n",
      "Epoch 14_86.504%:  Training average Loss: 0.742941\n",
      "Epoch 14_87.305%:  Training average Loss: 0.743149\n",
      "Epoch 14_88.106%:  Training average Loss: 0.743201\n",
      "Epoch 14_88.907%:  Training average Loss: 0.742986\n",
      "Epoch 14_89.708%:  Training average Loss: 0.743010\n",
      "Epoch 14_90.509%:  Training average Loss: 0.743820\n",
      "Epoch 14_91.310%:  Training average Loss: 0.744025\n",
      "Epoch 14_92.111%:  Training average Loss: 0.744298\n",
      "Epoch 14_92.912%:  Training average Loss: 0.744551\n",
      "Epoch 14_93.713%:  Training average Loss: 0.744569\n",
      "Epoch 14_94.514%:  Training average Loss: 0.744680\n",
      "Epoch 14_95.315%:  Training average Loss: 0.744860\n",
      "Epoch 14_96.116%:  Training average Loss: 0.744836\n",
      "Epoch 14_96.917%:  Training average Loss: 0.745285\n",
      "Epoch 14_97.718%:  Training average Loss: 0.745353\n",
      "Epoch 14_98.519%:  Training average Loss: 0.745339\n",
      "Epoch 14_99.320%:  Training average Loss: 0.745368\n",
      "Epoch 14 :  Verification average Loss: 0.859274, Verification accuracy: 65.222824%,Total Time:2424.106803\n",
      "Epoch 15_0.801%:  Training average Loss: 0.710024\n",
      "Epoch 15_1.602%:  Training average Loss: 0.713771\n",
      "Epoch 15_2.403%:  Training average Loss: 0.720658\n",
      "Epoch 15_3.204%:  Training average Loss: 0.713469\n",
      "Epoch 15_4.005%:  Training average Loss: 0.709413\n",
      "Epoch 15_4.806%:  Training average Loss: 0.714692\n",
      "Epoch 15_5.607%:  Training average Loss: 0.720519\n",
      "Epoch 15_6.408%:  Training average Loss: 0.718413\n",
      "Epoch 15_7.209%:  Training average Loss: 0.715159\n",
      "Epoch 15_8.010%:  Training average Loss: 0.717892\n",
      "Epoch 15_8.811%:  Training average Loss: 0.720217\n",
      "Epoch 15_9.612%:  Training average Loss: 0.719223\n",
      "Epoch 15_10.413%:  Training average Loss: 0.721378\n",
      "Epoch 15_11.214%:  Training average Loss: 0.723377\n",
      "Epoch 15_12.015%:  Training average Loss: 0.727144\n",
      "Epoch 15_12.815%:  Training average Loss: 0.729161\n",
      "Epoch 15_13.616%:  Training average Loss: 0.731430\n",
      "Epoch 15_14.417%:  Training average Loss: 0.732509\n",
      "Epoch 15_15.218%:  Training average Loss: 0.731490\n",
      "Epoch 15_16.019%:  Training average Loss: 0.730928\n",
      "Epoch 15_16.820%:  Training average Loss: 0.729916\n",
      "Epoch 15_17.621%:  Training average Loss: 0.732283\n",
      "Epoch 15_18.422%:  Training average Loss: 0.732054\n",
      "Epoch 15_19.223%:  Training average Loss: 0.731139\n",
      "Epoch 15_20.024%:  Training average Loss: 0.732403\n",
      "Epoch 15_20.825%:  Training average Loss: 0.731814\n",
      "Epoch 15_21.626%:  Training average Loss: 0.733263\n",
      "Epoch 15_22.427%:  Training average Loss: 0.733394\n",
      "Epoch 15_23.228%:  Training average Loss: 0.734053\n",
      "Epoch 15_24.029%:  Training average Loss: 0.733674\n",
      "Epoch 15_24.830%:  Training average Loss: 0.732736\n",
      "Epoch 15_25.631%:  Training average Loss: 0.732902\n",
      "Epoch 15_26.432%:  Training average Loss: 0.733237\n",
      "Epoch 15_27.233%:  Training average Loss: 0.733636\n",
      "Epoch 15_28.034%:  Training average Loss: 0.732930\n",
      "Epoch 15_28.835%:  Training average Loss: 0.733721\n",
      "Epoch 15_29.636%:  Training average Loss: 0.735433\n",
      "Epoch 15_30.437%:  Training average Loss: 0.734704\n",
      "Epoch 15_31.238%:  Training average Loss: 0.735134\n",
      "Epoch 15_32.039%:  Training average Loss: 0.735492\n",
      "Epoch 15_32.840%:  Training average Loss: 0.735794\n",
      "Epoch 15_33.641%:  Training average Loss: 0.735012\n",
      "Epoch 15_34.442%:  Training average Loss: 0.735702\n",
      "Epoch 15_35.243%:  Training average Loss: 0.736136\n",
      "Epoch 15_36.044%:  Training average Loss: 0.735487\n",
      "Epoch 15_36.845%:  Training average Loss: 0.735045\n",
      "Epoch 15_37.645%:  Training average Loss: 0.735685\n",
      "Epoch 15_38.446%:  Training average Loss: 0.736284\n",
      "Epoch 15_39.247%:  Training average Loss: 0.736256\n",
      "Epoch 15_40.048%:  Training average Loss: 0.736490\n",
      "Epoch 15_40.849%:  Training average Loss: 0.736087\n",
      "Epoch 15_41.650%:  Training average Loss: 0.734887\n",
      "Epoch 15_42.451%:  Training average Loss: 0.734547\n",
      "Epoch 15_43.252%:  Training average Loss: 0.734546\n",
      "Epoch 15_44.053%:  Training average Loss: 0.734642\n",
      "Epoch 15_44.854%:  Training average Loss: 0.734999\n",
      "Epoch 15_45.655%:  Training average Loss: 0.735279\n",
      "Epoch 15_46.456%:  Training average Loss: 0.735136\n",
      "Epoch 15_47.257%:  Training average Loss: 0.735374\n",
      "Epoch 15_48.058%:  Training average Loss: 0.734862\n",
      "Epoch 15_48.859%:  Training average Loss: 0.735330\n",
      "Epoch 15_49.660%:  Training average Loss: 0.734722\n",
      "Epoch 15_50.461%:  Training average Loss: 0.733909\n",
      "Epoch 15_51.262%:  Training average Loss: 0.734075\n",
      "Epoch 15_52.063%:  Training average Loss: 0.735307\n",
      "Epoch 15_52.864%:  Training average Loss: 0.735490\n",
      "Epoch 15_53.665%:  Training average Loss: 0.735702\n",
      "Epoch 15_54.466%:  Training average Loss: 0.735841\n",
      "Epoch 15_55.267%:  Training average Loss: 0.735731\n",
      "Epoch 15_56.068%:  Training average Loss: 0.736006\n",
      "Epoch 15_56.869%:  Training average Loss: 0.736214\n",
      "Epoch 15_57.670%:  Training average Loss: 0.735630\n",
      "Epoch 15_58.471%:  Training average Loss: 0.736201\n",
      "Epoch 15_59.272%:  Training average Loss: 0.736443\n",
      "Epoch 15_60.073%:  Training average Loss: 0.736541\n",
      "Epoch 15_60.874%:  Training average Loss: 0.736204\n",
      "Epoch 15_61.675%:  Training average Loss: 0.736723\n",
      "Epoch 15_62.475%:  Training average Loss: 0.737232\n",
      "Epoch 15_63.276%:  Training average Loss: 0.737244\n",
      "Epoch 15_64.077%:  Training average Loss: 0.736800\n",
      "Epoch 15_64.878%:  Training average Loss: 0.736897\n",
      "Epoch 15_65.679%:  Training average Loss: 0.737225\n",
      "Epoch 15_66.480%:  Training average Loss: 0.737307\n",
      "Epoch 15_67.281%:  Training average Loss: 0.737534\n",
      "Epoch 15_68.082%:  Training average Loss: 0.738102\n",
      "Epoch 15_68.883%:  Training average Loss: 0.738414\n",
      "Epoch 15_69.684%:  Training average Loss: 0.738603\n",
      "Epoch 15_70.485%:  Training average Loss: 0.738340\n",
      "Epoch 15_71.286%:  Training average Loss: 0.737952\n",
      "Epoch 15_72.087%:  Training average Loss: 0.737313\n",
      "Epoch 15_72.888%:  Training average Loss: 0.736894\n",
      "Epoch 15_73.689%:  Training average Loss: 0.736750\n",
      "Epoch 15_74.490%:  Training average Loss: 0.737253\n",
      "Epoch 15_75.291%:  Training average Loss: 0.737215\n",
      "Epoch 15_76.092%:  Training average Loss: 0.737516\n",
      "Epoch 15_76.893%:  Training average Loss: 0.737309\n",
      "Epoch 15_77.694%:  Training average Loss: 0.737367\n",
      "Epoch 15_78.495%:  Training average Loss: 0.737653\n",
      "Epoch 15_79.296%:  Training average Loss: 0.738085\n",
      "Epoch 15_80.097%:  Training average Loss: 0.737666\n",
      "Epoch 15_80.898%:  Training average Loss: 0.737769\n",
      "Epoch 15_81.699%:  Training average Loss: 0.737938\n",
      "Epoch 15_82.500%:  Training average Loss: 0.737845\n",
      "Epoch 15_83.301%:  Training average Loss: 0.737826\n",
      "Epoch 15_84.102%:  Training average Loss: 0.738009\n",
      "Epoch 15_84.903%:  Training average Loss: 0.737628\n",
      "Epoch 15_85.704%:  Training average Loss: 0.737800\n",
      "Epoch 15_86.504%:  Training average Loss: 0.738044\n",
      "Epoch 15_87.305%:  Training average Loss: 0.737871\n",
      "Epoch 15_88.106%:  Training average Loss: 0.737998\n",
      "Epoch 15_88.907%:  Training average Loss: 0.737747\n",
      "Epoch 15_89.708%:  Training average Loss: 0.737826\n",
      "Epoch 15_90.509%:  Training average Loss: 0.738075\n",
      "Epoch 15_91.310%:  Training average Loss: 0.737912\n",
      "Epoch 15_92.111%:  Training average Loss: 0.738005\n",
      "Epoch 15_92.912%:  Training average Loss: 0.737837\n",
      "Epoch 15_93.713%:  Training average Loss: 0.737881\n",
      "Epoch 15_94.514%:  Training average Loss: 0.738054\n",
      "Epoch 15_95.315%:  Training average Loss: 0.737720\n",
      "Epoch 15_96.116%:  Training average Loss: 0.737859\n",
      "Epoch 15_96.917%:  Training average Loss: 0.737821\n",
      "Epoch 15_97.718%:  Training average Loss: 0.738275\n",
      "Epoch 15_98.519%:  Training average Loss: 0.738300\n",
      "Epoch 15_99.320%:  Training average Loss: 0.738673\n",
      "Epoch 15 :  Verification average Loss: 0.847372, Verification accuracy: 65.161952%,Total Time:2584.269867\n",
      "Epoch 16_0.801%:  Training average Loss: 0.763477\n",
      "Epoch 16_1.602%:  Training average Loss: 0.742158\n",
      "Epoch 16_2.403%:  Training average Loss: 0.735533\n",
      "Epoch 16_3.204%:  Training average Loss: 0.737637\n",
      "Epoch 16_4.005%:  Training average Loss: 0.726683\n",
      "Epoch 16_4.806%:  Training average Loss: 0.725484\n",
      "Epoch 16_5.607%:  Training average Loss: 0.727156\n",
      "Epoch 16_6.408%:  Training average Loss: 0.721491\n",
      "Epoch 16_7.209%:  Training average Loss: 0.721470\n",
      "Epoch 16_8.010%:  Training average Loss: 0.725004\n",
      "Epoch 16_8.811%:  Training average Loss: 0.726853\n",
      "Epoch 16_9.612%:  Training average Loss: 0.726666\n",
      "Epoch 16_10.413%:  Training average Loss: 0.726027\n",
      "Epoch 16_11.214%:  Training average Loss: 0.724800\n",
      "Epoch 16_12.015%:  Training average Loss: 0.727415\n",
      "Epoch 16_12.815%:  Training average Loss: 0.725440\n",
      "Epoch 16_13.616%:  Training average Loss: 0.725968\n",
      "Epoch 16_14.417%:  Training average Loss: 0.726885\n",
      "Epoch 16_15.218%:  Training average Loss: 0.728534\n",
      "Epoch 16_16.019%:  Training average Loss: 0.730037\n",
      "Epoch 16_16.820%:  Training average Loss: 0.728627\n",
      "Epoch 16_17.621%:  Training average Loss: 0.731017\n",
      "Epoch 16_18.422%:  Training average Loss: 0.729162\n",
      "Epoch 16_19.223%:  Training average Loss: 0.729326\n",
      "Epoch 16_20.024%:  Training average Loss: 0.729520\n",
      "Epoch 16_20.825%:  Training average Loss: 0.728607\n",
      "Epoch 16_21.626%:  Training average Loss: 0.729102\n",
      "Epoch 16_22.427%:  Training average Loss: 0.728363\n",
      "Epoch 16_23.228%:  Training average Loss: 0.729485\n",
      "Epoch 16_24.029%:  Training average Loss: 0.728749\n",
      "Epoch 16_24.830%:  Training average Loss: 0.729138\n",
      "Epoch 16_25.631%:  Training average Loss: 0.729429\n",
      "Epoch 16_26.432%:  Training average Loss: 0.730715\n",
      "Epoch 16_27.233%:  Training average Loss: 0.730745\n",
      "Epoch 16_28.034%:  Training average Loss: 0.732008\n",
      "Epoch 16_28.835%:  Training average Loss: 0.731246\n",
      "Epoch 16_29.636%:  Training average Loss: 0.730576\n",
      "Epoch 16_30.437%:  Training average Loss: 0.731419\n",
      "Epoch 16_31.238%:  Training average Loss: 0.731783\n",
      "Epoch 16_32.039%:  Training average Loss: 0.732180\n",
      "Epoch 16_32.840%:  Training average Loss: 0.732251\n",
      "Epoch 16_33.641%:  Training average Loss: 0.732025\n",
      "Epoch 16_34.442%:  Training average Loss: 0.731416\n",
      "Epoch 16_35.243%:  Training average Loss: 0.730284\n",
      "Epoch 16_36.044%:  Training average Loss: 0.730016\n",
      "Epoch 16_36.845%:  Training average Loss: 0.729377\n",
      "Epoch 16_37.645%:  Training average Loss: 0.728932\n",
      "Epoch 16_38.446%:  Training average Loss: 0.728655\n",
      "Epoch 16_39.247%:  Training average Loss: 0.730018\n",
      "Epoch 16_40.048%:  Training average Loss: 0.729716\n",
      "Epoch 16_40.849%:  Training average Loss: 0.729304\n",
      "Epoch 16_41.650%:  Training average Loss: 0.729600\n",
      "Epoch 16_42.451%:  Training average Loss: 0.730073\n",
      "Epoch 16_43.252%:  Training average Loss: 0.730328\n",
      "Epoch 16_44.053%:  Training average Loss: 0.730076\n",
      "Epoch 16_44.854%:  Training average Loss: 0.730323\n",
      "Epoch 16_45.655%:  Training average Loss: 0.730166\n",
      "Epoch 16_46.456%:  Training average Loss: 0.730545\n",
      "Epoch 16_47.257%:  Training average Loss: 0.730530\n",
      "Epoch 16_48.058%:  Training average Loss: 0.730433\n",
      "Epoch 16_48.859%:  Training average Loss: 0.730801\n",
      "Epoch 16_49.660%:  Training average Loss: 0.730712\n",
      "Epoch 16_50.461%:  Training average Loss: 0.730077\n",
      "Epoch 16_51.262%:  Training average Loss: 0.730693\n",
      "Epoch 16_52.063%:  Training average Loss: 0.730507\n",
      "Epoch 16_52.864%:  Training average Loss: 0.730079\n",
      "Epoch 16_53.665%:  Training average Loss: 0.730573\n",
      "Epoch 16_54.466%:  Training average Loss: 0.730745\n",
      "Epoch 16_55.267%:  Training average Loss: 0.731261\n",
      "Epoch 16_56.068%:  Training average Loss: 0.731879\n",
      "Epoch 16_56.869%:  Training average Loss: 0.731092\n",
      "Epoch 16_57.670%:  Training average Loss: 0.731046\n",
      "Epoch 16_58.471%:  Training average Loss: 0.730963\n",
      "Epoch 16_59.272%:  Training average Loss: 0.731009\n",
      "Epoch 16_60.073%:  Training average Loss: 0.731262\n",
      "Epoch 16_60.874%:  Training average Loss: 0.731635\n",
      "Epoch 16_61.675%:  Training average Loss: 0.731929\n",
      "Epoch 16_62.475%:  Training average Loss: 0.731831\n",
      "Epoch 16_63.276%:  Training average Loss: 0.732406\n",
      "Epoch 16_64.077%:  Training average Loss: 0.732739\n",
      "Epoch 16_64.878%:  Training average Loss: 0.732924\n",
      "Epoch 16_65.679%:  Training average Loss: 0.732850\n",
      "Epoch 16_66.480%:  Training average Loss: 0.732298\n",
      "Epoch 16_67.281%:  Training average Loss: 0.732201\n",
      "Epoch 16_68.082%:  Training average Loss: 0.732706\n",
      "Epoch 16_68.883%:  Training average Loss: 0.732553\n",
      "Epoch 16_69.684%:  Training average Loss: 0.732609\n",
      "Epoch 16_70.485%:  Training average Loss: 0.732714\n",
      "Epoch 16_71.286%:  Training average Loss: 0.732843\n",
      "Epoch 16_72.087%:  Training average Loss: 0.733195\n",
      "Epoch 16_72.888%:  Training average Loss: 0.733042\n",
      "Epoch 16_73.689%:  Training average Loss: 0.733240\n",
      "Epoch 16_74.490%:  Training average Loss: 0.732920\n",
      "Epoch 16_75.291%:  Training average Loss: 0.732570\n",
      "Epoch 16_76.092%:  Training average Loss: 0.732490\n",
      "Epoch 16_76.893%:  Training average Loss: 0.732388\n",
      "Epoch 16_77.694%:  Training average Loss: 0.732858\n",
      "Epoch 16_78.495%:  Training average Loss: 0.732919\n",
      "Epoch 16_79.296%:  Training average Loss: 0.732506\n",
      "Epoch 16_80.097%:  Training average Loss: 0.732322\n",
      "Epoch 16_80.898%:  Training average Loss: 0.732361\n",
      "Epoch 16_81.699%:  Training average Loss: 0.732919\n",
      "Epoch 16_82.500%:  Training average Loss: 0.733034\n",
      "Epoch 16_83.301%:  Training average Loss: 0.733248\n",
      "Epoch 16_84.102%:  Training average Loss: 0.733220\n",
      "Epoch 16_84.903%:  Training average Loss: 0.732924\n",
      "Epoch 16_85.704%:  Training average Loss: 0.733096\n",
      "Epoch 16_86.504%:  Training average Loss: 0.733331\n",
      "Epoch 16_87.305%:  Training average Loss: 0.733187\n",
      "Epoch 16_88.106%:  Training average Loss: 0.733003\n",
      "Epoch 16_88.907%:  Training average Loss: 0.733160\n",
      "Epoch 16_89.708%:  Training average Loss: 0.732674\n",
      "Epoch 16_90.509%:  Training average Loss: 0.732727\n",
      "Epoch 16_91.310%:  Training average Loss: 0.733252\n",
      "Epoch 16_92.111%:  Training average Loss: 0.733463\n",
      "Epoch 16_92.912%:  Training average Loss: 0.733572\n",
      "Epoch 16_93.713%:  Training average Loss: 0.733632\n",
      "Epoch 16_94.514%:  Training average Loss: 0.733610\n",
      "Epoch 16_95.315%:  Training average Loss: 0.733426\n",
      "Epoch 16_96.116%:  Training average Loss: 0.734070\n",
      "Epoch 16_96.917%:  Training average Loss: 0.734015\n",
      "Epoch 16_97.718%:  Training average Loss: 0.733830\n",
      "Epoch 16_98.519%:  Training average Loss: 0.733800\n",
      "Epoch 16_99.320%:  Training average Loss: 0.733889\n",
      "Epoch 16 :  Verification average Loss: 0.857021, Verification accuracy: 65.046615%,Total Time:2745.419491\n",
      "Epoch 17_0.801%:  Training average Loss: 0.739991\n",
      "Epoch 17_1.602%:  Training average Loss: 0.737446\n",
      "Epoch 17_2.403%:  Training average Loss: 0.733458\n",
      "Epoch 17_3.204%:  Training average Loss: 0.721522\n",
      "Epoch 17_4.005%:  Training average Loss: 0.726953\n",
      "Epoch 17_4.806%:  Training average Loss: 0.729987\n",
      "Epoch 17_5.607%:  Training average Loss: 0.725869\n",
      "Epoch 17_6.408%:  Training average Loss: 0.724754\n",
      "Epoch 17_7.209%:  Training average Loss: 0.723224\n",
      "Epoch 17_8.010%:  Training average Loss: 0.723940\n",
      "Epoch 17_8.811%:  Training average Loss: 0.723524\n",
      "Epoch 17_9.612%:  Training average Loss: 0.721975\n",
      "Epoch 17_10.413%:  Training average Loss: 0.723810\n",
      "Epoch 17_11.214%:  Training average Loss: 0.724035\n",
      "Epoch 17_12.015%:  Training average Loss: 0.724897\n",
      "Epoch 17_12.815%:  Training average Loss: 0.725535\n",
      "Epoch 17_13.616%:  Training average Loss: 0.724558\n",
      "Epoch 17_14.417%:  Training average Loss: 0.722206\n",
      "Epoch 17_15.218%:  Training average Loss: 0.720521\n",
      "Epoch 17_16.019%:  Training average Loss: 0.721447\n",
      "Epoch 17_16.820%:  Training average Loss: 0.720437\n",
      "Epoch 17_17.621%:  Training average Loss: 0.719056\n",
      "Epoch 17_18.422%:  Training average Loss: 0.719639\n",
      "Epoch 17_19.223%:  Training average Loss: 0.718062\n",
      "Epoch 17_20.024%:  Training average Loss: 0.720087\n",
      "Epoch 17_20.825%:  Training average Loss: 0.720459\n",
      "Epoch 17_21.626%:  Training average Loss: 0.721178\n",
      "Epoch 17_22.427%:  Training average Loss: 0.722274\n",
      "Epoch 17_23.228%:  Training average Loss: 0.722582\n",
      "Epoch 17_24.029%:  Training average Loss: 0.723808\n",
      "Epoch 17_24.830%:  Training average Loss: 0.721872\n",
      "Epoch 17_25.631%:  Training average Loss: 0.721038\n",
      "Epoch 17_26.432%:  Training average Loss: 0.721241\n",
      "Epoch 17_27.233%:  Training average Loss: 0.721617\n",
      "Epoch 17_28.034%:  Training average Loss: 0.720674\n",
      "Epoch 17_28.835%:  Training average Loss: 0.722238\n",
      "Epoch 17_29.636%:  Training average Loss: 0.722151\n",
      "Epoch 17_30.437%:  Training average Loss: 0.722975\n",
      "Epoch 17_31.238%:  Training average Loss: 0.722558\n",
      "Epoch 17_32.039%:  Training average Loss: 0.722620\n",
      "Epoch 17_32.840%:  Training average Loss: 0.723296\n",
      "Epoch 17_33.641%:  Training average Loss: 0.723099\n",
      "Epoch 17_34.442%:  Training average Loss: 0.724057\n",
      "Epoch 17_35.243%:  Training average Loss: 0.723584\n",
      "Epoch 17_36.044%:  Training average Loss: 0.723180\n",
      "Epoch 17_36.845%:  Training average Loss: 0.722821\n",
      "Epoch 17_37.645%:  Training average Loss: 0.722363\n",
      "Epoch 17_38.446%:  Training average Loss: 0.723121\n",
      "Epoch 17_39.247%:  Training average Loss: 0.723259\n",
      "Epoch 17_40.048%:  Training average Loss: 0.722872\n",
      "Epoch 17_40.849%:  Training average Loss: 0.722911\n",
      "Epoch 17_41.650%:  Training average Loss: 0.722108\n",
      "Epoch 17_42.451%:  Training average Loss: 0.721950\n",
      "Epoch 17_43.252%:  Training average Loss: 0.723142\n",
      "Epoch 17_44.053%:  Training average Loss: 0.723124\n",
      "Epoch 17_44.854%:  Training average Loss: 0.723855\n",
      "Epoch 17_45.655%:  Training average Loss: 0.723548\n",
      "Epoch 17_46.456%:  Training average Loss: 0.723982\n",
      "Epoch 17_47.257%:  Training average Loss: 0.723020\n",
      "Epoch 17_48.058%:  Training average Loss: 0.723058\n",
      "Epoch 17_48.859%:  Training average Loss: 0.722675\n",
      "Epoch 17_49.660%:  Training average Loss: 0.722750\n",
      "Epoch 17_50.461%:  Training average Loss: 0.722400\n",
      "Epoch 17_51.262%:  Training average Loss: 0.723133\n",
      "Epoch 17_52.063%:  Training average Loss: 0.722750\n",
      "Epoch 17_52.864%:  Training average Loss: 0.723227\n",
      "Epoch 17_53.665%:  Training average Loss: 0.723393\n",
      "Epoch 17_54.466%:  Training average Loss: 0.723649\n",
      "Epoch 17_55.267%:  Training average Loss: 0.723764\n",
      "Epoch 17_56.068%:  Training average Loss: 0.724759\n",
      "Epoch 17_56.869%:  Training average Loss: 0.725059\n",
      "Epoch 17_57.670%:  Training average Loss: 0.724529\n",
      "Epoch 17_58.471%:  Training average Loss: 0.724158\n",
      "Epoch 17_59.272%:  Training average Loss: 0.724567\n",
      "Epoch 17_60.073%:  Training average Loss: 0.724862\n",
      "Epoch 17_60.874%:  Training average Loss: 0.724722\n",
      "Epoch 17_61.675%:  Training average Loss: 0.724482\n",
      "Epoch 17_62.475%:  Training average Loss: 0.724474\n",
      "Epoch 17_63.276%:  Training average Loss: 0.724340\n",
      "Epoch 17_64.077%:  Training average Loss: 0.724858\n",
      "Epoch 17_64.878%:  Training average Loss: 0.725563\n",
      "Epoch 17_65.679%:  Training average Loss: 0.726146\n",
      "Epoch 17_66.480%:  Training average Loss: 0.726181\n",
      "Epoch 17_67.281%:  Training average Loss: 0.726514\n",
      "Epoch 17_68.082%:  Training average Loss: 0.726888\n",
      "Epoch 17_68.883%:  Training average Loss: 0.726868\n",
      "Epoch 17_69.684%:  Training average Loss: 0.726520\n",
      "Epoch 17_70.485%:  Training average Loss: 0.726496\n",
      "Epoch 17_71.286%:  Training average Loss: 0.726272\n",
      "Epoch 17_72.087%:  Training average Loss: 0.726240\n",
      "Epoch 17_72.888%:  Training average Loss: 0.726932\n",
      "Epoch 17_73.689%:  Training average Loss: 0.727416\n",
      "Epoch 17_74.490%:  Training average Loss: 0.727701\n",
      "Epoch 17_75.291%:  Training average Loss: 0.727817\n",
      "Epoch 17_76.092%:  Training average Loss: 0.728524\n",
      "Epoch 17_76.893%:  Training average Loss: 0.728666\n",
      "Epoch 17_77.694%:  Training average Loss: 0.729093\n",
      "Epoch 17_78.495%:  Training average Loss: 0.729581\n",
      "Epoch 17_79.296%:  Training average Loss: 0.729835\n",
      "Epoch 17_80.097%:  Training average Loss: 0.729968\n",
      "Epoch 17_80.898%:  Training average Loss: 0.730059\n",
      "Epoch 17_81.699%:  Training average Loss: 0.730274\n",
      "Epoch 17_82.500%:  Training average Loss: 0.730255\n",
      "Epoch 17_83.301%:  Training average Loss: 0.729927\n",
      "Epoch 17_84.102%:  Training average Loss: 0.730126\n",
      "Epoch 17_84.903%:  Training average Loss: 0.730084\n",
      "Epoch 17_85.704%:  Training average Loss: 0.729971\n",
      "Epoch 17_86.504%:  Training average Loss: 0.729942\n",
      "Epoch 17_87.305%:  Training average Loss: 0.729922\n",
      "Epoch 17_88.106%:  Training average Loss: 0.729772\n",
      "Epoch 17_88.907%:  Training average Loss: 0.729547\n",
      "Epoch 17_89.708%:  Training average Loss: 0.729753\n",
      "Epoch 17_90.509%:  Training average Loss: 0.729825\n",
      "Epoch 17_91.310%:  Training average Loss: 0.729719\n",
      "Epoch 17_92.111%:  Training average Loss: 0.729988\n",
      "Epoch 17_92.912%:  Training average Loss: 0.730091\n",
      "Epoch 17_93.713%:  Training average Loss: 0.730402\n",
      "Epoch 17_94.514%:  Training average Loss: 0.730208\n",
      "Epoch 17_95.315%:  Training average Loss: 0.730534\n",
      "Epoch 17_96.116%:  Training average Loss: 0.730240\n",
      "Epoch 17_96.917%:  Training average Loss: 0.729853\n",
      "Epoch 17_97.718%:  Training average Loss: 0.729852\n",
      "Epoch 17_98.519%:  Training average Loss: 0.729867\n",
      "Epoch 17_99.320%:  Training average Loss: 0.729850\n",
      "Epoch 17 :  Verification average Loss: 0.851341, Verification accuracy: 65.187582%,Total Time:2906.981423\n",
      "Epoch 18_0.801%:  Training average Loss: 0.763214\n",
      "Epoch 18_1.602%:  Training average Loss: 0.753228\n",
      "Epoch 18_2.403%:  Training average Loss: 0.741317\n",
      "Epoch 18_3.204%:  Training average Loss: 0.735921\n",
      "Epoch 18_4.005%:  Training average Loss: 0.726383\n",
      "Epoch 18_4.806%:  Training average Loss: 0.727203\n",
      "Epoch 18_5.607%:  Training average Loss: 0.723729\n",
      "Epoch 18_6.408%:  Training average Loss: 0.724392\n",
      "Epoch 18_7.209%:  Training average Loss: 0.721938\n",
      "Epoch 18_8.010%:  Training average Loss: 0.718586\n",
      "Epoch 18_8.811%:  Training average Loss: 0.719790\n",
      "Epoch 18_9.612%:  Training average Loss: 0.718003\n",
      "Epoch 18_10.413%:  Training average Loss: 0.719126\n",
      "Epoch 18_11.214%:  Training average Loss: 0.721895\n",
      "Epoch 18_12.015%:  Training average Loss: 0.720872\n",
      "Epoch 18_12.815%:  Training average Loss: 0.718784\n",
      "Epoch 18_13.616%:  Training average Loss: 0.715975\n",
      "Epoch 18_14.417%:  Training average Loss: 0.718471\n",
      "Epoch 18_15.218%:  Training average Loss: 0.717993\n",
      "Epoch 18_16.019%:  Training average Loss: 0.717380\n",
      "Epoch 18_16.820%:  Training average Loss: 0.716243\n",
      "Epoch 18_17.621%:  Training average Loss: 0.714259\n",
      "Epoch 18_18.422%:  Training average Loss: 0.713151\n",
      "Epoch 18_19.223%:  Training average Loss: 0.711896\n",
      "Epoch 18_20.024%:  Training average Loss: 0.711903\n",
      "Epoch 18_20.825%:  Training average Loss: 0.712240\n",
      "Epoch 18_21.626%:  Training average Loss: 0.710169\n",
      "Epoch 18_22.427%:  Training average Loss: 0.711588\n",
      "Epoch 18_23.228%:  Training average Loss: 0.710476\n",
      "Epoch 18_24.029%:  Training average Loss: 0.709874\n",
      "Epoch 18_24.830%:  Training average Loss: 0.709809\n",
      "Epoch 18_25.631%:  Training average Loss: 0.710476\n",
      "Epoch 18_26.432%:  Training average Loss: 0.711805\n",
      "Epoch 18_27.233%:  Training average Loss: 0.711579\n",
      "Epoch 18_28.034%:  Training average Loss: 0.711455\n",
      "Epoch 18_28.835%:  Training average Loss: 0.711424\n",
      "Epoch 18_29.636%:  Training average Loss: 0.712335\n",
      "Epoch 18_30.437%:  Training average Loss: 0.712783\n",
      "Epoch 18_31.238%:  Training average Loss: 0.713034\n",
      "Epoch 18_32.039%:  Training average Loss: 0.713321\n",
      "Epoch 18_32.840%:  Training average Loss: 0.713104\n",
      "Epoch 18_33.641%:  Training average Loss: 0.712935\n",
      "Epoch 18_34.442%:  Training average Loss: 0.714304\n",
      "Epoch 18_35.243%:  Training average Loss: 0.714200\n",
      "Epoch 18_36.044%:  Training average Loss: 0.713926\n",
      "Epoch 18_36.845%:  Training average Loss: 0.714067\n",
      "Epoch 18_37.645%:  Training average Loss: 0.714488\n",
      "Epoch 18_38.446%:  Training average Loss: 0.714696\n",
      "Epoch 18_39.247%:  Training average Loss: 0.714769\n",
      "Epoch 18_40.048%:  Training average Loss: 0.714597\n",
      "Epoch 18_40.849%:  Training average Loss: 0.713866\n",
      "Epoch 18_41.650%:  Training average Loss: 0.714166\n",
      "Epoch 18_42.451%:  Training average Loss: 0.714149\n",
      "Epoch 18_43.252%:  Training average Loss: 0.714089\n",
      "Epoch 18_44.053%:  Training average Loss: 0.713822\n",
      "Epoch 18_44.854%:  Training average Loss: 0.713831\n",
      "Epoch 18_45.655%:  Training average Loss: 0.714330\n",
      "Epoch 18_46.456%:  Training average Loss: 0.714151\n",
      "Epoch 18_47.257%:  Training average Loss: 0.715388\n",
      "Epoch 18_48.058%:  Training average Loss: 0.715887\n",
      "Epoch 18_48.859%:  Training average Loss: 0.715480\n",
      "Epoch 18_49.660%:  Training average Loss: 0.716479\n",
      "Epoch 18_50.461%:  Training average Loss: 0.716423\n",
      "Epoch 18_51.262%:  Training average Loss: 0.717200\n",
      "Epoch 18_52.063%:  Training average Loss: 0.716705\n",
      "Epoch 18_52.864%:  Training average Loss: 0.717338\n",
      "Epoch 18_53.665%:  Training average Loss: 0.717779\n",
      "Epoch 18_54.466%:  Training average Loss: 0.718299\n",
      "Epoch 18_55.267%:  Training average Loss: 0.718442\n",
      "Epoch 18_56.068%:  Training average Loss: 0.718443\n",
      "Epoch 18_56.869%:  Training average Loss: 0.718248\n",
      "Epoch 18_57.670%:  Training average Loss: 0.719525\n",
      "Epoch 18_58.471%:  Training average Loss: 0.720400\n",
      "Epoch 18_59.272%:  Training average Loss: 0.720498\n",
      "Epoch 18_60.073%:  Training average Loss: 0.721665\n",
      "Epoch 18_60.874%:  Training average Loss: 0.722060\n",
      "Epoch 18_61.675%:  Training average Loss: 0.721906\n",
      "Epoch 18_62.475%:  Training average Loss: 0.722344\n",
      "Epoch 18_63.276%:  Training average Loss: 0.722180\n",
      "Epoch 18_64.077%:  Training average Loss: 0.722836\n",
      "Epoch 18_64.878%:  Training average Loss: 0.722687\n",
      "Epoch 18_65.679%:  Training average Loss: 0.723111\n",
      "Epoch 18_66.480%:  Training average Loss: 0.722899\n",
      "Epoch 18_67.281%:  Training average Loss: 0.722892\n",
      "Epoch 18_68.082%:  Training average Loss: 0.723167\n",
      "Epoch 18_68.883%:  Training average Loss: 0.723334\n",
      "Epoch 18_69.684%:  Training average Loss: 0.723323\n",
      "Epoch 18_70.485%:  Training average Loss: 0.722824\n",
      "Epoch 18_71.286%:  Training average Loss: 0.722638\n",
      "Epoch 18_72.087%:  Training average Loss: 0.722633\n",
      "Epoch 18_72.888%:  Training average Loss: 0.722881\n",
      "Epoch 18_73.689%:  Training average Loss: 0.722511\n",
      "Epoch 18_74.490%:  Training average Loss: 0.722564\n",
      "Epoch 18_75.291%:  Training average Loss: 0.722984\n",
      "Epoch 18_76.092%:  Training average Loss: 0.722878\n",
      "Epoch 18_76.893%:  Training average Loss: 0.723283\n",
      "Epoch 18_77.694%:  Training average Loss: 0.723654\n",
      "Epoch 18_78.495%:  Training average Loss: 0.724091\n",
      "Epoch 18_79.296%:  Training average Loss: 0.724444\n",
      "Epoch 18_80.097%:  Training average Loss: 0.724359\n",
      "Epoch 18_80.898%:  Training average Loss: 0.724026\n",
      "Epoch 18_81.699%:  Training average Loss: 0.724191\n",
      "Epoch 18_82.500%:  Training average Loss: 0.724080\n",
      "Epoch 18_83.301%:  Training average Loss: 0.724610\n",
      "Epoch 18_84.102%:  Training average Loss: 0.724851\n",
      "Epoch 18_84.903%:  Training average Loss: 0.724711\n",
      "Epoch 18_85.704%:  Training average Loss: 0.724513\n",
      "Epoch 18_86.504%:  Training average Loss: 0.724081\n",
      "Epoch 18_87.305%:  Training average Loss: 0.724504\n",
      "Epoch 18_88.106%:  Training average Loss: 0.724364\n",
      "Epoch 18_88.907%:  Training average Loss: 0.724841\n",
      "Epoch 18_89.708%:  Training average Loss: 0.724920\n",
      "Epoch 18_90.509%:  Training average Loss: 0.725303\n",
      "Epoch 18_91.310%:  Training average Loss: 0.725316\n",
      "Epoch 18_92.111%:  Training average Loss: 0.725668\n",
      "Epoch 18_92.912%:  Training average Loss: 0.725661\n",
      "Epoch 18_93.713%:  Training average Loss: 0.725699\n",
      "Epoch 18_94.514%:  Training average Loss: 0.725740\n",
      "Epoch 18_95.315%:  Training average Loss: 0.725616\n",
      "Epoch 18_96.116%:  Training average Loss: 0.725757\n",
      "Epoch 18_96.917%:  Training average Loss: 0.725747\n",
      "Epoch 18_97.718%:  Training average Loss: 0.725997\n",
      "Epoch 18_98.519%:  Training average Loss: 0.725515\n",
      "Epoch 18_99.320%:  Training average Loss: 0.725779\n",
      "Epoch 18 :  Verification average Loss: 0.866580, Verification accuracy: 65.379810%,Total Time:3067.885843\n",
      "Epoch 19_0.801%:  Training average Loss: 0.739606\n",
      "Epoch 19_1.602%:  Training average Loss: 0.733213\n",
      "Epoch 19_2.403%:  Training average Loss: 0.746898\n",
      "Epoch 19_3.204%:  Training average Loss: 0.734425\n",
      "Epoch 19_4.005%:  Training average Loss: 0.719597\n",
      "Epoch 19_4.806%:  Training average Loss: 0.719360\n",
      "Epoch 19_5.607%:  Training average Loss: 0.724914\n",
      "Epoch 19_6.408%:  Training average Loss: 0.726869\n",
      "Epoch 19_7.209%:  Training average Loss: 0.723957\n",
      "Epoch 19_8.010%:  Training average Loss: 0.723247\n",
      "Epoch 19_8.811%:  Training average Loss: 0.717966\n",
      "Epoch 19_9.612%:  Training average Loss: 0.715000\n",
      "Epoch 19_10.413%:  Training average Loss: 0.717011\n",
      "Epoch 19_11.214%:  Training average Loss: 0.719720\n",
      "Epoch 19_12.015%:  Training average Loss: 0.720364\n",
      "Epoch 19_12.815%:  Training average Loss: 0.718790\n",
      "Epoch 19_13.616%:  Training average Loss: 0.719481\n",
      "Epoch 19_14.417%:  Training average Loss: 0.719882\n",
      "Epoch 19_15.218%:  Training average Loss: 0.721876\n",
      "Epoch 19_16.019%:  Training average Loss: 0.717717\n",
      "Epoch 19_16.820%:  Training average Loss: 0.715364\n",
      "Epoch 19_17.621%:  Training average Loss: 0.716172\n",
      "Epoch 19_18.422%:  Training average Loss: 0.716095\n",
      "Epoch 19_19.223%:  Training average Loss: 0.716659\n",
      "Epoch 19_20.024%:  Training average Loss: 0.715248\n",
      "Epoch 19_20.825%:  Training average Loss: 0.716875\n",
      "Epoch 19_21.626%:  Training average Loss: 0.716318\n",
      "Epoch 19_22.427%:  Training average Loss: 0.717384\n",
      "Epoch 19_23.228%:  Training average Loss: 0.716554\n",
      "Epoch 19_24.029%:  Training average Loss: 0.717106\n",
      "Epoch 19_24.830%:  Training average Loss: 0.715409\n",
      "Epoch 19_25.631%:  Training average Loss: 0.715408\n",
      "Epoch 19_26.432%:  Training average Loss: 0.715957\n",
      "Epoch 19_27.233%:  Training average Loss: 0.715942\n",
      "Epoch 19_28.034%:  Training average Loss: 0.716225\n",
      "Epoch 19_28.835%:  Training average Loss: 0.715245\n",
      "Epoch 19_29.636%:  Training average Loss: 0.715632\n",
      "Epoch 19_30.437%:  Training average Loss: 0.715685\n",
      "Epoch 19_31.238%:  Training average Loss: 0.714883\n",
      "Epoch 19_32.039%:  Training average Loss: 0.714874\n",
      "Epoch 19_32.840%:  Training average Loss: 0.713844\n",
      "Epoch 19_33.641%:  Training average Loss: 0.713608\n",
      "Epoch 19_34.442%:  Training average Loss: 0.714107\n",
      "Epoch 19_35.243%:  Training average Loss: 0.714678\n",
      "Epoch 19_36.044%:  Training average Loss: 0.715164\n",
      "Epoch 19_36.845%:  Training average Loss: 0.716011\n",
      "Epoch 19_37.645%:  Training average Loss: 0.715831\n",
      "Epoch 19_38.446%:  Training average Loss: 0.717025\n",
      "Epoch 19_39.247%:  Training average Loss: 0.717319\n",
      "Epoch 19_40.048%:  Training average Loss: 0.717394\n",
      "Epoch 19_40.849%:  Training average Loss: 0.717381\n",
      "Epoch 19_41.650%:  Training average Loss: 0.717675\n",
      "Epoch 19_42.451%:  Training average Loss: 0.718045\n",
      "Epoch 19_43.252%:  Training average Loss: 0.719467\n",
      "Epoch 19_44.053%:  Training average Loss: 0.720020\n",
      "Epoch 19_44.854%:  Training average Loss: 0.720503\n",
      "Epoch 19_45.655%:  Training average Loss: 0.721088\n",
      "Epoch 19_46.456%:  Training average Loss: 0.721374\n",
      "Epoch 19_47.257%:  Training average Loss: 0.721237\n",
      "Epoch 19_48.058%:  Training average Loss: 0.720839\n",
      "Epoch 19_48.859%:  Training average Loss: 0.720962\n",
      "Epoch 19_49.660%:  Training average Loss: 0.720855\n",
      "Epoch 19_50.461%:  Training average Loss: 0.720765\n",
      "Epoch 19_51.262%:  Training average Loss: 0.720378\n",
      "Epoch 19_52.063%:  Training average Loss: 0.720830\n",
      "Epoch 19_52.864%:  Training average Loss: 0.720536\n",
      "Epoch 19_53.665%:  Training average Loss: 0.720691\n",
      "Epoch 19_54.466%:  Training average Loss: 0.720517\n",
      "Epoch 19_55.267%:  Training average Loss: 0.720542\n",
      "Epoch 19_56.068%:  Training average Loss: 0.720775\n",
      "Epoch 19_56.869%:  Training average Loss: 0.721350\n",
      "Epoch 19_57.670%:  Training average Loss: 0.721748\n",
      "Epoch 19_58.471%:  Training average Loss: 0.721818\n",
      "Epoch 19_59.272%:  Training average Loss: 0.722124\n",
      "Epoch 19_60.073%:  Training average Loss: 0.721979\n",
      "Epoch 19_60.874%:  Training average Loss: 0.722092\n",
      "Epoch 19_61.675%:  Training average Loss: 0.722177\n",
      "Epoch 19_62.475%:  Training average Loss: 0.722233\n",
      "Epoch 19_63.276%:  Training average Loss: 0.722499\n",
      "Epoch 19_64.077%:  Training average Loss: 0.722582\n",
      "Epoch 19_64.878%:  Training average Loss: 0.722092\n",
      "Epoch 19_65.679%:  Training average Loss: 0.722441\n",
      "Epoch 19_66.480%:  Training average Loss: 0.722722\n",
      "Epoch 19_67.281%:  Training average Loss: 0.722810\n",
      "Epoch 19_68.082%:  Training average Loss: 0.722631\n",
      "Epoch 19_68.883%:  Training average Loss: 0.722845\n",
      "Epoch 19_69.684%:  Training average Loss: 0.722839\n",
      "Epoch 19_70.485%:  Training average Loss: 0.722564\n",
      "Epoch 19_71.286%:  Training average Loss: 0.722553\n",
      "Epoch 19_72.087%:  Training average Loss: 0.722736\n",
      "Epoch 19_72.888%:  Training average Loss: 0.722997\n",
      "Epoch 19_73.689%:  Training average Loss: 0.723600\n",
      "Epoch 19_74.490%:  Training average Loss: 0.723470\n",
      "Epoch 19_75.291%:  Training average Loss: 0.723499\n",
      "Epoch 19_76.092%:  Training average Loss: 0.723315\n",
      "Epoch 19_76.893%:  Training average Loss: 0.723056\n",
      "Epoch 19_77.694%:  Training average Loss: 0.722560\n",
      "Epoch 19_78.495%:  Training average Loss: 0.722733\n",
      "Epoch 19_79.296%:  Training average Loss: 0.722776\n",
      "Epoch 19_80.097%:  Training average Loss: 0.722684\n",
      "Epoch 19_80.898%:  Training average Loss: 0.722332\n",
      "Epoch 19_81.699%:  Training average Loss: 0.722314\n",
      "Epoch 19_82.500%:  Training average Loss: 0.722022\n",
      "Epoch 19_83.301%:  Training average Loss: 0.722239\n",
      "Epoch 19_84.102%:  Training average Loss: 0.721787\n",
      "Epoch 19_84.903%:  Training average Loss: 0.721905\n",
      "Epoch 19_85.704%:  Training average Loss: 0.722199\n",
      "Epoch 19_86.504%:  Training average Loss: 0.722103\n",
      "Epoch 19_87.305%:  Training average Loss: 0.722422\n",
      "Epoch 19_88.106%:  Training average Loss: 0.722539\n",
      "Epoch 19_88.907%:  Training average Loss: 0.722526\n",
      "Epoch 19_89.708%:  Training average Loss: 0.722425\n",
      "Epoch 19_90.509%:  Training average Loss: 0.722191\n",
      "Epoch 19_91.310%:  Training average Loss: 0.722642\n",
      "Epoch 19_92.111%:  Training average Loss: 0.723066\n",
      "Epoch 19_92.912%:  Training average Loss: 0.722815\n",
      "Epoch 19_93.713%:  Training average Loss: 0.722738\n",
      "Epoch 19_94.514%:  Training average Loss: 0.722618\n",
      "Epoch 19_95.315%:  Training average Loss: 0.722349\n",
      "Epoch 19_96.116%:  Training average Loss: 0.722497\n",
      "Epoch 19_96.917%:  Training average Loss: 0.722338\n",
      "Epoch 19_97.718%:  Training average Loss: 0.722420\n",
      "Epoch 19_98.519%:  Training average Loss: 0.722477\n",
      "Epoch 19_99.320%:  Training average Loss: 0.722286\n",
      "Epoch 19 :  Verification average Loss: 0.877977, Verification accuracy: 65.037004%,Total Time:3229.097956\n",
      "Epoch 20_0.801%:  Training average Loss: 0.720525\n",
      "Epoch 20_1.602%:  Training average Loss: 0.704657\n",
      "Epoch 20_2.403%:  Training average Loss: 0.703339\n",
      "Epoch 20_3.204%:  Training average Loss: 0.702038\n",
      "Epoch 20_4.005%:  Training average Loss: 0.699088\n",
      "Epoch 20_4.806%:  Training average Loss: 0.696804\n",
      "Epoch 20_5.607%:  Training average Loss: 0.696096\n",
      "Epoch 20_6.408%:  Training average Loss: 0.697380\n",
      "Epoch 20_7.209%:  Training average Loss: 0.702623\n",
      "Epoch 20_8.010%:  Training average Loss: 0.705173\n",
      "Epoch 20_8.811%:  Training average Loss: 0.704001\n",
      "Epoch 20_9.612%:  Training average Loss: 0.703326\n",
      "Epoch 20_10.413%:  Training average Loss: 0.704414\n",
      "Epoch 20_11.214%:  Training average Loss: 0.704748\n",
      "Epoch 20_12.015%:  Training average Loss: 0.703979\n",
      "Epoch 20_12.815%:  Training average Loss: 0.705057\n",
      "Epoch 20_13.616%:  Training average Loss: 0.707196\n",
      "Epoch 20_14.417%:  Training average Loss: 0.706560\n",
      "Epoch 20_15.218%:  Training average Loss: 0.706849\n",
      "Epoch 20_16.019%:  Training average Loss: 0.706873\n",
      "Epoch 20_16.820%:  Training average Loss: 0.707637\n",
      "Epoch 20_17.621%:  Training average Loss: 0.707525\n",
      "Epoch 20_18.422%:  Training average Loss: 0.707154\n",
      "Epoch 20_19.223%:  Training average Loss: 0.705750\n",
      "Epoch 20_20.024%:  Training average Loss: 0.707543\n",
      "Epoch 20_20.825%:  Training average Loss: 0.706743\n",
      "Epoch 20_21.626%:  Training average Loss: 0.706425\n",
      "Epoch 20_22.427%:  Training average Loss: 0.706384\n",
      "Epoch 20_23.228%:  Training average Loss: 0.706779\n",
      "Epoch 20_24.029%:  Training average Loss: 0.707226\n",
      "Epoch 20_24.830%:  Training average Loss: 0.707951\n",
      "Epoch 20_25.631%:  Training average Loss: 0.707701\n",
      "Epoch 20_26.432%:  Training average Loss: 0.707550\n",
      "Epoch 20_27.233%:  Training average Loss: 0.707864\n",
      "Epoch 20_28.034%:  Training average Loss: 0.707913\n",
      "Epoch 20_28.835%:  Training average Loss: 0.709090\n",
      "Epoch 20_29.636%:  Training average Loss: 0.708947\n",
      "Epoch 20_30.437%:  Training average Loss: 0.708905\n",
      "Epoch 20_31.238%:  Training average Loss: 0.709019\n",
      "Epoch 20_32.039%:  Training average Loss: 0.709625\n",
      "Epoch 20_32.840%:  Training average Loss: 0.709237\n",
      "Epoch 20_33.641%:  Training average Loss: 0.708962\n",
      "Epoch 20_34.442%:  Training average Loss: 0.708814\n",
      "Epoch 20_35.243%:  Training average Loss: 0.708060\n",
      "Epoch 20_36.044%:  Training average Loss: 0.708166\n",
      "Epoch 20_36.845%:  Training average Loss: 0.708631\n",
      "Epoch 20_37.645%:  Training average Loss: 0.708529\n",
      "Epoch 20_38.446%:  Training average Loss: 0.709143\n",
      "Epoch 20_39.247%:  Training average Loss: 0.708746\n",
      "Epoch 20_40.048%:  Training average Loss: 0.708862\n",
      "Epoch 20_40.849%:  Training average Loss: 0.708523\n",
      "Epoch 20_41.650%:  Training average Loss: 0.710340\n",
      "Epoch 20_42.451%:  Training average Loss: 0.710596\n",
      "Epoch 20_43.252%:  Training average Loss: 0.711004\n",
      "Epoch 20_44.053%:  Training average Loss: 0.710800\n",
      "Epoch 20_44.854%:  Training average Loss: 0.711139\n",
      "Epoch 20_45.655%:  Training average Loss: 0.711440\n",
      "Epoch 20_46.456%:  Training average Loss: 0.712406\n",
      "Epoch 20_47.257%:  Training average Loss: 0.713178\n",
      "Epoch 20_48.058%:  Training average Loss: 0.712680\n",
      "Epoch 20_48.859%:  Training average Loss: 0.712529\n",
      "Epoch 20_49.660%:  Training average Loss: 0.712537\n",
      "Epoch 20_50.461%:  Training average Loss: 0.712481\n",
      "Epoch 20_51.262%:  Training average Loss: 0.711974\n",
      "Epoch 20_52.063%:  Training average Loss: 0.712250\n",
      "Epoch 20_52.864%:  Training average Loss: 0.712119\n",
      "Epoch 20_53.665%:  Training average Loss: 0.712767\n",
      "Epoch 20_54.466%:  Training average Loss: 0.713753\n",
      "Epoch 20_55.267%:  Training average Loss: 0.713524\n",
      "Epoch 20_56.068%:  Training average Loss: 0.713461\n",
      "Epoch 20_56.869%:  Training average Loss: 0.713667\n",
      "Epoch 20_57.670%:  Training average Loss: 0.713847\n",
      "Epoch 20_58.471%:  Training average Loss: 0.713859\n",
      "Epoch 20_59.272%:  Training average Loss: 0.714113\n",
      "Epoch 20_60.073%:  Training average Loss: 0.714149\n",
      "Epoch 20_60.874%:  Training average Loss: 0.713294\n",
      "Epoch 20_61.675%:  Training average Loss: 0.713439\n",
      "Epoch 20_62.475%:  Training average Loss: 0.713543\n",
      "Epoch 20_63.276%:  Training average Loss: 0.713826\n",
      "Epoch 20_64.077%:  Training average Loss: 0.713448\n",
      "Epoch 20_64.878%:  Training average Loss: 0.713521\n",
      "Epoch 20_65.679%:  Training average Loss: 0.713549\n",
      "Epoch 20_66.480%:  Training average Loss: 0.713696\n",
      "Epoch 20_67.281%:  Training average Loss: 0.714017\n",
      "Epoch 20_68.082%:  Training average Loss: 0.714467\n",
      "Epoch 20_68.883%:  Training average Loss: 0.714412\n",
      "Epoch 20_69.684%:  Training average Loss: 0.714632\n",
      "Epoch 20_70.485%:  Training average Loss: 0.714937\n",
      "Epoch 20_71.286%:  Training average Loss: 0.714594\n",
      "Epoch 20_72.087%:  Training average Loss: 0.714230\n",
      "Epoch 20_72.888%:  Training average Loss: 0.714564\n",
      "Epoch 20_73.689%:  Training average Loss: 0.714469\n",
      "Epoch 20_74.490%:  Training average Loss: 0.714408\n",
      "Epoch 20_75.291%:  Training average Loss: 0.714354\n",
      "Epoch 20_76.092%:  Training average Loss: 0.714510\n",
      "Epoch 20_76.893%:  Training average Loss: 0.714390\n",
      "Epoch 20_77.694%:  Training average Loss: 0.714166\n",
      "Epoch 20_78.495%:  Training average Loss: 0.713974\n",
      "Epoch 20_79.296%:  Training average Loss: 0.713693\n",
      "Epoch 20_80.097%:  Training average Loss: 0.714009\n",
      "Epoch 20_80.898%:  Training average Loss: 0.713783\n",
      "Epoch 20_81.699%:  Training average Loss: 0.713704\n",
      "Epoch 20_82.500%:  Training average Loss: 0.713737\n",
      "Epoch 20_83.301%:  Training average Loss: 0.714151\n",
      "Epoch 20_84.102%:  Training average Loss: 0.714006\n",
      "Epoch 20_84.903%:  Training average Loss: 0.713615\n",
      "Epoch 20_85.704%:  Training average Loss: 0.713852\n",
      "Epoch 20_86.504%:  Training average Loss: 0.713409\n",
      "Epoch 20_87.305%:  Training average Loss: 0.713833\n",
      "Epoch 20_88.106%:  Training average Loss: 0.713789\n",
      "Epoch 20_88.907%:  Training average Loss: 0.713782\n",
      "Epoch 20_89.708%:  Training average Loss: 0.713832\n",
      "Epoch 20_90.509%:  Training average Loss: 0.714238\n",
      "Epoch 20_91.310%:  Training average Loss: 0.714680\n",
      "Epoch 20_92.111%:  Training average Loss: 0.715162\n",
      "Epoch 20_92.912%:  Training average Loss: 0.715487\n",
      "Epoch 20_93.713%:  Training average Loss: 0.714953\n",
      "Epoch 20_94.514%:  Training average Loss: 0.715589\n",
      "Epoch 20_95.315%:  Training average Loss: 0.715607\n",
      "Epoch 20_96.116%:  Training average Loss: 0.715690\n",
      "Epoch 20_96.917%:  Training average Loss: 0.715738\n",
      "Epoch 20_97.718%:  Training average Loss: 0.715853\n",
      "Epoch 20_98.519%:  Training average Loss: 0.715910\n",
      "Epoch 20_99.320%:  Training average Loss: 0.716051\n",
      "Epoch 20 :  Verification average Loss: 0.849691, Verification accuracy: 65.366995%,Total Time:3389.986755\n",
      "Epoch 21_0.801%:  Training average Loss: 0.669091\n",
      "Epoch 21_1.602%:  Training average Loss: 0.670115\n",
      "Epoch 21_2.403%:  Training average Loss: 0.670825\n",
      "Epoch 21_3.204%:  Training average Loss: 0.691926\n",
      "Epoch 21_4.005%:  Training average Loss: 0.688940\n",
      "Epoch 21_4.806%:  Training average Loss: 0.698377\n",
      "Epoch 21_5.607%:  Training average Loss: 0.701172\n",
      "Epoch 21_6.408%:  Training average Loss: 0.700743\n",
      "Epoch 21_7.209%:  Training average Loss: 0.694602\n",
      "Epoch 21_8.010%:  Training average Loss: 0.697780\n",
      "Epoch 21_8.811%:  Training average Loss: 0.701113\n",
      "Epoch 21_9.612%:  Training average Loss: 0.697551\n",
      "Epoch 21_10.413%:  Training average Loss: 0.701888\n",
      "Epoch 21_11.214%:  Training average Loss: 0.701577\n",
      "Epoch 21_12.015%:  Training average Loss: 0.698712\n",
      "Epoch 21_12.815%:  Training average Loss: 0.700181\n",
      "Epoch 21_13.616%:  Training average Loss: 0.697299\n",
      "Epoch 21_14.417%:  Training average Loss: 0.700401\n",
      "Epoch 21_15.218%:  Training average Loss: 0.701878\n",
      "Epoch 21_16.019%:  Training average Loss: 0.700941\n",
      "Epoch 21_16.820%:  Training average Loss: 0.698974\n",
      "Epoch 21_17.621%:  Training average Loss: 0.697672\n",
      "Epoch 21_18.422%:  Training average Loss: 0.698880\n",
      "Epoch 21_19.223%:  Training average Loss: 0.699075\n",
      "Epoch 21_20.024%:  Training average Loss: 0.698603\n",
      "Epoch 21_20.825%:  Training average Loss: 0.699675\n",
      "Epoch 21_21.626%:  Training average Loss: 0.699927\n",
      "Epoch 21_22.427%:  Training average Loss: 0.699240\n",
      "Epoch 21_23.228%:  Training average Loss: 0.700571\n",
      "Epoch 21_24.029%:  Training average Loss: 0.700378\n",
      "Epoch 21_24.830%:  Training average Loss: 0.699018\n",
      "Epoch 21_25.631%:  Training average Loss: 0.698982\n",
      "Epoch 21_26.432%:  Training average Loss: 0.700005\n",
      "Epoch 21_27.233%:  Training average Loss: 0.700284\n",
      "Epoch 21_28.034%:  Training average Loss: 0.700550\n",
      "Epoch 21_28.835%:  Training average Loss: 0.700550\n",
      "Epoch 21_29.636%:  Training average Loss: 0.700557\n",
      "Epoch 21_30.437%:  Training average Loss: 0.698637\n",
      "Epoch 21_31.238%:  Training average Loss: 0.698674\n",
      "Epoch 21_32.039%:  Training average Loss: 0.699553\n",
      "Epoch 21_32.840%:  Training average Loss: 0.700975\n",
      "Epoch 21_33.641%:  Training average Loss: 0.701425\n",
      "Epoch 21_34.442%:  Training average Loss: 0.700666\n",
      "Epoch 21_35.243%:  Training average Loss: 0.701333\n",
      "Epoch 21_36.044%:  Training average Loss: 0.700838\n",
      "Epoch 21_36.845%:  Training average Loss: 0.701344\n",
      "Epoch 21_37.645%:  Training average Loss: 0.700955\n",
      "Epoch 21_38.446%:  Training average Loss: 0.700435\n",
      "Epoch 21_39.247%:  Training average Loss: 0.701141\n",
      "Epoch 21_40.048%:  Training average Loss: 0.701185\n",
      "Epoch 21_40.849%:  Training average Loss: 0.701924\n",
      "Epoch 21_41.650%:  Training average Loss: 0.701948\n",
      "Epoch 21_42.451%:  Training average Loss: 0.702370\n",
      "Epoch 21_43.252%:  Training average Loss: 0.703296\n",
      "Epoch 21_44.053%:  Training average Loss: 0.703475\n",
      "Epoch 21_44.854%:  Training average Loss: 0.704318\n",
      "Epoch 21_45.655%:  Training average Loss: 0.704347\n",
      "Epoch 21_46.456%:  Training average Loss: 0.705298\n",
      "Epoch 21_47.257%:  Training average Loss: 0.705408\n",
      "Epoch 21_48.058%:  Training average Loss: 0.706398\n",
      "Epoch 21_48.859%:  Training average Loss: 0.706564\n",
      "Epoch 21_49.660%:  Training average Loss: 0.706679\n",
      "Epoch 21_50.461%:  Training average Loss: 0.706579\n",
      "Epoch 21_51.262%:  Training average Loss: 0.706271\n",
      "Epoch 21_52.063%:  Training average Loss: 0.706092\n",
      "Epoch 21_52.864%:  Training average Loss: 0.706367\n",
      "Epoch 21_53.665%:  Training average Loss: 0.706770\n",
      "Epoch 21_54.466%:  Training average Loss: 0.706782\n",
      "Epoch 21_55.267%:  Training average Loss: 0.707092\n",
      "Epoch 21_56.068%:  Training average Loss: 0.707516\n",
      "Epoch 21_56.869%:  Training average Loss: 0.708062\n",
      "Epoch 21_57.670%:  Training average Loss: 0.708234\n",
      "Epoch 21_58.471%:  Training average Loss: 0.708492\n",
      "Epoch 21_59.272%:  Training average Loss: 0.708901\n",
      "Epoch 21_60.073%:  Training average Loss: 0.708739\n",
      "Epoch 21_60.874%:  Training average Loss: 0.708932\n",
      "Epoch 21_61.675%:  Training average Loss: 0.708614\n",
      "Epoch 21_62.475%:  Training average Loss: 0.709068\n",
      "Epoch 21_63.276%:  Training average Loss: 0.709927\n",
      "Epoch 21_64.077%:  Training average Loss: 0.709865\n",
      "Epoch 21_64.878%:  Training average Loss: 0.709497\n",
      "Epoch 21_65.679%:  Training average Loss: 0.708873\n",
      "Epoch 21_66.480%:  Training average Loss: 0.709118\n",
      "Epoch 21_67.281%:  Training average Loss: 0.708753\n",
      "Epoch 21_68.082%:  Training average Loss: 0.708943\n",
      "Epoch 21_68.883%:  Training average Loss: 0.709052\n",
      "Epoch 21_69.684%:  Training average Loss: 0.709394\n",
      "Epoch 21_70.485%:  Training average Loss: 0.709056\n",
      "Epoch 21_71.286%:  Training average Loss: 0.708969\n",
      "Epoch 21_72.087%:  Training average Loss: 0.709004\n",
      "Epoch 21_72.888%:  Training average Loss: 0.709188\n",
      "Epoch 21_73.689%:  Training average Loss: 0.709533\n",
      "Epoch 21_74.490%:  Training average Loss: 0.709309\n",
      "Epoch 21_75.291%:  Training average Loss: 0.709264\n",
      "Epoch 21_76.092%:  Training average Loss: 0.709315\n",
      "Epoch 21_76.893%:  Training average Loss: 0.709372\n",
      "Epoch 21_77.694%:  Training average Loss: 0.709777\n",
      "Epoch 21_78.495%:  Training average Loss: 0.709466\n",
      "Epoch 21_79.296%:  Training average Loss: 0.709288\n",
      "Epoch 21_80.097%:  Training average Loss: 0.709283\n",
      "Epoch 21_80.898%:  Training average Loss: 0.709379\n",
      "Epoch 21_81.699%:  Training average Loss: 0.709418\n",
      "Epoch 21_82.500%:  Training average Loss: 0.709501\n",
      "Epoch 21_83.301%:  Training average Loss: 0.709443\n",
      "Epoch 21_84.102%:  Training average Loss: 0.709258\n",
      "Epoch 21_84.903%:  Training average Loss: 0.709174\n",
      "Epoch 21_85.704%:  Training average Loss: 0.709262\n",
      "Epoch 21_86.504%:  Training average Loss: 0.709413\n",
      "Epoch 21_87.305%:  Training average Loss: 0.709542\n",
      "Epoch 21_88.106%:  Training average Loss: 0.709823\n",
      "Epoch 21_88.907%:  Training average Loss: 0.709899\n",
      "Epoch 21_89.708%:  Training average Loss: 0.710138\n",
      "Epoch 21_90.509%:  Training average Loss: 0.710453\n",
      "Epoch 21_91.310%:  Training average Loss: 0.710568\n",
      "Epoch 21_92.111%:  Training average Loss: 0.710676\n",
      "Epoch 21_92.912%:  Training average Loss: 0.711063\n",
      "Epoch 21_93.713%:  Training average Loss: 0.711142\n",
      "Epoch 21_94.514%:  Training average Loss: 0.711604\n",
      "Epoch 21_95.315%:  Training average Loss: 0.711801\n",
      "Epoch 21_96.116%:  Training average Loss: 0.711894\n",
      "Epoch 21_96.917%:  Training average Loss: 0.712298\n",
      "Epoch 21_97.718%:  Training average Loss: 0.712921\n",
      "Epoch 21_98.519%:  Training average Loss: 0.712839\n",
      "Epoch 21_99.320%:  Training average Loss: 0.713072\n",
      "Epoch 21 :  Verification average Loss: 0.865314, Verification accuracy: 65.360587%,Total Time:3550.998675\n",
      "Epoch 22_0.801%:  Training average Loss: 0.699790\n",
      "Epoch 22_1.602%:  Training average Loss: 0.688643\n",
      "Epoch 22_2.403%:  Training average Loss: 0.691958\n",
      "Epoch 22_3.204%:  Training average Loss: 0.697533\n",
      "Epoch 22_4.005%:  Training average Loss: 0.700347\n",
      "Epoch 22_4.806%:  Training average Loss: 0.691716\n",
      "Epoch 22_5.607%:  Training average Loss: 0.697693\n",
      "Epoch 22_6.408%:  Training average Loss: 0.694883\n",
      "Epoch 22_7.209%:  Training average Loss: 0.694891\n",
      "Epoch 22_8.010%:  Training average Loss: 0.695586\n",
      "Epoch 22_8.811%:  Training average Loss: 0.697081\n",
      "Epoch 22_9.612%:  Training average Loss: 0.696197\n",
      "Epoch 22_10.413%:  Training average Loss: 0.697933\n",
      "Epoch 22_11.214%:  Training average Loss: 0.696833\n",
      "Epoch 22_12.015%:  Training average Loss: 0.699599\n",
      "Epoch 22_12.815%:  Training average Loss: 0.701916\n",
      "Epoch 22_13.616%:  Training average Loss: 0.701146\n",
      "Epoch 22_14.417%:  Training average Loss: 0.699232\n",
      "Epoch 22_15.218%:  Training average Loss: 0.698165\n",
      "Epoch 22_16.019%:  Training average Loss: 0.697661\n",
      "Epoch 22_16.820%:  Training average Loss: 0.700205\n",
      "Epoch 22_17.621%:  Training average Loss: 0.697724\n",
      "Epoch 22_18.422%:  Training average Loss: 0.695332\n",
      "Epoch 22_19.223%:  Training average Loss: 0.695682\n",
      "Epoch 22_20.024%:  Training average Loss: 0.695715\n",
      "Epoch 22_20.825%:  Training average Loss: 0.697295\n",
      "Epoch 22_21.626%:  Training average Loss: 0.697178\n",
      "Epoch 22_22.427%:  Training average Loss: 0.695316\n",
      "Epoch 22_23.228%:  Training average Loss: 0.696026\n",
      "Epoch 22_24.029%:  Training average Loss: 0.694501\n",
      "Epoch 22_24.830%:  Training average Loss: 0.694821\n",
      "Epoch 22_25.631%:  Training average Loss: 0.694771\n",
      "Epoch 22_26.432%:  Training average Loss: 0.694656\n",
      "Epoch 22_27.233%:  Training average Loss: 0.695342\n",
      "Epoch 22_28.034%:  Training average Loss: 0.696708\n",
      "Epoch 22_28.835%:  Training average Loss: 0.698250\n",
      "Epoch 22_29.636%:  Training average Loss: 0.698662\n",
      "Epoch 22_30.437%:  Training average Loss: 0.698724\n",
      "Epoch 22_31.238%:  Training average Loss: 0.699139\n",
      "Epoch 22_32.039%:  Training average Loss: 0.699858\n",
      "Epoch 22_32.840%:  Training average Loss: 0.698984\n",
      "Epoch 22_33.641%:  Training average Loss: 0.698704\n",
      "Epoch 22_34.442%:  Training average Loss: 0.699207\n",
      "Epoch 22_35.243%:  Training average Loss: 0.699099\n",
      "Epoch 22_36.044%:  Training average Loss: 0.699545\n",
      "Epoch 22_36.845%:  Training average Loss: 0.699525\n",
      "Epoch 22_37.645%:  Training average Loss: 0.699444\n",
      "Epoch 22_38.446%:  Training average Loss: 0.699673\n",
      "Epoch 22_39.247%:  Training average Loss: 0.699651\n",
      "Epoch 22_40.048%:  Training average Loss: 0.699365\n",
      "Epoch 22_40.849%:  Training average Loss: 0.699765\n",
      "Epoch 22_41.650%:  Training average Loss: 0.700259\n",
      "Epoch 22_42.451%:  Training average Loss: 0.699599\n",
      "Epoch 22_43.252%:  Training average Loss: 0.699622\n",
      "Epoch 22_44.053%:  Training average Loss: 0.699598\n",
      "Epoch 22_44.854%:  Training average Loss: 0.699251\n",
      "Epoch 22_45.655%:  Training average Loss: 0.699466\n",
      "Epoch 22_46.456%:  Training average Loss: 0.699314\n",
      "Epoch 22_47.257%:  Training average Loss: 0.699774\n",
      "Epoch 22_48.058%:  Training average Loss: 0.700632\n",
      "Epoch 22_48.859%:  Training average Loss: 0.700790\n",
      "Epoch 22_49.660%:  Training average Loss: 0.700662\n",
      "Epoch 22_50.461%:  Training average Loss: 0.701216\n",
      "Epoch 22_51.262%:  Training average Loss: 0.701169\n",
      "Epoch 22_52.063%:  Training average Loss: 0.700671\n",
      "Epoch 22_52.864%:  Training average Loss: 0.700756\n",
      "Epoch 22_53.665%:  Training average Loss: 0.700789\n",
      "Epoch 22_54.466%:  Training average Loss: 0.701214\n",
      "Epoch 22_55.267%:  Training average Loss: 0.701526\n",
      "Epoch 22_56.068%:  Training average Loss: 0.701513\n",
      "Epoch 22_56.869%:  Training average Loss: 0.701514\n",
      "Epoch 22_57.670%:  Training average Loss: 0.702077\n",
      "Epoch 22_58.471%:  Training average Loss: 0.701435\n",
      "Epoch 22_59.272%:  Training average Loss: 0.701308\n",
      "Epoch 22_60.073%:  Training average Loss: 0.701305\n",
      "Epoch 22_60.874%:  Training average Loss: 0.701977\n",
      "Epoch 22_61.675%:  Training average Loss: 0.701473\n",
      "Epoch 22_62.475%:  Training average Loss: 0.702257\n",
      "Epoch 22_63.276%:  Training average Loss: 0.702563\n",
      "Epoch 22_64.077%:  Training average Loss: 0.702432\n",
      "Epoch 22_64.878%:  Training average Loss: 0.701930\n",
      "Epoch 22_65.679%:  Training average Loss: 0.701760\n",
      "Epoch 22_66.480%:  Training average Loss: 0.702239\n",
      "Epoch 22_67.281%:  Training average Loss: 0.703102\n",
      "Epoch 22_68.082%:  Training average Loss: 0.703116\n",
      "Epoch 22_68.883%:  Training average Loss: 0.703030\n",
      "Epoch 22_69.684%:  Training average Loss: 0.703394\n",
      "Epoch 22_70.485%:  Training average Loss: 0.704083\n",
      "Epoch 22_71.286%:  Training average Loss: 0.704160\n",
      "Epoch 22_72.087%:  Training average Loss: 0.704305\n",
      "Epoch 22_72.888%:  Training average Loss: 0.704082\n",
      "Epoch 22_73.689%:  Training average Loss: 0.704392\n",
      "Epoch 22_74.490%:  Training average Loss: 0.704772\n",
      "Epoch 22_75.291%:  Training average Loss: 0.704861\n",
      "Epoch 22_76.092%:  Training average Loss: 0.704948\n",
      "Epoch 22_76.893%:  Training average Loss: 0.705157\n",
      "Epoch 22_77.694%:  Training average Loss: 0.704728\n",
      "Epoch 22_78.495%:  Training average Loss: 0.704876\n",
      "Epoch 22_79.296%:  Training average Loss: 0.704677\n",
      "Epoch 22_80.097%:  Training average Loss: 0.704793\n",
      "Epoch 22_80.898%:  Training average Loss: 0.705316\n",
      "Epoch 22_81.699%:  Training average Loss: 0.705519\n",
      "Epoch 22_82.500%:  Training average Loss: 0.705726\n",
      "Epoch 22_83.301%:  Training average Loss: 0.706263\n",
      "Epoch 22_84.102%:  Training average Loss: 0.706779\n",
      "Epoch 22_84.903%:  Training average Loss: 0.707129\n",
      "Epoch 22_85.704%:  Training average Loss: 0.707326\n",
      "Epoch 22_86.504%:  Training average Loss: 0.707523\n",
      "Epoch 22_87.305%:  Training average Loss: 0.707443\n",
      "Epoch 22_88.106%:  Training average Loss: 0.707179\n",
      "Epoch 22_88.907%:  Training average Loss: 0.707240\n",
      "Epoch 22_89.708%:  Training average Loss: 0.707601\n",
      "Epoch 22_90.509%:  Training average Loss: 0.707880\n",
      "Epoch 22_91.310%:  Training average Loss: 0.707670\n",
      "Epoch 22_92.111%:  Training average Loss: 0.708099\n",
      "Epoch 22_92.912%:  Training average Loss: 0.708396\n",
      "Epoch 22_93.713%:  Training average Loss: 0.708482\n",
      "Epoch 22_94.514%:  Training average Loss: 0.708763\n",
      "Epoch 22_95.315%:  Training average Loss: 0.708522\n",
      "Epoch 22_96.116%:  Training average Loss: 0.708323\n",
      "Epoch 22_96.917%:  Training average Loss: 0.708494\n",
      "Epoch 22_97.718%:  Training average Loss: 0.708569\n",
      "Epoch 22_98.519%:  Training average Loss: 0.708650\n",
      "Epoch 22_99.320%:  Training average Loss: 0.708893\n",
      "Epoch 22 :  Verification average Loss: 0.862864, Verification accuracy: 65.238843%,Total Time:3711.507548\n",
      "Epoch 23_0.801%:  Training average Loss: 0.675035\n",
      "Epoch 23_1.602%:  Training average Loss: 0.676936\n",
      "Epoch 23_2.403%:  Training average Loss: 0.688240\n",
      "Epoch 23_3.204%:  Training average Loss: 0.693541\n",
      "Epoch 23_4.005%:  Training average Loss: 0.688815\n",
      "Epoch 23_4.806%:  Training average Loss: 0.688972\n",
      "Epoch 23_5.607%:  Training average Loss: 0.694323\n",
      "Epoch 23_6.408%:  Training average Loss: 0.695096\n",
      "Epoch 23_7.209%:  Training average Loss: 0.689443\n",
      "Epoch 23_8.010%:  Training average Loss: 0.687588\n",
      "Epoch 23_8.811%:  Training average Loss: 0.688674\n",
      "Epoch 23_9.612%:  Training average Loss: 0.694233\n",
      "Epoch 23_10.413%:  Training average Loss: 0.694263\n",
      "Epoch 23_11.214%:  Training average Loss: 0.695954\n",
      "Epoch 23_12.015%:  Training average Loss: 0.695508\n",
      "Epoch 23_12.815%:  Training average Loss: 0.693436\n",
      "Epoch 23_13.616%:  Training average Loss: 0.692925\n",
      "Epoch 23_14.417%:  Training average Loss: 0.693246\n",
      "Epoch 23_15.218%:  Training average Loss: 0.691200\n",
      "Epoch 23_16.019%:  Training average Loss: 0.689696\n",
      "Epoch 23_16.820%:  Training average Loss: 0.689519\n",
      "Epoch 23_17.621%:  Training average Loss: 0.690738\n",
      "Epoch 23_18.422%:  Training average Loss: 0.692048\n",
      "Epoch 23_19.223%:  Training average Loss: 0.691592\n",
      "Epoch 23_20.024%:  Training average Loss: 0.693115\n",
      "Epoch 23_20.825%:  Training average Loss: 0.693737\n",
      "Epoch 23_21.626%:  Training average Loss: 0.693655\n",
      "Epoch 23_22.427%:  Training average Loss: 0.693113\n",
      "Epoch 23_23.228%:  Training average Loss: 0.693658\n",
      "Epoch 23_24.029%:  Training average Loss: 0.694169\n",
      "Epoch 23_24.830%:  Training average Loss: 0.694368\n",
      "Epoch 23_25.631%:  Training average Loss: 0.695288\n",
      "Epoch 23_26.432%:  Training average Loss: 0.697217\n",
      "Epoch 23_27.233%:  Training average Loss: 0.697637\n",
      "Epoch 23_28.034%:  Training average Loss: 0.698957\n",
      "Epoch 23_28.835%:  Training average Loss: 0.698345\n",
      "Epoch 23_29.636%:  Training average Loss: 0.698138\n",
      "Epoch 23_30.437%:  Training average Loss: 0.697389\n",
      "Epoch 23_31.238%:  Training average Loss: 0.697082\n",
      "Epoch 23_32.039%:  Training average Loss: 0.695856\n",
      "Epoch 23_32.840%:  Training average Loss: 0.695884\n",
      "Epoch 23_33.641%:  Training average Loss: 0.695544\n",
      "Epoch 23_34.442%:  Training average Loss: 0.696672\n",
      "Epoch 23_35.243%:  Training average Loss: 0.696733\n",
      "Epoch 23_36.044%:  Training average Loss: 0.696749\n",
      "Epoch 23_36.845%:  Training average Loss: 0.696840\n",
      "Epoch 23_37.645%:  Training average Loss: 0.697431\n",
      "Epoch 23_38.446%:  Training average Loss: 0.696288\n",
      "Epoch 23_39.247%:  Training average Loss: 0.696057\n",
      "Epoch 23_40.048%:  Training average Loss: 0.696324\n",
      "Epoch 23_40.849%:  Training average Loss: 0.696849\n",
      "Epoch 23_41.650%:  Training average Loss: 0.697036\n",
      "Epoch 23_42.451%:  Training average Loss: 0.697281\n",
      "Epoch 23_43.252%:  Training average Loss: 0.698399\n",
      "Epoch 23_44.053%:  Training average Loss: 0.698141\n",
      "Epoch 23_44.854%:  Training average Loss: 0.698385\n",
      "Epoch 23_45.655%:  Training average Loss: 0.699176\n",
      "Epoch 23_46.456%:  Training average Loss: 0.699520\n",
      "Epoch 23_47.257%:  Training average Loss: 0.700605\n",
      "Epoch 23_48.058%:  Training average Loss: 0.700842\n",
      "Epoch 23_48.859%:  Training average Loss: 0.701087\n",
      "Epoch 23_49.660%:  Training average Loss: 0.701403\n",
      "Epoch 23_50.461%:  Training average Loss: 0.701682\n",
      "Epoch 23_51.262%:  Training average Loss: 0.701193\n",
      "Epoch 23_52.063%:  Training average Loss: 0.701632\n",
      "Epoch 23_52.864%:  Training average Loss: 0.701671\n",
      "Epoch 23_53.665%:  Training average Loss: 0.702234\n",
      "Epoch 23_54.466%:  Training average Loss: 0.702135\n",
      "Epoch 23_55.267%:  Training average Loss: 0.702337\n",
      "Epoch 23_56.068%:  Training average Loss: 0.702767\n",
      "Epoch 23_56.869%:  Training average Loss: 0.702873\n",
      "Epoch 23_57.670%:  Training average Loss: 0.703670\n",
      "Epoch 23_58.471%:  Training average Loss: 0.704285\n",
      "Epoch 23_59.272%:  Training average Loss: 0.704332\n",
      "Epoch 23_60.073%:  Training average Loss: 0.704449\n",
      "Epoch 23_60.874%:  Training average Loss: 0.703844\n",
      "Epoch 23_61.675%:  Training average Loss: 0.703999\n",
      "Epoch 23_62.475%:  Training average Loss: 0.704748\n",
      "Epoch 23_63.276%:  Training average Loss: 0.705029\n",
      "Epoch 23_64.077%:  Training average Loss: 0.705156\n",
      "Epoch 23_64.878%:  Training average Loss: 0.705283\n",
      "Epoch 23_65.679%:  Training average Loss: 0.705447\n",
      "Epoch 23_66.480%:  Training average Loss: 0.705013\n",
      "Epoch 23_67.281%:  Training average Loss: 0.705915\n",
      "Epoch 23_68.082%:  Training average Loss: 0.706355\n",
      "Epoch 23_68.883%:  Training average Loss: 0.706778\n",
      "Epoch 23_69.684%:  Training average Loss: 0.707152\n",
      "Epoch 23_70.485%:  Training average Loss: 0.707592\n",
      "Epoch 23_71.286%:  Training average Loss: 0.707396\n",
      "Epoch 23_72.087%:  Training average Loss: 0.707266\n",
      "Epoch 23_72.888%:  Training average Loss: 0.707212\n",
      "Epoch 23_73.689%:  Training average Loss: 0.707463\n",
      "Epoch 23_74.490%:  Training average Loss: 0.707745\n",
      "Epoch 23_75.291%:  Training average Loss: 0.707888\n",
      "Epoch 23_76.092%:  Training average Loss: 0.707664\n",
      "Epoch 23_76.893%:  Training average Loss: 0.707876\n",
      "Epoch 23_77.694%:  Training average Loss: 0.707977\n",
      "Epoch 23_78.495%:  Training average Loss: 0.708144\n",
      "Epoch 23_79.296%:  Training average Loss: 0.707812\n",
      "Epoch 23_80.097%:  Training average Loss: 0.708295\n",
      "Epoch 23_80.898%:  Training average Loss: 0.708523\n",
      "Epoch 23_81.699%:  Training average Loss: 0.708946\n",
      "Epoch 23_82.500%:  Training average Loss: 0.708991\n",
      "Epoch 23_83.301%:  Training average Loss: 0.709186\n",
      "Epoch 23_84.102%:  Training average Loss: 0.709203\n",
      "Epoch 23_84.903%:  Training average Loss: 0.708809\n",
      "Epoch 23_85.704%:  Training average Loss: 0.709094\n",
      "Epoch 23_86.504%:  Training average Loss: 0.708842\n",
      "Epoch 23_87.305%:  Training average Loss: 0.708675\n",
      "Epoch 23_88.106%:  Training average Loss: 0.708729\n",
      "Epoch 23_88.907%:  Training average Loss: 0.708433\n",
      "Epoch 23_89.708%:  Training average Loss: 0.708503\n",
      "Epoch 23_90.509%:  Training average Loss: 0.708629\n",
      "Epoch 23_91.310%:  Training average Loss: 0.708582\n",
      "Epoch 23_92.111%:  Training average Loss: 0.708575\n",
      "Epoch 23_92.912%:  Training average Loss: 0.708734\n",
      "Epoch 23_93.713%:  Training average Loss: 0.708626\n",
      "Epoch 23_94.514%:  Training average Loss: 0.708720\n",
      "Epoch 23_95.315%:  Training average Loss: 0.708778\n",
      "Epoch 23_96.116%:  Training average Loss: 0.708433\n",
      "Epoch 23_96.917%:  Training average Loss: 0.708763\n",
      "Epoch 23_97.718%:  Training average Loss: 0.708748\n",
      "Epoch 23_98.519%:  Training average Loss: 0.708544\n",
      "Epoch 23_99.320%:  Training average Loss: 0.708478\n",
      "Epoch 23 :  Verification average Loss: 0.864493, Verification accuracy: 65.479127%,Total Time:3873.083698\n",
      "Model is saved in model_dict/model_lstm/epoch_23_accuracy_0.654791\n",
      "Epoch 24_0.801%:  Training average Loss: 0.686020\n",
      "Epoch 24_1.602%:  Training average Loss: 0.693332\n",
      "Epoch 24_2.403%:  Training average Loss: 0.684182\n",
      "Epoch 24_3.204%:  Training average Loss: 0.682343\n",
      "Epoch 24_4.005%:  Training average Loss: 0.683915\n",
      "Epoch 24_4.806%:  Training average Loss: 0.682327\n",
      "Epoch 24_5.607%:  Training average Loss: 0.686844\n",
      "Epoch 24_6.408%:  Training average Loss: 0.686151\n",
      "Epoch 24_7.209%:  Training average Loss: 0.688349\n",
      "Epoch 24_8.010%:  Training average Loss: 0.690166\n",
      "Epoch 24_8.811%:  Training average Loss: 0.688913\n",
      "Epoch 24_9.612%:  Training average Loss: 0.689290\n",
      "Epoch 24_10.413%:  Training average Loss: 0.687089\n",
      "Epoch 24_11.214%:  Training average Loss: 0.688780\n",
      "Epoch 24_12.015%:  Training average Loss: 0.688729\n",
      "Epoch 24_12.815%:  Training average Loss: 0.689385\n",
      "Epoch 24_13.616%:  Training average Loss: 0.689198\n",
      "Epoch 24_14.417%:  Training average Loss: 0.688572\n",
      "Epoch 24_15.218%:  Training average Loss: 0.690824\n",
      "Epoch 24_16.019%:  Training average Loss: 0.691026\n",
      "Epoch 24_16.820%:  Training average Loss: 0.689820\n",
      "Epoch 24_17.621%:  Training average Loss: 0.690598\n",
      "Epoch 24_18.422%:  Training average Loss: 0.689545\n",
      "Epoch 24_19.223%:  Training average Loss: 0.691180\n",
      "Epoch 24_20.024%:  Training average Loss: 0.691461\n",
      "Epoch 24_20.825%:  Training average Loss: 0.691397\n",
      "Epoch 24_21.626%:  Training average Loss: 0.690725\n",
      "Epoch 24_22.427%:  Training average Loss: 0.689696\n",
      "Epoch 24_23.228%:  Training average Loss: 0.690291\n",
      "Epoch 24_24.029%:  Training average Loss: 0.689802\n",
      "Epoch 24_24.830%:  Training average Loss: 0.688598\n",
      "Epoch 24_25.631%:  Training average Loss: 0.688199\n",
      "Epoch 24_26.432%:  Training average Loss: 0.687993\n",
      "Epoch 24_27.233%:  Training average Loss: 0.688947\n",
      "Epoch 24_28.034%:  Training average Loss: 0.688919\n",
      "Epoch 24_28.835%:  Training average Loss: 0.689482\n",
      "Epoch 24_29.636%:  Training average Loss: 0.689985\n",
      "Epoch 24_30.437%:  Training average Loss: 0.689910\n",
      "Epoch 24_31.238%:  Training average Loss: 0.689249\n",
      "Epoch 24_32.039%:  Training average Loss: 0.690081\n",
      "Epoch 24_32.840%:  Training average Loss: 0.690386\n",
      "Epoch 24_33.641%:  Training average Loss: 0.691609\n",
      "Epoch 24_34.442%:  Training average Loss: 0.692048\n",
      "Epoch 24_35.243%:  Training average Loss: 0.692365\n",
      "Epoch 24_36.044%:  Training average Loss: 0.692067\n",
      "Epoch 24_36.845%:  Training average Loss: 0.691777\n",
      "Epoch 24_37.645%:  Training average Loss: 0.691284\n",
      "Epoch 24_38.446%:  Training average Loss: 0.690943\n",
      "Epoch 24_39.247%:  Training average Loss: 0.691005\n",
      "Epoch 24_40.048%:  Training average Loss: 0.691263\n",
      "Epoch 24_40.849%:  Training average Loss: 0.691712\n",
      "Epoch 24_41.650%:  Training average Loss: 0.691752\n",
      "Epoch 24_42.451%:  Training average Loss: 0.691619\n",
      "Epoch 24_43.252%:  Training average Loss: 0.691817\n",
      "Epoch 24_44.053%:  Training average Loss: 0.691645\n",
      "Epoch 24_44.854%:  Training average Loss: 0.691569\n",
      "Epoch 24_45.655%:  Training average Loss: 0.691659\n",
      "Epoch 24_46.456%:  Training average Loss: 0.692587\n",
      "Epoch 24_47.257%:  Training average Loss: 0.692720\n",
      "Epoch 24_48.058%:  Training average Loss: 0.693305\n",
      "Epoch 24_48.859%:  Training average Loss: 0.693229\n",
      "Epoch 24_49.660%:  Training average Loss: 0.693638\n",
      "Epoch 24_50.461%:  Training average Loss: 0.694513\n",
      "Epoch 24_51.262%:  Training average Loss: 0.694511\n",
      "Epoch 24_52.063%:  Training average Loss: 0.693872\n",
      "Epoch 24_52.864%:  Training average Loss: 0.693903\n",
      "Epoch 24_53.665%:  Training average Loss: 0.694030\n",
      "Epoch 24_54.466%:  Training average Loss: 0.693774\n",
      "Epoch 24_55.267%:  Training average Loss: 0.693323\n",
      "Epoch 24_56.068%:  Training average Loss: 0.693860\n",
      "Epoch 24_56.869%:  Training average Loss: 0.693690\n",
      "Epoch 24_57.670%:  Training average Loss: 0.693999\n",
      "Epoch 24_58.471%:  Training average Loss: 0.693611\n",
      "Epoch 24_59.272%:  Training average Loss: 0.694138\n",
      "Epoch 24_60.073%:  Training average Loss: 0.693880\n",
      "Epoch 24_60.874%:  Training average Loss: 0.694449\n",
      "Epoch 24_61.675%:  Training average Loss: 0.694929\n",
      "Epoch 24_62.475%:  Training average Loss: 0.695022\n",
      "Epoch 24_63.276%:  Training average Loss: 0.694924\n",
      "Epoch 24_64.077%:  Training average Loss: 0.695286\n",
      "Epoch 24_64.878%:  Training average Loss: 0.694871\n",
      "Epoch 24_65.679%:  Training average Loss: 0.695281\n",
      "Epoch 24_66.480%:  Training average Loss: 0.695389\n",
      "Epoch 24_67.281%:  Training average Loss: 0.695269\n",
      "Epoch 24_68.082%:  Training average Loss: 0.695700\n",
      "Epoch 24_68.883%:  Training average Loss: 0.695975\n",
      "Epoch 24_69.684%:  Training average Loss: 0.695777\n",
      "Epoch 24_70.485%:  Training average Loss: 0.695964\n",
      "Epoch 24_71.286%:  Training average Loss: 0.695879\n",
      "Epoch 24_72.087%:  Training average Loss: 0.696510\n",
      "Epoch 24_72.888%:  Training average Loss: 0.696433\n",
      "Epoch 24_73.689%:  Training average Loss: 0.696402\n",
      "Epoch 24_74.490%:  Training average Loss: 0.696828\n",
      "Epoch 24_75.291%:  Training average Loss: 0.697426\n",
      "Epoch 24_76.092%:  Training average Loss: 0.697530\n",
      "Epoch 24_76.893%:  Training average Loss: 0.697505\n",
      "Epoch 24_77.694%:  Training average Loss: 0.697874\n",
      "Epoch 24_78.495%:  Training average Loss: 0.697770\n",
      "Epoch 24_79.296%:  Training average Loss: 0.698314\n",
      "Epoch 24_80.097%:  Training average Loss: 0.698542\n",
      "Epoch 24_80.898%:  Training average Loss: 0.698399\n",
      "Epoch 24_81.699%:  Training average Loss: 0.698055\n",
      "Epoch 24_82.500%:  Training average Loss: 0.698194\n",
      "Epoch 24_83.301%:  Training average Loss: 0.698077\n",
      "Epoch 24_84.102%:  Training average Loss: 0.697941\n",
      "Epoch 24_84.903%:  Training average Loss: 0.698133\n",
      "Epoch 24_85.704%:  Training average Loss: 0.698205\n",
      "Epoch 24_86.504%:  Training average Loss: 0.698317\n",
      "Epoch 24_87.305%:  Training average Loss: 0.698803\n",
      "Epoch 24_88.106%:  Training average Loss: 0.699062\n",
      "Epoch 24_88.907%:  Training average Loss: 0.699129\n",
      "Epoch 24_89.708%:  Training average Loss: 0.699255\n",
      "Epoch 24_90.509%:  Training average Loss: 0.699385\n",
      "Epoch 24_91.310%:  Training average Loss: 0.699588\n",
      "Epoch 24_92.111%:  Training average Loss: 0.699265\n",
      "Epoch 24_92.912%:  Training average Loss: 0.699589\n",
      "Epoch 24_93.713%:  Training average Loss: 0.699413\n",
      "Epoch 24_94.514%:  Training average Loss: 0.699650\n",
      "Epoch 24_95.315%:  Training average Loss: 0.699996\n",
      "Epoch 24_96.116%:  Training average Loss: 0.699827\n",
      "Epoch 24_96.917%:  Training average Loss: 0.700325\n",
      "Epoch 24_97.718%:  Training average Loss: 0.700282\n",
      "Epoch 24_98.519%:  Training average Loss: 0.700426\n",
      "Epoch 24_99.320%:  Training average Loss: 0.700772\n",
      "Epoch 24 :  Verification average Loss: 0.852384, Verification accuracy: 65.193990%,Total Time:4033.391480\n",
      "Epoch 25_0.801%:  Training average Loss: 0.650728\n",
      "Epoch 25_1.602%:  Training average Loss: 0.645233\n",
      "Epoch 25_2.403%:  Training average Loss: 0.652727\n",
      "Epoch 25_3.204%:  Training average Loss: 0.665595\n",
      "Epoch 25_4.005%:  Training average Loss: 0.673428\n",
      "Epoch 25_4.806%:  Training average Loss: 0.669785\n",
      "Epoch 25_5.607%:  Training average Loss: 0.679492\n",
      "Epoch 25_6.408%:  Training average Loss: 0.681031\n",
      "Epoch 25_7.209%:  Training average Loss: 0.681529\n",
      "Epoch 25_8.010%:  Training average Loss: 0.677025\n",
      "Epoch 25_8.811%:  Training average Loss: 0.678981\n",
      "Epoch 25_9.612%:  Training average Loss: 0.679455\n",
      "Epoch 25_10.413%:  Training average Loss: 0.681465\n",
      "Epoch 25_11.214%:  Training average Loss: 0.680887\n",
      "Epoch 25_12.015%:  Training average Loss: 0.681614\n",
      "Epoch 25_12.815%:  Training average Loss: 0.685559\n",
      "Epoch 25_13.616%:  Training average Loss: 0.684809\n",
      "Epoch 25_14.417%:  Training average Loss: 0.684011\n",
      "Epoch 25_15.218%:  Training average Loss: 0.683151\n",
      "Epoch 25_16.019%:  Training average Loss: 0.683645\n",
      "Epoch 25_16.820%:  Training average Loss: 0.683421\n",
      "Epoch 25_17.621%:  Training average Loss: 0.685462\n",
      "Epoch 25_18.422%:  Training average Loss: 0.683585\n",
      "Epoch 25_19.223%:  Training average Loss: 0.683657\n",
      "Epoch 25_20.024%:  Training average Loss: 0.684471\n",
      "Epoch 25_20.825%:  Training average Loss: 0.685865\n",
      "Epoch 25_21.626%:  Training average Loss: 0.685124\n",
      "Epoch 25_22.427%:  Training average Loss: 0.685672\n",
      "Epoch 25_23.228%:  Training average Loss: 0.687581\n",
      "Epoch 25_24.029%:  Training average Loss: 0.688181\n",
      "Epoch 25_24.830%:  Training average Loss: 0.688815\n",
      "Epoch 25_25.631%:  Training average Loss: 0.689419\n",
      "Epoch 25_26.432%:  Training average Loss: 0.688214\n",
      "Epoch 25_27.233%:  Training average Loss: 0.687490\n",
      "Epoch 25_28.034%:  Training average Loss: 0.687133\n",
      "Epoch 25_28.835%:  Training average Loss: 0.686955\n",
      "Epoch 25_29.636%:  Training average Loss: 0.686466\n",
      "Epoch 25_30.437%:  Training average Loss: 0.685452\n",
      "Epoch 25_31.238%:  Training average Loss: 0.685219\n",
      "Epoch 25_32.039%:  Training average Loss: 0.685247\n",
      "Epoch 25_32.840%:  Training average Loss: 0.685412\n",
      "Epoch 25_33.641%:  Training average Loss: 0.685623\n",
      "Epoch 25_34.442%:  Training average Loss: 0.686698\n",
      "Epoch 25_35.243%:  Training average Loss: 0.686935\n",
      "Epoch 25_36.044%:  Training average Loss: 0.686615\n",
      "Epoch 25_36.845%:  Training average Loss: 0.686583\n",
      "Epoch 25_37.645%:  Training average Loss: 0.686413\n",
      "Epoch 25_38.446%:  Training average Loss: 0.686119\n",
      "Epoch 25_39.247%:  Training average Loss: 0.686536\n",
      "Epoch 25_40.048%:  Training average Loss: 0.686627\n",
      "Epoch 25_40.849%:  Training average Loss: 0.686832\n",
      "Epoch 25_41.650%:  Training average Loss: 0.687168\n",
      "Epoch 25_42.451%:  Training average Loss: 0.687669\n",
      "Epoch 25_43.252%:  Training average Loss: 0.687341\n",
      "Epoch 25_44.053%:  Training average Loss: 0.688473\n",
      "Epoch 25_44.854%:  Training average Loss: 0.690155\n",
      "Epoch 25_45.655%:  Training average Loss: 0.690438\n",
      "Epoch 25_46.456%:  Training average Loss: 0.691449\n",
      "Epoch 25_47.257%:  Training average Loss: 0.691961\n",
      "Epoch 25_48.058%:  Training average Loss: 0.692461\n",
      "Epoch 25_48.859%:  Training average Loss: 0.692563\n",
      "Epoch 25_49.660%:  Training average Loss: 0.692762\n",
      "Epoch 25_50.461%:  Training average Loss: 0.693134\n",
      "Epoch 25_51.262%:  Training average Loss: 0.693386\n",
      "Epoch 25_52.063%:  Training average Loss: 0.693655\n",
      "Epoch 25_52.864%:  Training average Loss: 0.693303\n",
      "Epoch 25_53.665%:  Training average Loss: 0.692819\n",
      "Epoch 25_54.466%:  Training average Loss: 0.692275\n",
      "Epoch 25_55.267%:  Training average Loss: 0.692734\n",
      "Epoch 25_56.068%:  Training average Loss: 0.693540\n",
      "Epoch 25_56.869%:  Training average Loss: 0.693132\n",
      "Epoch 25_57.670%:  Training average Loss: 0.693224\n",
      "Epoch 25_58.471%:  Training average Loss: 0.693744\n",
      "Epoch 25_59.272%:  Training average Loss: 0.693736\n",
      "Epoch 25_60.073%:  Training average Loss: 0.693768\n",
      "Epoch 25_60.874%:  Training average Loss: 0.694104\n",
      "Epoch 25_61.675%:  Training average Loss: 0.694885\n",
      "Epoch 25_62.475%:  Training average Loss: 0.694836\n",
      "Epoch 25_63.276%:  Training average Loss: 0.694791\n",
      "Epoch 25_64.077%:  Training average Loss: 0.695267\n",
      "Epoch 25_64.878%:  Training average Loss: 0.695993\n",
      "Epoch 25_65.679%:  Training average Loss: 0.696150\n",
      "Epoch 25_66.480%:  Training average Loss: 0.696468\n",
      "Epoch 25_67.281%:  Training average Loss: 0.696509\n",
      "Epoch 25_68.082%:  Training average Loss: 0.696898\n",
      "Epoch 25_68.883%:  Training average Loss: 0.697377\n",
      "Epoch 25_69.684%:  Training average Loss: 0.697560\n",
      "Epoch 25_70.485%:  Training average Loss: 0.697582\n",
      "Epoch 25_71.286%:  Training average Loss: 0.697320\n",
      "Epoch 25_72.087%:  Training average Loss: 0.697987\n",
      "Epoch 25_72.888%:  Training average Loss: 0.698661\n",
      "Epoch 25_73.689%:  Training average Loss: 0.698748\n",
      "Epoch 25_74.490%:  Training average Loss: 0.698530\n",
      "Epoch 25_75.291%:  Training average Loss: 0.698432\n",
      "Epoch 25_76.092%:  Training average Loss: 0.698983\n",
      "Epoch 25_76.893%:  Training average Loss: 0.698872\n",
      "Epoch 25_77.694%:  Training average Loss: 0.698998\n",
      "Epoch 25_78.495%:  Training average Loss: 0.698568\n",
      "Epoch 25_79.296%:  Training average Loss: 0.698808\n",
      "Epoch 25_80.097%:  Training average Loss: 0.698900\n",
      "Epoch 25_80.898%:  Training average Loss: 0.698674\n",
      "Epoch 25_81.699%:  Training average Loss: 0.698619\n",
      "Epoch 25_82.500%:  Training average Loss: 0.698559\n",
      "Epoch 25_83.301%:  Training average Loss: 0.698444\n",
      "Epoch 25_84.102%:  Training average Loss: 0.698609\n",
      "Epoch 25_84.903%:  Training average Loss: 0.699365\n",
      "Epoch 25_85.704%:  Training average Loss: 0.699495\n",
      "Epoch 25_86.504%:  Training average Loss: 0.699558\n",
      "Epoch 25_87.305%:  Training average Loss: 0.699007\n",
      "Epoch 25_88.106%:  Training average Loss: 0.699018\n",
      "Epoch 25_88.907%:  Training average Loss: 0.698945\n",
      "Epoch 25_89.708%:  Training average Loss: 0.699320\n",
      "Epoch 25_90.509%:  Training average Loss: 0.699588\n",
      "Epoch 25_91.310%:  Training average Loss: 0.699453\n",
      "Epoch 25_92.111%:  Training average Loss: 0.699339\n",
      "Epoch 25_92.912%:  Training average Loss: 0.699810\n",
      "Epoch 25_93.713%:  Training average Loss: 0.700163\n",
      "Epoch 25_94.514%:  Training average Loss: 0.699992\n",
      "Epoch 25_95.315%:  Training average Loss: 0.700535\n",
      "Epoch 25_96.116%:  Training average Loss: 0.700426\n",
      "Epoch 25_96.917%:  Training average Loss: 0.700737\n",
      "Epoch 25_97.718%:  Training average Loss: 0.700869\n",
      "Epoch 25_98.519%:  Training average Loss: 0.701011\n",
      "Epoch 25_99.320%:  Training average Loss: 0.701315\n",
      "Epoch 25 :  Verification average Loss: 0.872631, Verification accuracy: 65.299715%,Total Time:4194.486950\n",
      "Epoch 26_0.801%:  Training average Loss: 0.710219\n",
      "Epoch 26_1.602%:  Training average Loss: 0.686692\n",
      "Epoch 26_2.403%:  Training average Loss: 0.677559\n",
      "Epoch 26_3.204%:  Training average Loss: 0.675239\n",
      "Epoch 26_4.005%:  Training average Loss: 0.676220\n",
      "Epoch 26_4.806%:  Training average Loss: 0.674810\n",
      "Epoch 26_5.607%:  Training average Loss: 0.678794\n",
      "Epoch 26_6.408%:  Training average Loss: 0.675612\n",
      "Epoch 26_7.209%:  Training average Loss: 0.677209\n",
      "Epoch 26_8.010%:  Training average Loss: 0.674824\n",
      "Epoch 26_8.811%:  Training average Loss: 0.678679\n",
      "Epoch 26_9.612%:  Training average Loss: 0.678439\n",
      "Epoch 26_10.413%:  Training average Loss: 0.677534\n",
      "Epoch 26_11.214%:  Training average Loss: 0.677468\n",
      "Epoch 26_12.015%:  Training average Loss: 0.675900\n",
      "Epoch 26_12.815%:  Training average Loss: 0.675318\n",
      "Epoch 26_13.616%:  Training average Loss: 0.678384\n",
      "Epoch 26_14.417%:  Training average Loss: 0.678858\n",
      "Epoch 26_15.218%:  Training average Loss: 0.679516\n",
      "Epoch 26_16.019%:  Training average Loss: 0.681617\n",
      "Epoch 26_16.820%:  Training average Loss: 0.682631\n",
      "Epoch 26_17.621%:  Training average Loss: 0.683474\n",
      "Epoch 26_18.422%:  Training average Loss: 0.681836\n",
      "Epoch 26_19.223%:  Training average Loss: 0.683217\n",
      "Epoch 26_20.024%:  Training average Loss: 0.683735\n",
      "Epoch 26_20.825%:  Training average Loss: 0.685128\n",
      "Epoch 26_21.626%:  Training average Loss: 0.686173\n",
      "Epoch 26_22.427%:  Training average Loss: 0.685572\n",
      "Epoch 26_23.228%:  Training average Loss: 0.687008\n",
      "Epoch 26_24.029%:  Training average Loss: 0.689592\n",
      "Epoch 26_24.830%:  Training average Loss: 0.690894\n",
      "Epoch 26_25.631%:  Training average Loss: 0.691602\n",
      "Epoch 26_26.432%:  Training average Loss: 0.690933\n",
      "Epoch 26_27.233%:  Training average Loss: 0.691856\n",
      "Epoch 26_28.034%:  Training average Loss: 0.692041\n",
      "Epoch 26_28.835%:  Training average Loss: 0.692602\n",
      "Epoch 26_29.636%:  Training average Loss: 0.693044\n",
      "Epoch 26_30.437%:  Training average Loss: 0.693530\n",
      "Epoch 26_31.238%:  Training average Loss: 0.693497\n",
      "Epoch 26_32.039%:  Training average Loss: 0.693348\n",
      "Epoch 26_32.840%:  Training average Loss: 0.692616\n",
      "Epoch 26_33.641%:  Training average Loss: 0.694078\n",
      "Epoch 26_34.442%:  Training average Loss: 0.693307\n",
      "Epoch 26_35.243%:  Training average Loss: 0.693422\n",
      "Epoch 26_36.044%:  Training average Loss: 0.692950\n",
      "Epoch 26_36.845%:  Training average Loss: 0.692534\n",
      "Epoch 26_37.645%:  Training average Loss: 0.692435\n",
      "Epoch 26_38.446%:  Training average Loss: 0.692224\n",
      "Epoch 26_39.247%:  Training average Loss: 0.692382\n",
      "Epoch 26_40.048%:  Training average Loss: 0.692232\n",
      "Epoch 26_40.849%:  Training average Loss: 0.692945\n",
      "Epoch 26_41.650%:  Training average Loss: 0.692239\n",
      "Epoch 26_42.451%:  Training average Loss: 0.691969\n",
      "Epoch 26_43.252%:  Training average Loss: 0.691383\n",
      "Epoch 26_44.053%:  Training average Loss: 0.691772\n",
      "Epoch 26_44.854%:  Training average Loss: 0.692483\n",
      "Epoch 26_45.655%:  Training average Loss: 0.692572\n",
      "Epoch 26_46.456%:  Training average Loss: 0.692774\n",
      "Epoch 26_47.257%:  Training average Loss: 0.692414\n",
      "Epoch 26_48.058%:  Training average Loss: 0.692289\n",
      "Epoch 26_48.859%:  Training average Loss: 0.692412\n",
      "Epoch 26_49.660%:  Training average Loss: 0.692666\n",
      "Epoch 26_50.461%:  Training average Loss: 0.693122\n",
      "Epoch 26_51.262%:  Training average Loss: 0.693128\n",
      "Epoch 26_52.063%:  Training average Loss: 0.693451\n",
      "Epoch 26_52.864%:  Training average Loss: 0.693494\n",
      "Epoch 26_53.665%:  Training average Loss: 0.693668\n",
      "Epoch 26_54.466%:  Training average Loss: 0.694240\n",
      "Epoch 26_55.267%:  Training average Loss: 0.694226\n",
      "Epoch 26_56.068%:  Training average Loss: 0.694930\n",
      "Epoch 26_56.869%:  Training average Loss: 0.694332\n",
      "Epoch 26_57.670%:  Training average Loss: 0.695090\n",
      "Epoch 26_58.471%:  Training average Loss: 0.695299\n",
      "Epoch 26_59.272%:  Training average Loss: 0.694936\n",
      "Epoch 26_60.073%:  Training average Loss: 0.694872\n",
      "Epoch 26_60.874%:  Training average Loss: 0.695043\n",
      "Epoch 26_61.675%:  Training average Loss: 0.695215\n",
      "Epoch 26_62.475%:  Training average Loss: 0.694822\n",
      "Epoch 26_63.276%:  Training average Loss: 0.694206\n",
      "Epoch 26_64.077%:  Training average Loss: 0.694481\n",
      "Epoch 26_64.878%:  Training average Loss: 0.694594\n",
      "Epoch 26_65.679%:  Training average Loss: 0.694831\n",
      "Epoch 26_66.480%:  Training average Loss: 0.695430\n",
      "Epoch 26_67.281%:  Training average Loss: 0.695290\n",
      "Epoch 26_68.082%:  Training average Loss: 0.694857\n",
      "Epoch 26_68.883%:  Training average Loss: 0.695061\n",
      "Epoch 26_69.684%:  Training average Loss: 0.695340\n",
      "Epoch 26_70.485%:  Training average Loss: 0.695901\n",
      "Epoch 26_71.286%:  Training average Loss: 0.696159\n",
      "Epoch 26_72.087%:  Training average Loss: 0.696291\n",
      "Epoch 26_72.888%:  Training average Loss: 0.696229\n",
      "Epoch 26_73.689%:  Training average Loss: 0.696420\n",
      "Epoch 26_74.490%:  Training average Loss: 0.696835\n",
      "Epoch 26_75.291%:  Training average Loss: 0.697571\n",
      "Epoch 26_76.092%:  Training average Loss: 0.697960\n",
      "Epoch 26_76.893%:  Training average Loss: 0.698081\n",
      "Epoch 26_77.694%:  Training average Loss: 0.698005\n",
      "Epoch 26_78.495%:  Training average Loss: 0.698259\n",
      "Epoch 26_79.296%:  Training average Loss: 0.698103\n",
      "Epoch 26_80.097%:  Training average Loss: 0.697932\n",
      "Epoch 26_80.898%:  Training average Loss: 0.698137\n",
      "Epoch 26_81.699%:  Training average Loss: 0.698387\n",
      "Epoch 26_82.500%:  Training average Loss: 0.697967\n",
      "Epoch 26_83.301%:  Training average Loss: 0.697727\n",
      "Epoch 26_84.102%:  Training average Loss: 0.697011\n",
      "Epoch 26_84.903%:  Training average Loss: 0.696839\n",
      "Epoch 26_85.704%:  Training average Loss: 0.696744\n",
      "Epoch 26_86.504%:  Training average Loss: 0.696842\n",
      "Epoch 26_87.305%:  Training average Loss: 0.697345\n",
      "Epoch 26_88.106%:  Training average Loss: 0.697357\n",
      "Epoch 26_88.907%:  Training average Loss: 0.697458\n",
      "Epoch 26_89.708%:  Training average Loss: 0.697575\n",
      "Epoch 26_90.509%:  Training average Loss: 0.697812\n",
      "Epoch 26_91.310%:  Training average Loss: 0.697779\n",
      "Epoch 26_92.111%:  Training average Loss: 0.697958\n",
      "Epoch 26_92.912%:  Training average Loss: 0.698150\n",
      "Epoch 26_93.713%:  Training average Loss: 0.697972\n",
      "Epoch 26_94.514%:  Training average Loss: 0.698234\n",
      "Epoch 26_95.315%:  Training average Loss: 0.697965\n",
      "Epoch 26_96.116%:  Training average Loss: 0.698421\n",
      "Epoch 26_96.917%:  Training average Loss: 0.698471\n",
      "Epoch 26_97.718%:  Training average Loss: 0.698359\n",
      "Epoch 26_98.519%:  Training average Loss: 0.698837\n",
      "Epoch 26_99.320%:  Training average Loss: 0.698582\n",
      "Epoch 26 :  Verification average Loss: 0.861121, Verification accuracy: 65.466312%,Total Time:4356.430545\n",
      "Epoch 27_0.801%:  Training average Loss: 0.671699\n",
      "Epoch 27_1.602%:  Training average Loss: 0.670826\n",
      "Epoch 27_2.403%:  Training average Loss: 0.684077\n",
      "Epoch 27_3.204%:  Training average Loss: 0.686927\n",
      "Epoch 27_4.005%:  Training average Loss: 0.681615\n",
      "Epoch 27_4.806%:  Training average Loss: 0.683601\n",
      "Epoch 27_5.607%:  Training average Loss: 0.674538\n",
      "Epoch 27_6.408%:  Training average Loss: 0.669260\n",
      "Epoch 27_7.209%:  Training average Loss: 0.671453\n",
      "Epoch 27_8.010%:  Training average Loss: 0.669569\n",
      "Epoch 27_8.811%:  Training average Loss: 0.672380\n",
      "Epoch 27_9.612%:  Training average Loss: 0.669938\n",
      "Epoch 27_10.413%:  Training average Loss: 0.672345\n",
      "Epoch 27_11.214%:  Training average Loss: 0.674181\n",
      "Epoch 27_12.015%:  Training average Loss: 0.674015\n",
      "Epoch 27_12.815%:  Training average Loss: 0.678816\n",
      "Epoch 27_13.616%:  Training average Loss: 0.677477\n",
      "Epoch 27_14.417%:  Training average Loss: 0.678608\n",
      "Epoch 27_15.218%:  Training average Loss: 0.680091\n",
      "Epoch 27_16.019%:  Training average Loss: 0.679921\n",
      "Epoch 27_16.820%:  Training average Loss: 0.680272\n",
      "Epoch 27_17.621%:  Training average Loss: 0.680311\n",
      "Epoch 27_18.422%:  Training average Loss: 0.681517\n",
      "Epoch 27_19.223%:  Training average Loss: 0.682593\n",
      "Epoch 27_20.024%:  Training average Loss: 0.684372\n",
      "Epoch 27_20.825%:  Training average Loss: 0.684883\n",
      "Epoch 27_21.626%:  Training average Loss: 0.685809\n",
      "Epoch 27_22.427%:  Training average Loss: 0.685968\n",
      "Epoch 27_23.228%:  Training average Loss: 0.687508\n",
      "Epoch 27_24.029%:  Training average Loss: 0.688686\n",
      "Epoch 27_24.830%:  Training average Loss: 0.689865\n",
      "Epoch 27_25.631%:  Training average Loss: 0.690529\n",
      "Epoch 27_26.432%:  Training average Loss: 0.690685\n",
      "Epoch 27_27.233%:  Training average Loss: 0.690528\n",
      "Epoch 27_28.034%:  Training average Loss: 0.690881\n",
      "Epoch 27_28.835%:  Training average Loss: 0.690849\n",
      "Epoch 27_29.636%:  Training average Loss: 0.690817\n",
      "Epoch 27_30.437%:  Training average Loss: 0.690747\n",
      "Epoch 27_31.238%:  Training average Loss: 0.691459\n",
      "Epoch 27_32.039%:  Training average Loss: 0.690038\n",
      "Epoch 27_32.840%:  Training average Loss: 0.689528\n",
      "Epoch 27_33.641%:  Training average Loss: 0.688693\n",
      "Epoch 27_34.442%:  Training average Loss: 0.687993\n",
      "Epoch 27_35.243%:  Training average Loss: 0.687877\n",
      "Epoch 27_36.044%:  Training average Loss: 0.689328\n",
      "Epoch 27_36.845%:  Training average Loss: 0.689617\n",
      "Epoch 27_37.645%:  Training average Loss: 0.690070\n",
      "Epoch 27_38.446%:  Training average Loss: 0.690680\n",
      "Epoch 27_39.247%:  Training average Loss: 0.690716\n",
      "Epoch 27_40.048%:  Training average Loss: 0.691147\n",
      "Epoch 27_40.849%:  Training average Loss: 0.691020\n",
      "Epoch 27_41.650%:  Training average Loss: 0.690565\n",
      "Epoch 27_42.451%:  Training average Loss: 0.691097\n",
      "Epoch 27_43.252%:  Training average Loss: 0.690004\n",
      "Epoch 27_44.053%:  Training average Loss: 0.690600\n",
      "Epoch 27_44.854%:  Training average Loss: 0.690188\n",
      "Epoch 27_45.655%:  Training average Loss: 0.691228\n",
      "Epoch 27_46.456%:  Training average Loss: 0.691378\n",
      "Epoch 27_47.257%:  Training average Loss: 0.692122\n",
      "Epoch 27_48.058%:  Training average Loss: 0.692777\n",
      "Epoch 27_48.859%:  Training average Loss: 0.692977\n",
      "Epoch 27_49.660%:  Training average Loss: 0.692554\n",
      "Epoch 27_50.461%:  Training average Loss: 0.692673\n",
      "Epoch 27_51.262%:  Training average Loss: 0.692346\n",
      "Epoch 27_52.063%:  Training average Loss: 0.691820\n",
      "Epoch 27_52.864%:  Training average Loss: 0.691858\n",
      "Epoch 27_53.665%:  Training average Loss: 0.693053\n",
      "Epoch 27_54.466%:  Training average Loss: 0.692934\n",
      "Epoch 27_55.267%:  Training average Loss: 0.692502\n",
      "Epoch 27_56.068%:  Training average Loss: 0.692769\n",
      "Epoch 27_56.869%:  Training average Loss: 0.692950\n",
      "Epoch 27_57.670%:  Training average Loss: 0.693709\n",
      "Epoch 27_58.471%:  Training average Loss: 0.693016\n",
      "Epoch 27_59.272%:  Training average Loss: 0.693769\n",
      "Epoch 27_60.073%:  Training average Loss: 0.693912\n",
      "Epoch 27_60.874%:  Training average Loss: 0.693811\n",
      "Epoch 27_61.675%:  Training average Loss: 0.694104\n",
      "Epoch 27_62.475%:  Training average Loss: 0.694455\n",
      "Epoch 27_63.276%:  Training average Loss: 0.694631\n",
      "Epoch 27_64.077%:  Training average Loss: 0.694637\n",
      "Epoch 27_64.878%:  Training average Loss: 0.694750\n",
      "Epoch 27_65.679%:  Training average Loss: 0.694731\n",
      "Epoch 27_66.480%:  Training average Loss: 0.694563\n",
      "Epoch 27_67.281%:  Training average Loss: 0.694501\n",
      "Epoch 27_68.082%:  Training average Loss: 0.694889\n",
      "Epoch 27_68.883%:  Training average Loss: 0.694608\n",
      "Epoch 27_69.684%:  Training average Loss: 0.694331\n",
      "Epoch 27_70.485%:  Training average Loss: 0.694623\n",
      "Epoch 27_71.286%:  Training average Loss: 0.694060\n",
      "Epoch 27_72.087%:  Training average Loss: 0.694479\n",
      "Epoch 27_72.888%:  Training average Loss: 0.694609\n",
      "Epoch 27_73.689%:  Training average Loss: 0.694782\n",
      "Epoch 27_74.490%:  Training average Loss: 0.694907\n",
      "Epoch 27_75.291%:  Training average Loss: 0.694515\n",
      "Epoch 27_76.092%:  Training average Loss: 0.694573\n",
      "Epoch 27_76.893%:  Training average Loss: 0.694578\n",
      "Epoch 27_77.694%:  Training average Loss: 0.694407\n",
      "Epoch 27_78.495%:  Training average Loss: 0.694875\n",
      "Epoch 27_79.296%:  Training average Loss: 0.695667\n",
      "Epoch 27_80.097%:  Training average Loss: 0.695183\n",
      "Epoch 27_80.898%:  Training average Loss: 0.694787\n",
      "Epoch 27_81.699%:  Training average Loss: 0.695008\n",
      "Epoch 27_82.500%:  Training average Loss: 0.694800\n",
      "Epoch 27_83.301%:  Training average Loss: 0.695062\n",
      "Epoch 27_84.102%:  Training average Loss: 0.695568\n",
      "Epoch 27_84.903%:  Training average Loss: 0.696041\n",
      "Epoch 27_85.704%:  Training average Loss: 0.695698\n",
      "Epoch 27_86.504%:  Training average Loss: 0.696247\n",
      "Epoch 27_87.305%:  Training average Loss: 0.696679\n",
      "Epoch 27_88.106%:  Training average Loss: 0.696591\n",
      "Epoch 27_88.907%:  Training average Loss: 0.696505\n",
      "Epoch 27_89.708%:  Training average Loss: 0.696693\n",
      "Epoch 27_90.509%:  Training average Loss: 0.696734\n",
      "Epoch 27_91.310%:  Training average Loss: 0.697009\n",
      "Epoch 27_92.111%:  Training average Loss: 0.697005\n",
      "Epoch 27_92.912%:  Training average Loss: 0.697028\n",
      "Epoch 27_93.713%:  Training average Loss: 0.696901\n",
      "Epoch 27_94.514%:  Training average Loss: 0.696974\n",
      "Epoch 27_95.315%:  Training average Loss: 0.697051\n",
      "Epoch 27_96.116%:  Training average Loss: 0.696975\n",
      "Epoch 27_96.917%:  Training average Loss: 0.696783\n",
      "Epoch 27_97.718%:  Training average Loss: 0.696434\n",
      "Epoch 27_98.519%:  Training average Loss: 0.696577\n",
      "Epoch 27_99.320%:  Training average Loss: 0.696916\n",
      "Epoch 27 :  Verification average Loss: 0.860593, Verification accuracy: 65.376606%,Total Time:4513.586463\n",
      "Epoch 28_0.801%:  Training average Loss: 0.687524\n",
      "Epoch 28_1.602%:  Training average Loss: 0.681560\n",
      "Epoch 28_2.403%:  Training average Loss: 0.679580\n",
      "Epoch 28_3.204%:  Training average Loss: 0.673628\n",
      "Epoch 28_4.005%:  Training average Loss: 0.676990\n",
      "Epoch 28_4.806%:  Training average Loss: 0.671419\n",
      "Epoch 28_5.607%:  Training average Loss: 0.666781\n",
      "Epoch 28_6.408%:  Training average Loss: 0.668229\n",
      "Epoch 28_7.209%:  Training average Loss: 0.673509\n",
      "Epoch 28_8.010%:  Training average Loss: 0.676829\n",
      "Epoch 28_8.811%:  Training average Loss: 0.675168\n",
      "Epoch 28_9.612%:  Training average Loss: 0.674862\n",
      "Epoch 28_10.413%:  Training average Loss: 0.675596\n",
      "Epoch 28_11.214%:  Training average Loss: 0.676259\n",
      "Epoch 28_12.015%:  Training average Loss: 0.675963\n",
      "Epoch 28_12.815%:  Training average Loss: 0.679700\n",
      "Epoch 28_13.616%:  Training average Loss: 0.681505\n",
      "Epoch 28_14.417%:  Training average Loss: 0.679824\n",
      "Epoch 28_15.218%:  Training average Loss: 0.679906\n",
      "Epoch 28_16.019%:  Training average Loss: 0.679714\n",
      "Epoch 28_16.820%:  Training average Loss: 0.680100\n",
      "Epoch 28_17.621%:  Training average Loss: 0.680098\n",
      "Epoch 28_18.422%:  Training average Loss: 0.679358\n",
      "Epoch 28_19.223%:  Training average Loss: 0.680571\n",
      "Epoch 28_20.024%:  Training average Loss: 0.681238\n",
      "Epoch 28_20.825%:  Training average Loss: 0.681234\n",
      "Epoch 28_21.626%:  Training average Loss: 0.682058\n",
      "Epoch 28_22.427%:  Training average Loss: 0.683351\n",
      "Epoch 28_23.228%:  Training average Loss: 0.683914\n",
      "Epoch 28_24.029%:  Training average Loss: 0.682828\n",
      "Epoch 28_24.830%:  Training average Loss: 0.681714\n",
      "Epoch 28_25.631%:  Training average Loss: 0.682105\n",
      "Epoch 28_26.432%:  Training average Loss: 0.682027\n",
      "Epoch 28_27.233%:  Training average Loss: 0.682128\n",
      "Epoch 28_28.034%:  Training average Loss: 0.682813\n",
      "Epoch 28_28.835%:  Training average Loss: 0.682372\n",
      "Epoch 28_29.636%:  Training average Loss: 0.682566\n",
      "Epoch 28_30.437%:  Training average Loss: 0.681510\n",
      "Epoch 28_31.238%:  Training average Loss: 0.680999\n",
      "Epoch 28_32.039%:  Training average Loss: 0.682016\n",
      "Epoch 28_32.840%:  Training average Loss: 0.682073\n",
      "Epoch 28_33.641%:  Training average Loss: 0.682114\n",
      "Epoch 28_34.442%:  Training average Loss: 0.682431\n",
      "Epoch 28_35.243%:  Training average Loss: 0.683605\n",
      "Epoch 28_36.044%:  Training average Loss: 0.683317\n",
      "Epoch 28_36.845%:  Training average Loss: 0.684376\n",
      "Epoch 28_37.645%:  Training average Loss: 0.684123\n",
      "Epoch 28_38.446%:  Training average Loss: 0.684235\n",
      "Epoch 28_39.247%:  Training average Loss: 0.684130\n",
      "Epoch 28_40.048%:  Training average Loss: 0.685132\n",
      "Epoch 28_40.849%:  Training average Loss: 0.684451\n",
      "Epoch 28_41.650%:  Training average Loss: 0.684903\n",
      "Epoch 28_42.451%:  Training average Loss: 0.685284\n",
      "Epoch 28_43.252%:  Training average Loss: 0.685274\n",
      "Epoch 28_44.053%:  Training average Loss: 0.685187\n",
      "Epoch 28_44.854%:  Training average Loss: 0.685851\n",
      "Epoch 28_45.655%:  Training average Loss: 0.686556\n",
      "Epoch 28_46.456%:  Training average Loss: 0.687111\n",
      "Epoch 28_47.257%:  Training average Loss: 0.687886\n",
      "Epoch 28_48.058%:  Training average Loss: 0.688531\n",
      "Epoch 28_48.859%:  Training average Loss: 0.688534\n",
      "Epoch 28_49.660%:  Training average Loss: 0.688989\n",
      "Epoch 28_50.461%:  Training average Loss: 0.689145\n",
      "Epoch 28_51.262%:  Training average Loss: 0.688964\n",
      "Epoch 28_52.063%:  Training average Loss: 0.689195\n",
      "Epoch 28_52.864%:  Training average Loss: 0.689342\n",
      "Epoch 28_53.665%:  Training average Loss: 0.688917\n",
      "Epoch 28_54.466%:  Training average Loss: 0.688373\n",
      "Epoch 28_55.267%:  Training average Loss: 0.688782\n",
      "Epoch 28_56.068%:  Training average Loss: 0.689217\n",
      "Epoch 28_56.869%:  Training average Loss: 0.689266\n",
      "Epoch 28_57.670%:  Training average Loss: 0.689274\n",
      "Epoch 28_58.471%:  Training average Loss: 0.689470\n",
      "Epoch 28_59.272%:  Training average Loss: 0.689658\n",
      "Epoch 28_60.073%:  Training average Loss: 0.690046\n",
      "Epoch 28_60.874%:  Training average Loss: 0.689879\n",
      "Epoch 28_61.675%:  Training average Loss: 0.689827\n",
      "Epoch 28_62.475%:  Training average Loss: 0.689461\n",
      "Epoch 28_63.276%:  Training average Loss: 0.689793\n",
      "Epoch 28_64.077%:  Training average Loss: 0.690432\n",
      "Epoch 28_64.878%:  Training average Loss: 0.690523\n",
      "Epoch 28_65.679%:  Training average Loss: 0.690987\n",
      "Epoch 28_66.480%:  Training average Loss: 0.690589\n",
      "Epoch 28_67.281%:  Training average Loss: 0.690602\n",
      "Epoch 28_68.082%:  Training average Loss: 0.690742\n",
      "Epoch 28_68.883%:  Training average Loss: 0.690639\n",
      "Epoch 28_69.684%:  Training average Loss: 0.690176\n",
      "Epoch 28_70.485%:  Training average Loss: 0.690397\n",
      "Epoch 28_71.286%:  Training average Loss: 0.690259\n",
      "Epoch 28_72.087%:  Training average Loss: 0.690212\n",
      "Epoch 28_72.888%:  Training average Loss: 0.690204\n",
      "Epoch 28_73.689%:  Training average Loss: 0.690259\n",
      "Epoch 28_74.490%:  Training average Loss: 0.690212\n",
      "Epoch 28_75.291%:  Training average Loss: 0.690114\n",
      "Epoch 28_76.092%:  Training average Loss: 0.690128\n",
      "Epoch 28_76.893%:  Training average Loss: 0.690222\n",
      "Epoch 28_77.694%:  Training average Loss: 0.689863\n",
      "Epoch 28_78.495%:  Training average Loss: 0.690143\n",
      "Epoch 28_79.296%:  Training average Loss: 0.689970\n",
      "Epoch 28_80.097%:  Training average Loss: 0.690330\n",
      "Epoch 28_80.898%:  Training average Loss: 0.690512\n",
      "Epoch 28_81.699%:  Training average Loss: 0.690871\n",
      "Epoch 28_82.500%:  Training average Loss: 0.691200\n",
      "Epoch 28_83.301%:  Training average Loss: 0.691246\n",
      "Epoch 28_84.102%:  Training average Loss: 0.691370\n",
      "Epoch 28_84.903%:  Training average Loss: 0.691598\n",
      "Epoch 28_85.704%:  Training average Loss: 0.691710\n",
      "Epoch 28_86.504%:  Training average Loss: 0.691686\n",
      "Epoch 28_87.305%:  Training average Loss: 0.691564\n",
      "Epoch 28_88.106%:  Training average Loss: 0.692004\n",
      "Epoch 28_88.907%:  Training average Loss: 0.692171\n",
      "Epoch 28_89.708%:  Training average Loss: 0.692277\n",
      "Epoch 28_90.509%:  Training average Loss: 0.692280\n",
      "Epoch 28_91.310%:  Training average Loss: 0.692560\n",
      "Epoch 28_92.111%:  Training average Loss: 0.692565\n",
      "Epoch 28_92.912%:  Training average Loss: 0.692324\n",
      "Epoch 28_93.713%:  Training average Loss: 0.692607\n",
      "Epoch 28_94.514%:  Training average Loss: 0.692853\n",
      "Epoch 28_95.315%:  Training average Loss: 0.692980\n",
      "Epoch 28_96.116%:  Training average Loss: 0.693146\n",
      "Epoch 28_96.917%:  Training average Loss: 0.693295\n",
      "Epoch 28_97.718%:  Training average Loss: 0.693427\n",
      "Epoch 28_98.519%:  Training average Loss: 0.693625\n",
      "Epoch 28_99.320%:  Training average Loss: 0.693609\n",
      "Epoch 28 :  Verification average Loss: 0.866313, Verification accuracy: 65.556018%,Total Time:4669.330567\n",
      "Model is saved in model_dict/model_lstm/epoch_28_accuracy_0.655560\n",
      "Epoch 29_0.801%:  Training average Loss: 0.681235\n",
      "Epoch 29_1.602%:  Training average Loss: 0.669678\n",
      "Epoch 29_2.403%:  Training average Loss: 0.675839\n",
      "Epoch 29_3.204%:  Training average Loss: 0.675018\n",
      "Epoch 29_4.005%:  Training average Loss: 0.675463\n",
      "Epoch 29_4.806%:  Training average Loss: 0.667313\n",
      "Epoch 29_5.607%:  Training average Loss: 0.669833\n",
      "Epoch 29_6.408%:  Training average Loss: 0.670784\n",
      "Epoch 29_7.209%:  Training average Loss: 0.669487\n",
      "Epoch 29_8.010%:  Training average Loss: 0.676633\n",
      "Epoch 29_8.811%:  Training average Loss: 0.677403\n",
      "Epoch 29_9.612%:  Training average Loss: 0.679049\n",
      "Epoch 29_10.413%:  Training average Loss: 0.681123\n",
      "Epoch 29_11.214%:  Training average Loss: 0.677075\n",
      "Epoch 29_12.015%:  Training average Loss: 0.673625\n",
      "Epoch 29_12.815%:  Training average Loss: 0.671219\n",
      "Epoch 29_13.616%:  Training average Loss: 0.671300\n",
      "Epoch 29_14.417%:  Training average Loss: 0.672076\n",
      "Epoch 29_15.218%:  Training average Loss: 0.676036\n",
      "Epoch 29_16.019%:  Training average Loss: 0.679097\n",
      "Epoch 29_16.820%:  Training average Loss: 0.678421\n",
      "Epoch 29_17.621%:  Training average Loss: 0.677071\n",
      "Epoch 29_18.422%:  Training average Loss: 0.677302\n",
      "Epoch 29_19.223%:  Training average Loss: 0.677968\n",
      "Epoch 29_20.024%:  Training average Loss: 0.677463\n",
      "Epoch 29_20.825%:  Training average Loss: 0.678547\n",
      "Epoch 29_21.626%:  Training average Loss: 0.679423\n",
      "Epoch 29_22.427%:  Training average Loss: 0.679197\n",
      "Epoch 29_23.228%:  Training average Loss: 0.680223\n",
      "Epoch 29_24.029%:  Training average Loss: 0.679221\n",
      "Epoch 29_24.830%:  Training average Loss: 0.680035\n",
      "Epoch 29_25.631%:  Training average Loss: 0.679634\n",
      "Epoch 29_26.432%:  Training average Loss: 0.678564\n",
      "Epoch 29_27.233%:  Training average Loss: 0.679727\n",
      "Epoch 29_28.034%:  Training average Loss: 0.678501\n",
      "Epoch 29_28.835%:  Training average Loss: 0.679943\n",
      "Epoch 29_29.636%:  Training average Loss: 0.679981\n",
      "Epoch 29_30.437%:  Training average Loss: 0.680440\n",
      "Epoch 29_31.238%:  Training average Loss: 0.679181\n",
      "Epoch 29_32.039%:  Training average Loss: 0.679775\n",
      "Epoch 29_32.840%:  Training average Loss: 0.680535\n",
      "Epoch 29_33.641%:  Training average Loss: 0.680249\n",
      "Epoch 29_34.442%:  Training average Loss: 0.680996\n",
      "Epoch 29_35.243%:  Training average Loss: 0.680355\n",
      "Epoch 29_36.044%:  Training average Loss: 0.680378\n",
      "Epoch 29_36.845%:  Training average Loss: 0.680440\n",
      "Epoch 29_37.645%:  Training average Loss: 0.681868\n",
      "Epoch 29_38.446%:  Training average Loss: 0.682050\n",
      "Epoch 29_39.247%:  Training average Loss: 0.681650\n",
      "Epoch 29_40.048%:  Training average Loss: 0.683303\n",
      "Epoch 29_40.849%:  Training average Loss: 0.683861\n",
      "Epoch 29_41.650%:  Training average Loss: 0.683708\n",
      "Epoch 29_42.451%:  Training average Loss: 0.683404\n",
      "Epoch 29_43.252%:  Training average Loss: 0.683321\n",
      "Epoch 29_44.053%:  Training average Loss: 0.682840\n",
      "Epoch 29_44.854%:  Training average Loss: 0.683260\n",
      "Epoch 29_45.655%:  Training average Loss: 0.682859\n",
      "Epoch 29_46.456%:  Training average Loss: 0.683318\n",
      "Epoch 29_47.257%:  Training average Loss: 0.682881\n",
      "Epoch 29_48.058%:  Training average Loss: 0.682901\n",
      "Epoch 29_48.859%:  Training average Loss: 0.683008\n",
      "Epoch 29_49.660%:  Training average Loss: 0.682814\n",
      "Epoch 29_50.461%:  Training average Loss: 0.683721\n",
      "Epoch 29_51.262%:  Training average Loss: 0.684424\n",
      "Epoch 29_52.063%:  Training average Loss: 0.684763\n",
      "Epoch 29_52.864%:  Training average Loss: 0.684337\n",
      "Epoch 29_53.665%:  Training average Loss: 0.684408\n",
      "Epoch 29_54.466%:  Training average Loss: 0.684241\n",
      "Epoch 29_55.267%:  Training average Loss: 0.683947\n",
      "Epoch 29_56.068%:  Training average Loss: 0.684304\n",
      "Epoch 29_56.869%:  Training average Loss: 0.684674\n",
      "Epoch 29_57.670%:  Training average Loss: 0.684363\n",
      "Epoch 29_58.471%:  Training average Loss: 0.684535\n",
      "Epoch 29_59.272%:  Training average Loss: 0.684956\n",
      "Epoch 29_60.073%:  Training average Loss: 0.684973\n",
      "Epoch 29_60.874%:  Training average Loss: 0.685359\n",
      "Epoch 29_61.675%:  Training average Loss: 0.685785\n",
      "Epoch 29_62.475%:  Training average Loss: 0.685329\n",
      "Epoch 29_63.276%:  Training average Loss: 0.684965\n",
      "Epoch 29_64.077%:  Training average Loss: 0.685444\n",
      "Epoch 29_64.878%:  Training average Loss: 0.685608\n",
      "Epoch 29_65.679%:  Training average Loss: 0.685988\n",
      "Epoch 29_66.480%:  Training average Loss: 0.686339\n",
      "Epoch 29_67.281%:  Training average Loss: 0.686293\n",
      "Epoch 29_68.082%:  Training average Loss: 0.686349\n",
      "Epoch 29_68.883%:  Training average Loss: 0.686580\n",
      "Epoch 29_69.684%:  Training average Loss: 0.686937\n",
      "Epoch 29_70.485%:  Training average Loss: 0.687254\n",
      "Epoch 29_71.286%:  Training average Loss: 0.686982\n",
      "Epoch 29_72.087%:  Training average Loss: 0.687357\n",
      "Epoch 29_72.888%:  Training average Loss: 0.687178\n",
      "Epoch 29_73.689%:  Training average Loss: 0.687148\n",
      "Epoch 29_74.490%:  Training average Loss: 0.687456\n",
      "Epoch 29_75.291%:  Training average Loss: 0.687670\n",
      "Epoch 29_76.092%:  Training average Loss: 0.687669\n",
      "Epoch 29_76.893%:  Training average Loss: 0.687434\n",
      "Epoch 29_77.694%:  Training average Loss: 0.687629\n",
      "Epoch 29_78.495%:  Training average Loss: 0.687736\n",
      "Epoch 29_79.296%:  Training average Loss: 0.687550\n",
      "Epoch 29_80.097%:  Training average Loss: 0.687660\n",
      "Epoch 29_80.898%:  Training average Loss: 0.687875\n",
      "Epoch 29_81.699%:  Training average Loss: 0.688224\n",
      "Epoch 29_82.500%:  Training average Loss: 0.688131\n",
      "Epoch 29_83.301%:  Training average Loss: 0.688083\n",
      "Epoch 29_84.102%:  Training average Loss: 0.688474\n",
      "Epoch 29_84.903%:  Training average Loss: 0.688477\n",
      "Epoch 29_85.704%:  Training average Loss: 0.688771\n",
      "Epoch 29_86.504%:  Training average Loss: 0.688118\n",
      "Epoch 29_87.305%:  Training average Loss: 0.688054\n",
      "Epoch 29_88.106%:  Training average Loss: 0.688189\n",
      "Epoch 29_88.907%:  Training average Loss: 0.688281\n",
      "Epoch 29_89.708%:  Training average Loss: 0.688459\n",
      "Epoch 29_90.509%:  Training average Loss: 0.688775\n",
      "Epoch 29_91.310%:  Training average Loss: 0.688983\n",
      "Epoch 29_92.111%:  Training average Loss: 0.689507\n",
      "Epoch 29_92.912%:  Training average Loss: 0.689696\n",
      "Epoch 29_93.713%:  Training average Loss: 0.689946\n",
      "Epoch 29_94.514%:  Training average Loss: 0.689986\n",
      "Epoch 29_95.315%:  Training average Loss: 0.690307\n",
      "Epoch 29_96.116%:  Training average Loss: 0.690123\n",
      "Epoch 29_96.917%:  Training average Loss: 0.690295\n",
      "Epoch 29_97.718%:  Training average Loss: 0.690704\n",
      "Epoch 29_98.519%:  Training average Loss: 0.691019\n",
      "Epoch 29_99.320%:  Training average Loss: 0.690852\n",
      "Epoch 29 :  Verification average Loss: 0.860204, Verification accuracy: 65.315734%,Total Time:4825.446780\n",
      "Epoch 30_0.801%:  Training average Loss: 0.692859\n",
      "Epoch 30_1.602%:  Training average Loss: 0.679507\n",
      "Epoch 30_2.403%:  Training average Loss: 0.673406\n",
      "Epoch 30_3.204%:  Training average Loss: 0.679357\n",
      "Epoch 30_4.005%:  Training average Loss: 0.677468\n",
      "Epoch 30_4.806%:  Training average Loss: 0.672518\n",
      "Epoch 30_5.607%:  Training average Loss: 0.669818\n",
      "Epoch 30_6.408%:  Training average Loss: 0.669860\n",
      "Epoch 30_7.209%:  Training average Loss: 0.674301\n",
      "Epoch 30_8.010%:  Training average Loss: 0.676852\n",
      "Epoch 30_8.811%:  Training average Loss: 0.674134\n",
      "Epoch 30_9.612%:  Training average Loss: 0.677221\n",
      "Epoch 30_10.413%:  Training average Loss: 0.679134\n",
      "Epoch 30_11.214%:  Training average Loss: 0.681980\n",
      "Epoch 30_12.015%:  Training average Loss: 0.684329\n",
      "Epoch 30_12.815%:  Training average Loss: 0.685482\n",
      "Epoch 30_13.616%:  Training average Loss: 0.684388\n",
      "Epoch 30_14.417%:  Training average Loss: 0.684120\n",
      "Epoch 30_15.218%:  Training average Loss: 0.682537\n",
      "Epoch 30_16.019%:  Training average Loss: 0.683481\n",
      "Epoch 30_16.820%:  Training average Loss: 0.683346\n",
      "Epoch 30_17.621%:  Training average Loss: 0.681952\n",
      "Epoch 30_18.422%:  Training average Loss: 0.680492\n",
      "Epoch 30_19.223%:  Training average Loss: 0.680473\n",
      "Epoch 30_20.024%:  Training average Loss: 0.681945\n",
      "Epoch 30_20.825%:  Training average Loss: 0.681440\n",
      "Epoch 30_21.626%:  Training average Loss: 0.681450\n",
      "Epoch 30_22.427%:  Training average Loss: 0.681816\n",
      "Epoch 30_23.228%:  Training average Loss: 0.680413\n",
      "Epoch 30_24.029%:  Training average Loss: 0.681946\n",
      "Epoch 30_24.830%:  Training average Loss: 0.681749\n",
      "Epoch 30_25.631%:  Training average Loss: 0.682818\n",
      "Epoch 30_26.432%:  Training average Loss: 0.682423\n",
      "Epoch 30_27.233%:  Training average Loss: 0.682813\n",
      "Epoch 30_28.034%:  Training average Loss: 0.682894\n",
      "Epoch 30_28.835%:  Training average Loss: 0.682650\n",
      "Epoch 30_29.636%:  Training average Loss: 0.681763\n",
      "Epoch 30_30.437%:  Training average Loss: 0.681833\n",
      "Epoch 30_31.238%:  Training average Loss: 0.681762\n",
      "Epoch 30_32.039%:  Training average Loss: 0.681028\n",
      "Epoch 30_32.840%:  Training average Loss: 0.680377\n",
      "Epoch 30_33.641%:  Training average Loss: 0.680238\n",
      "Epoch 30_34.442%:  Training average Loss: 0.679906\n",
      "Epoch 30_35.243%:  Training average Loss: 0.679813\n",
      "Epoch 30_36.044%:  Training average Loss: 0.679874\n",
      "Epoch 30_36.845%:  Training average Loss: 0.679782\n",
      "Epoch 30_37.645%:  Training average Loss: 0.680413\n",
      "Epoch 30_38.446%:  Training average Loss: 0.680265\n",
      "Epoch 30_39.247%:  Training average Loss: 0.680118\n",
      "Epoch 30_40.048%:  Training average Loss: 0.680720\n",
      "Epoch 30_40.849%:  Training average Loss: 0.680453\n",
      "Epoch 30_41.650%:  Training average Loss: 0.681061\n",
      "Epoch 30_42.451%:  Training average Loss: 0.681745\n",
      "Epoch 30_43.252%:  Training average Loss: 0.681687\n",
      "Epoch 30_44.053%:  Training average Loss: 0.682227\n",
      "Epoch 30_44.854%:  Training average Loss: 0.682037\n",
      "Epoch 30_45.655%:  Training average Loss: 0.681567\n",
      "Epoch 30_46.456%:  Training average Loss: 0.682232\n",
      "Epoch 30_47.257%:  Training average Loss: 0.682166\n",
      "Epoch 30_48.058%:  Training average Loss: 0.682165\n",
      "Epoch 30_48.859%:  Training average Loss: 0.682070\n",
      "Epoch 30_49.660%:  Training average Loss: 0.681568\n",
      "Epoch 30_50.461%:  Training average Loss: 0.681183\n",
      "Epoch 30_51.262%:  Training average Loss: 0.682377\n",
      "Epoch 30_52.063%:  Training average Loss: 0.683101\n",
      "Epoch 30_52.864%:  Training average Loss: 0.683098\n",
      "Epoch 30_53.665%:  Training average Loss: 0.683122\n",
      "Epoch 30_54.466%:  Training average Loss: 0.683020\n",
      "Epoch 30_55.267%:  Training average Loss: 0.682406\n",
      "Epoch 30_56.068%:  Training average Loss: 0.682702\n",
      "Epoch 30_56.869%:  Training average Loss: 0.683108\n",
      "Epoch 30_57.670%:  Training average Loss: 0.683619\n",
      "Epoch 30_58.471%:  Training average Loss: 0.683744\n",
      "Epoch 30_59.272%:  Training average Loss: 0.683608\n",
      "Epoch 30_60.073%:  Training average Loss: 0.683876\n",
      "Epoch 30_60.874%:  Training average Loss: 0.684075\n",
      "Epoch 30_61.675%:  Training average Loss: 0.683573\n",
      "Epoch 30_62.475%:  Training average Loss: 0.683553\n",
      "Epoch 30_63.276%:  Training average Loss: 0.683554\n",
      "Epoch 30_64.077%:  Training average Loss: 0.684085\n",
      "Epoch 30_64.878%:  Training average Loss: 0.684973\n",
      "Epoch 30_65.679%:  Training average Loss: 0.685356\n",
      "Epoch 30_66.480%:  Training average Loss: 0.685674\n",
      "Epoch 30_67.281%:  Training average Loss: 0.685857\n",
      "Epoch 30_68.082%:  Training average Loss: 0.685783\n",
      "Epoch 30_68.883%:  Training average Loss: 0.685607\n",
      "Epoch 30_69.684%:  Training average Loss: 0.685843\n",
      "Epoch 30_70.485%:  Training average Loss: 0.685654\n",
      "Epoch 30_71.286%:  Training average Loss: 0.685203\n",
      "Epoch 30_72.087%:  Training average Loss: 0.685135\n",
      "Epoch 30_72.888%:  Training average Loss: 0.685748\n",
      "Epoch 30_73.689%:  Training average Loss: 0.685815\n",
      "Epoch 30_74.490%:  Training average Loss: 0.685940\n",
      "Epoch 30_75.291%:  Training average Loss: 0.685982\n",
      "Epoch 30_76.092%:  Training average Loss: 0.685901\n",
      "Epoch 30_76.893%:  Training average Loss: 0.686317\n",
      "Epoch 30_77.694%:  Training average Loss: 0.686607\n",
      "Epoch 30_78.495%:  Training average Loss: 0.686958\n",
      "Epoch 30_79.296%:  Training average Loss: 0.686777\n",
      "Epoch 30_80.097%:  Training average Loss: 0.686584\n",
      "Epoch 30_80.898%:  Training average Loss: 0.686523\n",
      "Epoch 30_81.699%:  Training average Loss: 0.686888\n",
      "Epoch 30_82.500%:  Training average Loss: 0.686644\n",
      "Epoch 30_83.301%:  Training average Loss: 0.686600\n",
      "Epoch 30_84.102%:  Training average Loss: 0.686665\n",
      "Epoch 30_84.903%:  Training average Loss: 0.686786\n",
      "Epoch 30_85.704%:  Training average Loss: 0.686790\n",
      "Epoch 30_86.504%:  Training average Loss: 0.687169\n",
      "Epoch 30_87.305%:  Training average Loss: 0.686929\n",
      "Epoch 30_88.106%:  Training average Loss: 0.686974\n",
      "Epoch 30_88.907%:  Training average Loss: 0.687358\n",
      "Epoch 30_89.708%:  Training average Loss: 0.687353\n",
      "Epoch 30_90.509%:  Training average Loss: 0.687573\n",
      "Epoch 30_91.310%:  Training average Loss: 0.687421\n",
      "Epoch 30_92.111%:  Training average Loss: 0.687918\n",
      "Epoch 30_92.912%:  Training average Loss: 0.687819\n",
      "Epoch 30_93.713%:  Training average Loss: 0.687915\n",
      "Epoch 30_94.514%:  Training average Loss: 0.688341\n",
      "Epoch 30_95.315%:  Training average Loss: 0.688564\n",
      "Epoch 30_96.116%:  Training average Loss: 0.688914\n",
      "Epoch 30_96.917%:  Training average Loss: 0.688713\n",
      "Epoch 30_97.718%:  Training average Loss: 0.688894\n",
      "Epoch 30_98.519%:  Training average Loss: 0.688553\n",
      "Epoch 30_99.320%:  Training average Loss: 0.688765\n",
      "Epoch 30 :  Verification average Loss: 0.865149, Verification accuracy: 65.415051%,Total Time:4984.554226\n",
      "Epoch 31_0.801%:  Training average Loss: 0.638280\n",
      "Epoch 31_1.602%:  Training average Loss: 0.676548\n",
      "Epoch 31_2.403%:  Training average Loss: 0.686115\n",
      "Epoch 31_3.204%:  Training average Loss: 0.671169\n",
      "Epoch 31_4.005%:  Training average Loss: 0.662978\n",
      "Epoch 31_4.806%:  Training average Loss: 0.654522\n",
      "Epoch 31_5.607%:  Training average Loss: 0.663068\n",
      "Epoch 31_6.408%:  Training average Loss: 0.660737\n",
      "Epoch 31_7.209%:  Training average Loss: 0.667273\n",
      "Epoch 31_8.010%:  Training average Loss: 0.668342\n",
      "Epoch 31_8.811%:  Training average Loss: 0.666858\n",
      "Epoch 31_9.612%:  Training average Loss: 0.669146\n",
      "Epoch 31_10.413%:  Training average Loss: 0.666634\n",
      "Epoch 31_11.214%:  Training average Loss: 0.669380\n",
      "Epoch 31_12.015%:  Training average Loss: 0.669218\n",
      "Epoch 31_12.815%:  Training average Loss: 0.668236\n",
      "Epoch 31_13.616%:  Training average Loss: 0.671978\n",
      "Epoch 31_14.417%:  Training average Loss: 0.672444\n",
      "Epoch 31_15.218%:  Training average Loss: 0.670808\n",
      "Epoch 31_16.019%:  Training average Loss: 0.669349\n",
      "Epoch 31_16.820%:  Training average Loss: 0.670295\n",
      "Epoch 31_17.621%:  Training average Loss: 0.668665\n",
      "Epoch 31_18.422%:  Training average Loss: 0.668500\n",
      "Epoch 31_19.223%:  Training average Loss: 0.668612\n",
      "Epoch 31_20.024%:  Training average Loss: 0.667134\n",
      "Epoch 31_20.825%:  Training average Loss: 0.667524\n",
      "Epoch 31_21.626%:  Training average Loss: 0.669397\n",
      "Epoch 31_22.427%:  Training average Loss: 0.669872\n",
      "Epoch 31_23.228%:  Training average Loss: 0.670666\n",
      "Epoch 31_24.029%:  Training average Loss: 0.671424\n",
      "Epoch 31_24.830%:  Training average Loss: 0.672523\n",
      "Epoch 31_25.631%:  Training average Loss: 0.673716\n",
      "Epoch 31_26.432%:  Training average Loss: 0.674264\n",
      "Epoch 31_27.233%:  Training average Loss: 0.673893\n",
      "Epoch 31_28.034%:  Training average Loss: 0.673811\n",
      "Epoch 31_28.835%:  Training average Loss: 0.673120\n",
      "Epoch 31_29.636%:  Training average Loss: 0.673041\n",
      "Epoch 31_30.437%:  Training average Loss: 0.673941\n",
      "Epoch 31_31.238%:  Training average Loss: 0.673781\n",
      "Epoch 31_32.039%:  Training average Loss: 0.673719\n",
      "Epoch 31_32.840%:  Training average Loss: 0.673453\n",
      "Epoch 31_33.641%:  Training average Loss: 0.673559\n",
      "Epoch 31_34.442%:  Training average Loss: 0.673878\n",
      "Epoch 31_35.243%:  Training average Loss: 0.673907\n",
      "Epoch 31_36.044%:  Training average Loss: 0.673975\n",
      "Epoch 31_36.845%:  Training average Loss: 0.674125\n",
      "Epoch 31_37.645%:  Training average Loss: 0.674421\n",
      "Epoch 31_38.446%:  Training average Loss: 0.674389\n",
      "Epoch 31_39.247%:  Training average Loss: 0.674258\n",
      "Epoch 31_40.048%:  Training average Loss: 0.673243\n",
      "Epoch 31_40.849%:  Training average Loss: 0.672885\n",
      "Epoch 31_41.650%:  Training average Loss: 0.673066\n",
      "Epoch 31_42.451%:  Training average Loss: 0.673698\n",
      "Epoch 31_43.252%:  Training average Loss: 0.675099\n",
      "Epoch 31_44.053%:  Training average Loss: 0.675860\n",
      "Epoch 31_44.854%:  Training average Loss: 0.676342\n",
      "Epoch 31_45.655%:  Training average Loss: 0.676233\n",
      "Epoch 31_46.456%:  Training average Loss: 0.675773\n",
      "Epoch 31_47.257%:  Training average Loss: 0.675954\n",
      "Epoch 31_48.058%:  Training average Loss: 0.675962\n",
      "Epoch 31_48.859%:  Training average Loss: 0.676673\n",
      "Epoch 31_49.660%:  Training average Loss: 0.676358\n",
      "Epoch 31_50.461%:  Training average Loss: 0.676736\n",
      "Epoch 31_51.262%:  Training average Loss: 0.676159\n",
      "Epoch 31_52.063%:  Training average Loss: 0.676994\n",
      "Epoch 31_52.864%:  Training average Loss: 0.677005\n",
      "Epoch 31_53.665%:  Training average Loss: 0.676329\n",
      "Epoch 31_54.466%:  Training average Loss: 0.676226\n",
      "Epoch 31_55.267%:  Training average Loss: 0.676457\n",
      "Epoch 31_56.068%:  Training average Loss: 0.676598\n",
      "Epoch 31_56.869%:  Training average Loss: 0.677170\n",
      "Epoch 31_57.670%:  Training average Loss: 0.677269\n",
      "Epoch 31_58.471%:  Training average Loss: 0.677793\n",
      "Epoch 31_59.272%:  Training average Loss: 0.678418\n",
      "Epoch 31_60.073%:  Training average Loss: 0.678786\n",
      "Epoch 31_60.874%:  Training average Loss: 0.679519\n",
      "Epoch 31_61.675%:  Training average Loss: 0.679173\n",
      "Epoch 31_62.475%:  Training average Loss: 0.679291\n",
      "Epoch 31_63.276%:  Training average Loss: 0.678865\n",
      "Epoch 31_64.077%:  Training average Loss: 0.679127\n",
      "Epoch 31_64.878%:  Training average Loss: 0.679167\n",
      "Epoch 31_65.679%:  Training average Loss: 0.679742\n",
      "Epoch 31_66.480%:  Training average Loss: 0.679443\n",
      "Epoch 31_67.281%:  Training average Loss: 0.679517\n",
      "Epoch 31_68.082%:  Training average Loss: 0.679613\n",
      "Epoch 31_68.883%:  Training average Loss: 0.679657\n",
      "Epoch 31_69.684%:  Training average Loss: 0.680066\n",
      "Epoch 31_70.485%:  Training average Loss: 0.679977\n",
      "Epoch 31_71.286%:  Training average Loss: 0.680394\n",
      "Epoch 31_72.087%:  Training average Loss: 0.681155\n",
      "Epoch 31_72.888%:  Training average Loss: 0.681465\n",
      "Epoch 31_73.689%:  Training average Loss: 0.681665\n",
      "Epoch 31_74.490%:  Training average Loss: 0.681714\n",
      "Epoch 31_75.291%:  Training average Loss: 0.681585\n",
      "Epoch 31_76.092%:  Training average Loss: 0.681140\n",
      "Epoch 31_76.893%:  Training average Loss: 0.681322\n",
      "Epoch 31_77.694%:  Training average Loss: 0.681412\n",
      "Epoch 31_78.495%:  Training average Loss: 0.681537\n",
      "Epoch 31_79.296%:  Training average Loss: 0.681337\n",
      "Epoch 31_80.097%:  Training average Loss: 0.681596\n",
      "Epoch 31_80.898%:  Training average Loss: 0.681768\n",
      "Epoch 31_81.699%:  Training average Loss: 0.681799\n",
      "Epoch 31_82.500%:  Training average Loss: 0.682031\n",
      "Epoch 31_83.301%:  Training average Loss: 0.681941\n",
      "Epoch 31_84.102%:  Training average Loss: 0.682533\n",
      "Epoch 31_84.903%:  Training average Loss: 0.682925\n",
      "Epoch 31_85.704%:  Training average Loss: 0.682931\n",
      "Epoch 31_86.504%:  Training average Loss: 0.683211\n",
      "Epoch 31_87.305%:  Training average Loss: 0.683442\n",
      "Epoch 31_88.106%:  Training average Loss: 0.683626\n",
      "Epoch 31_88.907%:  Training average Loss: 0.683655\n",
      "Epoch 31_89.708%:  Training average Loss: 0.683790\n",
      "Epoch 31_90.509%:  Training average Loss: 0.684086\n",
      "Epoch 31_91.310%:  Training average Loss: 0.684200\n",
      "Epoch 31_92.111%:  Training average Loss: 0.684197\n",
      "Epoch 31_92.912%:  Training average Loss: 0.684297\n",
      "Epoch 31_93.713%:  Training average Loss: 0.684644\n",
      "Epoch 31_94.514%:  Training average Loss: 0.684988\n",
      "Epoch 31_95.315%:  Training average Loss: 0.685381\n",
      "Epoch 31_96.116%:  Training average Loss: 0.685444\n",
      "Epoch 31_96.917%:  Training average Loss: 0.685512\n",
      "Epoch 31_97.718%:  Training average Loss: 0.686167\n",
      "Epoch 31_98.519%:  Training average Loss: 0.686244\n",
      "Epoch 31_99.320%:  Training average Loss: 0.686611\n",
      "Epoch 31 :  Verification average Loss: 0.870040, Verification accuracy: 65.226028%,Total Time:5142.511173\n",
      "Epoch 32_0.801%:  Training average Loss: 0.661718\n",
      "Epoch 32_1.602%:  Training average Loss: 0.664993\n",
      "Epoch 32_2.403%:  Training average Loss: 0.672506\n",
      "Epoch 32_3.204%:  Training average Loss: 0.682512\n",
      "Epoch 32_4.005%:  Training average Loss: 0.672825\n",
      "Epoch 32_4.806%:  Training average Loss: 0.674273\n",
      "Epoch 32_5.607%:  Training average Loss: 0.672264\n",
      "Epoch 32_6.408%:  Training average Loss: 0.672334\n",
      "Epoch 32_7.209%:  Training average Loss: 0.669086\n",
      "Epoch 32_8.010%:  Training average Loss: 0.668716\n",
      "Epoch 32_8.811%:  Training average Loss: 0.666971\n",
      "Epoch 32_9.612%:  Training average Loss: 0.671125\n",
      "Epoch 32_10.413%:  Training average Loss: 0.669999\n",
      "Epoch 32_11.214%:  Training average Loss: 0.670174\n",
      "Epoch 32_12.015%:  Training average Loss: 0.669370\n",
      "Epoch 32_12.815%:  Training average Loss: 0.669489\n",
      "Epoch 32_13.616%:  Training average Loss: 0.670738\n",
      "Epoch 32_14.417%:  Training average Loss: 0.670567\n",
      "Epoch 32_15.218%:  Training average Loss: 0.669425\n",
      "Epoch 32_16.019%:  Training average Loss: 0.670746\n",
      "Epoch 32_16.820%:  Training average Loss: 0.671974\n",
      "Epoch 32_17.621%:  Training average Loss: 0.672700\n",
      "Epoch 32_18.422%:  Training average Loss: 0.671049\n",
      "Epoch 32_19.223%:  Training average Loss: 0.672964\n",
      "Epoch 32_20.024%:  Training average Loss: 0.673878\n",
      "Epoch 32_20.825%:  Training average Loss: 0.671975\n",
      "Epoch 32_21.626%:  Training average Loss: 0.671192\n",
      "Epoch 32_22.427%:  Training average Loss: 0.672209\n",
      "Epoch 32_23.228%:  Training average Loss: 0.672041\n",
      "Epoch 32_24.029%:  Training average Loss: 0.672472\n",
      "Epoch 32_24.830%:  Training average Loss: 0.672852\n",
      "Epoch 32_25.631%:  Training average Loss: 0.674199\n",
      "Epoch 32_26.432%:  Training average Loss: 0.673925\n",
      "Epoch 32_27.233%:  Training average Loss: 0.673705\n",
      "Epoch 32_28.034%:  Training average Loss: 0.674722\n",
      "Epoch 32_28.835%:  Training average Loss: 0.674356\n",
      "Epoch 32_29.636%:  Training average Loss: 0.674516\n",
      "Epoch 32_30.437%:  Training average Loss: 0.674092\n",
      "Epoch 32_31.238%:  Training average Loss: 0.673410\n",
      "Epoch 32_32.039%:  Training average Loss: 0.672681\n",
      "Epoch 32_32.840%:  Training average Loss: 0.672260\n",
      "Epoch 32_33.641%:  Training average Loss: 0.672529\n",
      "Epoch 32_34.442%:  Training average Loss: 0.672593\n",
      "Epoch 32_35.243%:  Training average Loss: 0.671859\n",
      "Epoch 32_36.044%:  Training average Loss: 0.672041\n",
      "Epoch 32_36.845%:  Training average Loss: 0.671967\n",
      "Epoch 32_37.645%:  Training average Loss: 0.672457\n",
      "Epoch 32_38.446%:  Training average Loss: 0.672987\n",
      "Epoch 32_39.247%:  Training average Loss: 0.673750\n",
      "Epoch 32_40.048%:  Training average Loss: 0.674131\n",
      "Epoch 32_40.849%:  Training average Loss: 0.674599\n",
      "Epoch 32_41.650%:  Training average Loss: 0.673923\n",
      "Epoch 32_42.451%:  Training average Loss: 0.674204\n",
      "Epoch 32_43.252%:  Training average Loss: 0.675289\n",
      "Epoch 32_44.053%:  Training average Loss: 0.675531\n",
      "Epoch 32_44.854%:  Training average Loss: 0.675807\n",
      "Epoch 32_45.655%:  Training average Loss: 0.675390\n",
      "Epoch 32_46.456%:  Training average Loss: 0.675012\n",
      "Epoch 32_47.257%:  Training average Loss: 0.675323\n",
      "Epoch 32_48.058%:  Training average Loss: 0.675326\n",
      "Epoch 32_48.859%:  Training average Loss: 0.675166\n",
      "Epoch 32_49.660%:  Training average Loss: 0.675470\n",
      "Epoch 32_50.461%:  Training average Loss: 0.675524\n",
      "Epoch 32_51.262%:  Training average Loss: 0.675489\n",
      "Epoch 32_52.063%:  Training average Loss: 0.675622\n",
      "Epoch 32_52.864%:  Training average Loss: 0.674973\n",
      "Epoch 32_53.665%:  Training average Loss: 0.675067\n",
      "Epoch 32_54.466%:  Training average Loss: 0.676105\n",
      "Epoch 32_55.267%:  Training average Loss: 0.676559\n",
      "Epoch 32_56.068%:  Training average Loss: 0.676308\n",
      "Epoch 32_56.869%:  Training average Loss: 0.676746\n",
      "Epoch 32_57.670%:  Training average Loss: 0.677235\n",
      "Epoch 32_58.471%:  Training average Loss: 0.677305\n",
      "Epoch 32_59.272%:  Training average Loss: 0.677361\n",
      "Epoch 32_60.073%:  Training average Loss: 0.677948\n",
      "Epoch 32_60.874%:  Training average Loss: 0.678123\n",
      "Epoch 32_61.675%:  Training average Loss: 0.678214\n",
      "Epoch 32_62.475%:  Training average Loss: 0.678503\n",
      "Epoch 32_63.276%:  Training average Loss: 0.678944\n",
      "Epoch 32_64.077%:  Training average Loss: 0.678847\n",
      "Epoch 32_64.878%:  Training average Loss: 0.678837\n",
      "Epoch 32_65.679%:  Training average Loss: 0.678560\n",
      "Epoch 32_66.480%:  Training average Loss: 0.678824\n",
      "Epoch 32_67.281%:  Training average Loss: 0.678699\n",
      "Epoch 32_68.082%:  Training average Loss: 0.679251\n",
      "Epoch 32_68.883%:  Training average Loss: 0.680203\n",
      "Epoch 32_69.684%:  Training average Loss: 0.679162\n",
      "Epoch 32_70.485%:  Training average Loss: 0.679165\n",
      "Epoch 32_71.286%:  Training average Loss: 0.679001\n",
      "Epoch 32_72.087%:  Training average Loss: 0.679194\n",
      "Epoch 32_72.888%:  Training average Loss: 0.679519\n",
      "Epoch 32_73.689%:  Training average Loss: 0.679375\n",
      "Epoch 32_74.490%:  Training average Loss: 0.679391\n",
      "Epoch 32_75.291%:  Training average Loss: 0.680056\n",
      "Epoch 32_76.092%:  Training average Loss: 0.680330\n",
      "Epoch 32_76.893%:  Training average Loss: 0.680439\n",
      "Epoch 32_77.694%:  Training average Loss: 0.680926\n",
      "Epoch 32_78.495%:  Training average Loss: 0.681014\n",
      "Epoch 32_79.296%:  Training average Loss: 0.680994\n",
      "Epoch 32_80.097%:  Training average Loss: 0.681189\n",
      "Epoch 32_80.898%:  Training average Loss: 0.681154\n",
      "Epoch 32_81.699%:  Training average Loss: 0.681143\n",
      "Epoch 32_82.500%:  Training average Loss: 0.681238\n",
      "Epoch 32_83.301%:  Training average Loss: 0.681274\n",
      "Epoch 32_84.102%:  Training average Loss: 0.681021\n",
      "Epoch 32_84.903%:  Training average Loss: 0.681054\n",
      "Epoch 32_85.704%:  Training average Loss: 0.681044\n",
      "Epoch 32_86.504%:  Training average Loss: 0.681320\n",
      "Epoch 32_87.305%:  Training average Loss: 0.681403\n",
      "Epoch 32_88.106%:  Training average Loss: 0.681578\n",
      "Epoch 32_88.907%:  Training average Loss: 0.681831\n",
      "Epoch 32_89.708%:  Training average Loss: 0.682067\n",
      "Epoch 32_90.509%:  Training average Loss: 0.681925\n",
      "Epoch 32_91.310%:  Training average Loss: 0.682249\n",
      "Epoch 32_92.111%:  Training average Loss: 0.682043\n",
      "Epoch 32_92.912%:  Training average Loss: 0.682180\n",
      "Epoch 32_93.713%:  Training average Loss: 0.681965\n",
      "Epoch 32_94.514%:  Training average Loss: 0.682090\n",
      "Epoch 32_95.315%:  Training average Loss: 0.682371\n",
      "Epoch 32_96.116%:  Training average Loss: 0.682502\n",
      "Epoch 32_96.917%:  Training average Loss: 0.682658\n",
      "Epoch 32_97.718%:  Training average Loss: 0.682589\n",
      "Epoch 32_98.519%:  Training average Loss: 0.683029\n",
      "Epoch 32_99.320%:  Training average Loss: 0.683676\n",
      "Epoch 32 :  Verification average Loss: 0.867836, Verification accuracy: 65.453497%,Total Time:5298.278021\n",
      "Epoch 33_0.801%:  Training average Loss: 0.682288\n",
      "Epoch 33_1.602%:  Training average Loss: 0.669259\n",
      "Epoch 33_2.403%:  Training average Loss: 0.659652\n",
      "Epoch 33_3.204%:  Training average Loss: 0.663084\n",
      "Epoch 33_4.005%:  Training average Loss: 0.672489\n",
      "Epoch 33_4.806%:  Training average Loss: 0.665345\n",
      "Epoch 33_5.607%:  Training average Loss: 0.658272\n",
      "Epoch 33_6.408%:  Training average Loss: 0.659677\n",
      "Epoch 33_7.209%:  Training average Loss: 0.660101\n",
      "Epoch 33_8.010%:  Training average Loss: 0.665508\n",
      "Epoch 33_8.811%:  Training average Loss: 0.663889\n",
      "Epoch 33_9.612%:  Training average Loss: 0.662741\n",
      "Epoch 33_10.413%:  Training average Loss: 0.662493\n",
      "Epoch 33_11.214%:  Training average Loss: 0.663323\n",
      "Epoch 33_12.015%:  Training average Loss: 0.664951\n",
      "Epoch 33_12.815%:  Training average Loss: 0.663632\n",
      "Epoch 33_13.616%:  Training average Loss: 0.659127\n",
      "Epoch 33_14.417%:  Training average Loss: 0.661199\n",
      "Epoch 33_15.218%:  Training average Loss: 0.662393\n",
      "Epoch 33_16.019%:  Training average Loss: 0.663109\n",
      "Epoch 33_16.820%:  Training average Loss: 0.664260\n",
      "Epoch 33_17.621%:  Training average Loss: 0.665379\n",
      "Epoch 33_18.422%:  Training average Loss: 0.666848\n",
      "Epoch 33_19.223%:  Training average Loss: 0.669707\n",
      "Epoch 33_20.024%:  Training average Loss: 0.668917\n",
      "Epoch 33_20.825%:  Training average Loss: 0.670032\n",
      "Epoch 33_21.626%:  Training average Loss: 0.669239\n",
      "Epoch 33_22.427%:  Training average Loss: 0.668795\n",
      "Epoch 33_23.228%:  Training average Loss: 0.668853\n",
      "Epoch 33_24.029%:  Training average Loss: 0.668230\n",
      "Epoch 33_24.830%:  Training average Loss: 0.668746\n",
      "Epoch 33_25.631%:  Training average Loss: 0.669952\n",
      "Epoch 33_26.432%:  Training average Loss: 0.669663\n",
      "Epoch 33_27.233%:  Training average Loss: 0.669724\n",
      "Epoch 33_28.034%:  Training average Loss: 0.668409\n",
      "Epoch 33_28.835%:  Training average Loss: 0.668172\n",
      "Epoch 33_29.636%:  Training average Loss: 0.667667\n",
      "Epoch 33_30.437%:  Training average Loss: 0.666953\n",
      "Epoch 33_31.238%:  Training average Loss: 0.666590\n",
      "Epoch 33_32.039%:  Training average Loss: 0.666597\n",
      "Epoch 33_32.840%:  Training average Loss: 0.666285\n",
      "Epoch 33_33.641%:  Training average Loss: 0.665909\n",
      "Epoch 33_34.442%:  Training average Loss: 0.665738\n",
      "Epoch 33_35.243%:  Training average Loss: 0.667218\n",
      "Epoch 33_36.044%:  Training average Loss: 0.667438\n",
      "Epoch 33_36.845%:  Training average Loss: 0.667010\n",
      "Epoch 33_37.645%:  Training average Loss: 0.668621\n",
      "Epoch 33_38.446%:  Training average Loss: 0.668554\n",
      "Epoch 33_39.247%:  Training average Loss: 0.667402\n",
      "Epoch 33_40.048%:  Training average Loss: 0.668027\n",
      "Epoch 33_40.849%:  Training average Loss: 0.668540\n",
      "Epoch 33_41.650%:  Training average Loss: 0.669182\n",
      "Epoch 33_42.451%:  Training average Loss: 0.669663\n",
      "Epoch 33_43.252%:  Training average Loss: 0.669938\n",
      "Epoch 33_44.053%:  Training average Loss: 0.670290\n",
      "Epoch 33_44.854%:  Training average Loss: 0.670915\n",
      "Epoch 33_45.655%:  Training average Loss: 0.670937\n",
      "Epoch 33_46.456%:  Training average Loss: 0.671051\n",
      "Epoch 33_47.257%:  Training average Loss: 0.671680\n",
      "Epoch 33_48.058%:  Training average Loss: 0.671942\n",
      "Epoch 33_48.859%:  Training average Loss: 0.672141\n",
      "Epoch 33_49.660%:  Training average Loss: 0.672701\n",
      "Epoch 33_50.461%:  Training average Loss: 0.672802\n",
      "Epoch 33_51.262%:  Training average Loss: 0.672816\n",
      "Epoch 33_52.063%:  Training average Loss: 0.672689\n",
      "Epoch 33_52.864%:  Training average Loss: 0.672964\n",
      "Epoch 33_53.665%:  Training average Loss: 0.673268\n",
      "Epoch 33_54.466%:  Training average Loss: 0.673323\n",
      "Epoch 33_55.267%:  Training average Loss: 0.674068\n",
      "Epoch 33_56.068%:  Training average Loss: 0.674189\n",
      "Epoch 33_56.869%:  Training average Loss: 0.674002\n",
      "Epoch 33_57.670%:  Training average Loss: 0.674061\n",
      "Epoch 33_58.471%:  Training average Loss: 0.674201\n",
      "Epoch 33_59.272%:  Training average Loss: 0.674382\n",
      "Epoch 33_60.073%:  Training average Loss: 0.675147\n",
      "Epoch 33_60.874%:  Training average Loss: 0.675260\n",
      "Epoch 33_61.675%:  Training average Loss: 0.675244\n",
      "Epoch 33_62.475%:  Training average Loss: 0.675291\n",
      "Epoch 33_63.276%:  Training average Loss: 0.675483\n",
      "Epoch 33_64.077%:  Training average Loss: 0.676087\n",
      "Epoch 33_64.878%:  Training average Loss: 0.676151\n",
      "Epoch 33_65.679%:  Training average Loss: 0.676424\n",
      "Epoch 33_66.480%:  Training average Loss: 0.676607\n",
      "Epoch 33_67.281%:  Training average Loss: 0.676300\n",
      "Epoch 33_68.082%:  Training average Loss: 0.676687\n",
      "Epoch 33_68.883%:  Training average Loss: 0.676622\n",
      "Epoch 33_69.684%:  Training average Loss: 0.676567\n",
      "Epoch 33_70.485%:  Training average Loss: 0.676287\n",
      "Epoch 33_71.286%:  Training average Loss: 0.676631\n",
      "Epoch 33_72.087%:  Training average Loss: 0.676925\n",
      "Epoch 33_72.888%:  Training average Loss: 0.677206\n",
      "Epoch 33_73.689%:  Training average Loss: 0.677392\n",
      "Epoch 33_74.490%:  Training average Loss: 0.677606\n",
      "Epoch 33_75.291%:  Training average Loss: 0.677646\n",
      "Epoch 33_76.092%:  Training average Loss: 0.677787\n",
      "Epoch 33_76.893%:  Training average Loss: 0.677951\n",
      "Epoch 33_77.694%:  Training average Loss: 0.678309\n",
      "Epoch 33_78.495%:  Training average Loss: 0.679137\n",
      "Epoch 33_79.296%:  Training average Loss: 0.679026\n",
      "Epoch 33_80.097%:  Training average Loss: 0.679180\n",
      "Epoch 33_80.898%:  Training average Loss: 0.679268\n",
      "Epoch 33_81.699%:  Training average Loss: 0.679267\n",
      "Epoch 33_82.500%:  Training average Loss: 0.679289\n",
      "Epoch 33_83.301%:  Training average Loss: 0.679503\n",
      "Epoch 33_84.102%:  Training average Loss: 0.679509\n",
      "Epoch 33_84.903%:  Training average Loss: 0.679394\n",
      "Epoch 33_85.704%:  Training average Loss: 0.679136\n",
      "Epoch 33_86.504%:  Training average Loss: 0.679479\n",
      "Epoch 33_87.305%:  Training average Loss: 0.679197\n",
      "Epoch 33_88.106%:  Training average Loss: 0.679318\n",
      "Epoch 33_88.907%:  Training average Loss: 0.679651\n",
      "Epoch 33_89.708%:  Training average Loss: 0.679727\n",
      "Epoch 33_90.509%:  Training average Loss: 0.679291\n",
      "Epoch 33_91.310%:  Training average Loss: 0.679512\n",
      "Epoch 33_92.111%:  Training average Loss: 0.679543\n",
      "Epoch 33_92.912%:  Training average Loss: 0.679482\n",
      "Epoch 33_93.713%:  Training average Loss: 0.679866\n",
      "Epoch 33_94.514%:  Training average Loss: 0.680002\n",
      "Epoch 33_95.315%:  Training average Loss: 0.680115\n",
      "Epoch 33_96.116%:  Training average Loss: 0.680428\n",
      "Epoch 33_96.917%:  Training average Loss: 0.680716\n",
      "Epoch 33_97.718%:  Training average Loss: 0.680520\n",
      "Epoch 33_98.519%:  Training average Loss: 0.680521\n",
      "Epoch 33_99.320%:  Training average Loss: 0.680678\n",
      "Epoch 33 :  Verification average Loss: 0.872541, Verification accuracy: 65.594464%,Total Time:5453.758587\n",
      "Model is saved in model_dict/model_lstm/epoch_33_accuracy_0.655945\n",
      "Epoch 34_0.801%:  Training average Loss: 0.651235\n",
      "Epoch 34_1.602%:  Training average Loss: 0.652897\n",
      "Epoch 34_2.403%:  Training average Loss: 0.667099\n",
      "Epoch 34_3.204%:  Training average Loss: 0.667014\n",
      "Epoch 34_4.005%:  Training average Loss: 0.664273\n",
      "Epoch 34_4.806%:  Training average Loss: 0.662779\n",
      "Epoch 34_5.607%:  Training average Loss: 0.664079\n",
      "Epoch 34_6.408%:  Training average Loss: 0.668568\n",
      "Epoch 34_7.209%:  Training average Loss: 0.665996\n",
      "Epoch 34_8.010%:  Training average Loss: 0.665602\n",
      "Epoch 34_8.811%:  Training average Loss: 0.665083\n",
      "Epoch 34_9.612%:  Training average Loss: 0.661248\n",
      "Epoch 34_10.413%:  Training average Loss: 0.659526\n",
      "Epoch 34_11.214%:  Training average Loss: 0.660828\n",
      "Epoch 34_12.015%:  Training average Loss: 0.664981\n",
      "Epoch 34_12.815%:  Training average Loss: 0.663043\n",
      "Epoch 34_13.616%:  Training average Loss: 0.660792\n",
      "Epoch 34_14.417%:  Training average Loss: 0.662513\n",
      "Epoch 34_15.218%:  Training average Loss: 0.660930\n",
      "Epoch 34_16.019%:  Training average Loss: 0.660284\n",
      "Epoch 34_16.820%:  Training average Loss: 0.660566\n",
      "Epoch 34_17.621%:  Training average Loss: 0.657642\n",
      "Epoch 34_18.422%:  Training average Loss: 0.657668\n",
      "Epoch 34_19.223%:  Training average Loss: 0.657577\n",
      "Epoch 34_20.024%:  Training average Loss: 0.658640\n",
      "Epoch 34_20.825%:  Training average Loss: 0.660855\n",
      "Epoch 34_21.626%:  Training average Loss: 0.660814\n",
      "Epoch 34_22.427%:  Training average Loss: 0.661194\n",
      "Epoch 34_23.228%:  Training average Loss: 0.661828\n",
      "Epoch 34_24.029%:  Training average Loss: 0.661169\n",
      "Epoch 34_24.830%:  Training average Loss: 0.661427\n",
      "Epoch 34_25.631%:  Training average Loss: 0.660759\n",
      "Epoch 34_26.432%:  Training average Loss: 0.659967\n",
      "Epoch 34_27.233%:  Training average Loss: 0.661096\n",
      "Epoch 34_28.034%:  Training average Loss: 0.660785\n",
      "Epoch 34_28.835%:  Training average Loss: 0.660837\n",
      "Epoch 34_29.636%:  Training average Loss: 0.662219\n",
      "Epoch 34_30.437%:  Training average Loss: 0.663021\n",
      "Epoch 34_31.238%:  Training average Loss: 0.663109\n",
      "Epoch 34_32.039%:  Training average Loss: 0.663563\n",
      "Epoch 34_32.840%:  Training average Loss: 0.664032\n",
      "Epoch 34_33.641%:  Training average Loss: 0.664513\n",
      "Epoch 34_34.442%:  Training average Loss: 0.665234\n",
      "Epoch 34_35.243%:  Training average Loss: 0.666118\n",
      "Epoch 34_36.044%:  Training average Loss: 0.666344\n",
      "Epoch 34_36.845%:  Training average Loss: 0.667645\n",
      "Epoch 34_37.645%:  Training average Loss: 0.667918\n",
      "Epoch 34_38.446%:  Training average Loss: 0.668479\n",
      "Epoch 34_39.247%:  Training average Loss: 0.668632\n",
      "Epoch 34_40.048%:  Training average Loss: 0.669043\n",
      "Epoch 34_40.849%:  Training average Loss: 0.668792\n",
      "Epoch 34_41.650%:  Training average Loss: 0.669544\n",
      "Epoch 34_42.451%:  Training average Loss: 0.669302\n",
      "Epoch 34_43.252%:  Training average Loss: 0.669314\n",
      "Epoch 34_44.053%:  Training average Loss: 0.669328\n",
      "Epoch 34_44.854%:  Training average Loss: 0.670177\n",
      "Epoch 34_45.655%:  Training average Loss: 0.669938\n",
      "Epoch 34_46.456%:  Training average Loss: 0.670295\n",
      "Epoch 34_47.257%:  Training average Loss: 0.670850\n",
      "Epoch 34_48.058%:  Training average Loss: 0.671339\n",
      "Epoch 34_48.859%:  Training average Loss: 0.671616\n",
      "Epoch 34_49.660%:  Training average Loss: 0.671674\n",
      "Epoch 34_50.461%:  Training average Loss: 0.671338\n",
      "Epoch 34_51.262%:  Training average Loss: 0.671509\n",
      "Epoch 34_52.063%:  Training average Loss: 0.671662\n",
      "Epoch 34_52.864%:  Training average Loss: 0.672465\n",
      "Epoch 34_53.665%:  Training average Loss: 0.672424\n",
      "Epoch 34_54.466%:  Training average Loss: 0.672768\n",
      "Epoch 34_55.267%:  Training average Loss: 0.673094\n",
      "Epoch 34_56.068%:  Training average Loss: 0.673304\n",
      "Epoch 34_56.869%:  Training average Loss: 0.673819\n",
      "Epoch 34_57.670%:  Training average Loss: 0.674167\n",
      "Epoch 34_58.471%:  Training average Loss: 0.674174\n",
      "Epoch 34_59.272%:  Training average Loss: 0.674087\n",
      "Epoch 34_60.073%:  Training average Loss: 0.674719\n",
      "Epoch 34_60.874%:  Training average Loss: 0.675219\n",
      "Epoch 34_61.675%:  Training average Loss: 0.675476\n",
      "Epoch 34_62.475%:  Training average Loss: 0.676149\n",
      "Epoch 34_63.276%:  Training average Loss: 0.676578\n",
      "Epoch 34_64.077%:  Training average Loss: 0.676488\n",
      "Epoch 34_64.878%:  Training average Loss: 0.676967\n",
      "Epoch 34_65.679%:  Training average Loss: 0.677442\n",
      "Epoch 34_66.480%:  Training average Loss: 0.677703\n",
      "Epoch 34_67.281%:  Training average Loss: 0.677809\n",
      "Epoch 34_68.082%:  Training average Loss: 0.678242\n",
      "Epoch 34_68.883%:  Training average Loss: 0.678314\n",
      "Epoch 34_69.684%:  Training average Loss: 0.678540\n",
      "Epoch 34_70.485%:  Training average Loss: 0.678171\n",
      "Epoch 34_71.286%:  Training average Loss: 0.678274\n",
      "Epoch 34_72.087%:  Training average Loss: 0.679081\n",
      "Epoch 34_72.888%:  Training average Loss: 0.678878\n",
      "Epoch 34_73.689%:  Training average Loss: 0.678109\n",
      "Epoch 34_74.490%:  Training average Loss: 0.678405\n",
      "Epoch 34_75.291%:  Training average Loss: 0.678824\n",
      "Epoch 34_76.092%:  Training average Loss: 0.679043\n",
      "Epoch 34_76.893%:  Training average Loss: 0.679562\n",
      "Epoch 34_77.694%:  Training average Loss: 0.680003\n",
      "Epoch 34_78.495%:  Training average Loss: 0.679697\n",
      "Epoch 34_79.296%:  Training average Loss: 0.679878\n",
      "Epoch 34_80.097%:  Training average Loss: 0.679915\n",
      "Epoch 34_80.898%:  Training average Loss: 0.680238\n",
      "Epoch 34_81.699%:  Training average Loss: 0.680084\n",
      "Epoch 34_82.500%:  Training average Loss: 0.680182\n",
      "Epoch 34_83.301%:  Training average Loss: 0.679984\n",
      "Epoch 34_84.102%:  Training average Loss: 0.680068\n",
      "Epoch 34_84.903%:  Training average Loss: 0.680227\n",
      "Epoch 34_85.704%:  Training average Loss: 0.680186\n",
      "Epoch 34_86.504%:  Training average Loss: 0.680428\n",
      "Epoch 34_87.305%:  Training average Loss: 0.680578\n",
      "Epoch 34_88.106%:  Training average Loss: 0.681205\n",
      "Epoch 34_88.907%:  Training average Loss: 0.681067\n",
      "Epoch 34_89.708%:  Training average Loss: 0.680576\n",
      "Epoch 34_90.509%:  Training average Loss: 0.680287\n",
      "Epoch 34_91.310%:  Training average Loss: 0.680494\n",
      "Epoch 34_92.111%:  Training average Loss: 0.680572\n",
      "Epoch 34_92.912%:  Training average Loss: 0.680187\n",
      "Epoch 34_93.713%:  Training average Loss: 0.680278\n",
      "Epoch 34_94.514%:  Training average Loss: 0.680876\n",
      "Epoch 34_95.315%:  Training average Loss: 0.681154\n",
      "Epoch 34_96.116%:  Training average Loss: 0.680740\n",
      "Epoch 34_96.917%:  Training average Loss: 0.681306\n",
      "Epoch 34_97.718%:  Training average Loss: 0.681139\n",
      "Epoch 34_98.519%:  Training average Loss: 0.681094\n",
      "Epoch 34_99.320%:  Training average Loss: 0.681222\n",
      "Epoch 34 :  Verification average Loss: 0.866235, Verification accuracy: 65.546407%,Total Time:5609.894611\n",
      "Epoch 35_0.801%:  Training average Loss: 0.652454\n",
      "Epoch 35_1.602%:  Training average Loss: 0.661773\n",
      "Epoch 35_2.403%:  Training average Loss: 0.668355\n",
      "Epoch 35_3.204%:  Training average Loss: 0.657110\n",
      "Epoch 35_4.005%:  Training average Loss: 0.662745\n",
      "Epoch 35_4.806%:  Training average Loss: 0.670093\n",
      "Epoch 35_5.607%:  Training average Loss: 0.670488\n",
      "Epoch 35_6.408%:  Training average Loss: 0.665090\n",
      "Epoch 35_7.209%:  Training average Loss: 0.664911\n",
      "Epoch 35_8.010%:  Training average Loss: 0.665614\n",
      "Epoch 35_8.811%:  Training average Loss: 0.669762\n",
      "Epoch 35_9.612%:  Training average Loss: 0.671231\n",
      "Epoch 35_10.413%:  Training average Loss: 0.671098\n",
      "Epoch 35_11.214%:  Training average Loss: 0.668522\n",
      "Epoch 35_12.015%:  Training average Loss: 0.670022\n",
      "Epoch 35_12.815%:  Training average Loss: 0.669726\n",
      "Epoch 35_13.616%:  Training average Loss: 0.670980\n",
      "Epoch 35_14.417%:  Training average Loss: 0.672562\n",
      "Epoch 35_15.218%:  Training average Loss: 0.672376\n",
      "Epoch 35_16.019%:  Training average Loss: 0.672136\n",
      "Epoch 35_16.820%:  Training average Loss: 0.670793\n",
      "Epoch 35_17.621%:  Training average Loss: 0.669697\n",
      "Epoch 35_18.422%:  Training average Loss: 0.669507\n",
      "Epoch 35_19.223%:  Training average Loss: 0.667578\n",
      "Epoch 35_20.024%:  Training average Loss: 0.668149\n",
      "Epoch 35_20.825%:  Training average Loss: 0.667294\n",
      "Epoch 35_21.626%:  Training average Loss: 0.667173\n",
      "Epoch 35_22.427%:  Training average Loss: 0.667844\n",
      "Epoch 35_23.228%:  Training average Loss: 0.668320\n",
      "Epoch 35_24.029%:  Training average Loss: 0.666936\n",
      "Epoch 35_24.830%:  Training average Loss: 0.666089\n",
      "Epoch 35_25.631%:  Training average Loss: 0.666522\n",
      "Epoch 35_26.432%:  Training average Loss: 0.666519\n",
      "Epoch 35_27.233%:  Training average Loss: 0.666561\n",
      "Epoch 35_28.034%:  Training average Loss: 0.665724\n",
      "Epoch 35_28.835%:  Training average Loss: 0.665345\n",
      "Epoch 35_29.636%:  Training average Loss: 0.665795\n",
      "Epoch 35_30.437%:  Training average Loss: 0.665543\n",
      "Epoch 35_31.238%:  Training average Loss: 0.666412\n",
      "Epoch 35_32.039%:  Training average Loss: 0.665951\n",
      "Epoch 35_32.840%:  Training average Loss: 0.665521\n",
      "Epoch 35_33.641%:  Training average Loss: 0.665413\n",
      "Epoch 35_34.442%:  Training average Loss: 0.666187\n",
      "Epoch 35_35.243%:  Training average Loss: 0.666674\n",
      "Epoch 35_36.044%:  Training average Loss: 0.666772\n",
      "Epoch 35_36.845%:  Training average Loss: 0.666260\n",
      "Epoch 35_37.645%:  Training average Loss: 0.667048\n",
      "Epoch 35_38.446%:  Training average Loss: 0.667987\n",
      "Epoch 35_39.247%:  Training average Loss: 0.668565\n",
      "Epoch 35_40.048%:  Training average Loss: 0.667888\n",
      "Epoch 35_40.849%:  Training average Loss: 0.668264\n",
      "Epoch 35_41.650%:  Training average Loss: 0.668735\n",
      "Epoch 35_42.451%:  Training average Loss: 0.668760\n",
      "Epoch 35_43.252%:  Training average Loss: 0.668344\n",
      "Epoch 35_44.053%:  Training average Loss: 0.668290\n",
      "Epoch 35_44.854%:  Training average Loss: 0.668979\n",
      "Epoch 35_45.655%:  Training average Loss: 0.669143\n",
      "Epoch 35_46.456%:  Training average Loss: 0.669129\n",
      "Epoch 35_47.257%:  Training average Loss: 0.669743\n",
      "Epoch 35_48.058%:  Training average Loss: 0.669864\n",
      "Epoch 35_48.859%:  Training average Loss: 0.670497\n",
      "Epoch 35_49.660%:  Training average Loss: 0.671309\n",
      "Epoch 35_50.461%:  Training average Loss: 0.671383\n",
      "Epoch 35_51.262%:  Training average Loss: 0.671771\n",
      "Epoch 35_52.063%:  Training average Loss: 0.671255\n",
      "Epoch 35_52.864%:  Training average Loss: 0.671480\n",
      "Epoch 35_53.665%:  Training average Loss: 0.671280\n",
      "Epoch 35_54.466%:  Training average Loss: 0.671618\n",
      "Epoch 35_55.267%:  Training average Loss: 0.671557\n",
      "Epoch 35_56.068%:  Training average Loss: 0.671354\n",
      "Epoch 35_56.869%:  Training average Loss: 0.671457\n",
      "Epoch 35_57.670%:  Training average Loss: 0.671902\n",
      "Epoch 35_58.471%:  Training average Loss: 0.672471\n",
      "Epoch 35_59.272%:  Training average Loss: 0.672506\n",
      "Epoch 35_60.073%:  Training average Loss: 0.673141\n",
      "Epoch 35_60.874%:  Training average Loss: 0.673948\n",
      "Epoch 35_61.675%:  Training average Loss: 0.674740\n",
      "Epoch 35_62.475%:  Training average Loss: 0.675409\n",
      "Epoch 35_63.276%:  Training average Loss: 0.675404\n",
      "Epoch 35_64.077%:  Training average Loss: 0.675606\n",
      "Epoch 35_64.878%:  Training average Loss: 0.675766\n",
      "Epoch 35_65.679%:  Training average Loss: 0.675974\n",
      "Epoch 35_66.480%:  Training average Loss: 0.675560\n",
      "Epoch 35_67.281%:  Training average Loss: 0.675725\n",
      "Epoch 35_68.082%:  Training average Loss: 0.675747\n",
      "Epoch 35_68.883%:  Training average Loss: 0.675545\n",
      "Epoch 35_69.684%:  Training average Loss: 0.675572\n",
      "Epoch 35_70.485%:  Training average Loss: 0.675379\n",
      "Epoch 35_71.286%:  Training average Loss: 0.675429\n",
      "Epoch 35_72.087%:  Training average Loss: 0.675436\n",
      "Epoch 35_72.888%:  Training average Loss: 0.675212\n",
      "Epoch 35_73.689%:  Training average Loss: 0.675297\n",
      "Epoch 35_74.490%:  Training average Loss: 0.675526\n",
      "Epoch 35_75.291%:  Training average Loss: 0.675581\n",
      "Epoch 35_76.092%:  Training average Loss: 0.675415\n",
      "Epoch 35_76.893%:  Training average Loss: 0.675796\n",
      "Epoch 35_77.694%:  Training average Loss: 0.676555\n",
      "Epoch 35_78.495%:  Training average Loss: 0.677334\n",
      "Epoch 35_79.296%:  Training average Loss: 0.677913\n",
      "Epoch 35_80.097%:  Training average Loss: 0.677879\n",
      "Epoch 35_80.898%:  Training average Loss: 0.677632\n",
      "Epoch 35_81.699%:  Training average Loss: 0.677811\n",
      "Epoch 35_82.500%:  Training average Loss: 0.678126\n",
      "Epoch 35_83.301%:  Training average Loss: 0.678727\n",
      "Epoch 35_84.102%:  Training average Loss: 0.678900\n",
      "Epoch 35_84.903%:  Training average Loss: 0.678954\n",
      "Epoch 35_85.704%:  Training average Loss: 0.679144\n",
      "Epoch 35_86.504%:  Training average Loss: 0.679097\n",
      "Epoch 35_87.305%:  Training average Loss: 0.679195\n",
      "Epoch 35_88.106%:  Training average Loss: 0.678524\n",
      "Epoch 35_88.907%:  Training average Loss: 0.679034\n",
      "Epoch 35_89.708%:  Training average Loss: 0.679164\n",
      "Epoch 35_90.509%:  Training average Loss: 0.679410\n",
      "Epoch 35_91.310%:  Training average Loss: 0.679619\n",
      "Epoch 35_92.111%:  Training average Loss: 0.679765\n",
      "Epoch 35_92.912%:  Training average Loss: 0.679774\n",
      "Epoch 35_93.713%:  Training average Loss: 0.679599\n",
      "Epoch 35_94.514%:  Training average Loss: 0.679778\n",
      "Epoch 35_95.315%:  Training average Loss: 0.679727\n",
      "Epoch 35_96.116%:  Training average Loss: 0.679690\n",
      "Epoch 35_96.917%:  Training average Loss: 0.680007\n",
      "Epoch 35_97.718%:  Training average Loss: 0.680311\n",
      "Epoch 35_98.519%:  Training average Loss: 0.680241\n",
      "Epoch 35_99.320%:  Training average Loss: 0.680706\n",
      "Epoch 35 :  Verification average Loss: 0.876918, Verification accuracy: 65.427867%,Total Time:5765.856962\n",
      "Epoch 36_0.801%:  Training average Loss: 0.658105\n",
      "Epoch 36_1.602%:  Training average Loss: 0.669788\n",
      "Epoch 36_2.403%:  Training average Loss: 0.664902\n",
      "Epoch 36_3.204%:  Training average Loss: 0.666773\n",
      "Epoch 36_4.005%:  Training average Loss: 0.666815\n",
      "Epoch 36_4.806%:  Training average Loss: 0.666078\n",
      "Epoch 36_5.607%:  Training average Loss: 0.658761\n",
      "Epoch 36_6.408%:  Training average Loss: 0.661375\n",
      "Epoch 36_7.209%:  Training average Loss: 0.660125\n",
      "Epoch 36_8.010%:  Training average Loss: 0.662050\n",
      "Epoch 36_8.811%:  Training average Loss: 0.661462\n",
      "Epoch 36_9.612%:  Training average Loss: 0.665826\n",
      "Epoch 36_10.413%:  Training average Loss: 0.666390\n",
      "Epoch 36_11.214%:  Training average Loss: 0.668351\n",
      "Epoch 36_12.015%:  Training average Loss: 0.671788\n",
      "Epoch 36_12.815%:  Training average Loss: 0.671285\n",
      "Epoch 36_13.616%:  Training average Loss: 0.672457\n",
      "Epoch 36_14.417%:  Training average Loss: 0.673249\n",
      "Epoch 36_15.218%:  Training average Loss: 0.675314\n",
      "Epoch 36_16.019%:  Training average Loss: 0.676609\n",
      "Epoch 36_16.820%:  Training average Loss: 0.674637\n",
      "Epoch 36_17.621%:  Training average Loss: 0.674714\n",
      "Epoch 36_18.422%:  Training average Loss: 0.673935\n",
      "Epoch 36_19.223%:  Training average Loss: 0.672399\n",
      "Epoch 36_20.024%:  Training average Loss: 0.673091\n",
      "Epoch 36_20.825%:  Training average Loss: 0.673023\n",
      "Epoch 36_21.626%:  Training average Loss: 0.674162\n",
      "Epoch 36_22.427%:  Training average Loss: 0.674346\n",
      "Epoch 36_23.228%:  Training average Loss: 0.674388\n",
      "Epoch 36_24.029%:  Training average Loss: 0.672819\n",
      "Epoch 36_24.830%:  Training average Loss: 0.672682\n",
      "Epoch 36_25.631%:  Training average Loss: 0.672150\n",
      "Epoch 36_26.432%:  Training average Loss: 0.671552\n",
      "Epoch 36_27.233%:  Training average Loss: 0.671495\n",
      "Epoch 36_28.034%:  Training average Loss: 0.670815\n",
      "Epoch 36_28.835%:  Training average Loss: 0.671455\n",
      "Epoch 36_29.636%:  Training average Loss: 0.671259\n",
      "Epoch 36_30.437%:  Training average Loss: 0.670838\n",
      "Epoch 36_31.238%:  Training average Loss: 0.670217\n",
      "Epoch 36_32.039%:  Training average Loss: 0.669746\n",
      "Epoch 36_32.840%:  Training average Loss: 0.670332\n",
      "Epoch 36_33.641%:  Training average Loss: 0.670687\n",
      "Epoch 36_34.442%:  Training average Loss: 0.670647\n",
      "Epoch 36_35.243%:  Training average Loss: 0.669713\n",
      "Epoch 36_36.044%:  Training average Loss: 0.669794\n",
      "Epoch 36_36.845%:  Training average Loss: 0.670025\n",
      "Epoch 36_37.645%:  Training average Loss: 0.670171\n",
      "Epoch 36_38.446%:  Training average Loss: 0.669428\n",
      "Epoch 36_39.247%:  Training average Loss: 0.669015\n",
      "Epoch 36_40.048%:  Training average Loss: 0.669727\n",
      "Epoch 36_40.849%:  Training average Loss: 0.669538\n",
      "Epoch 36_41.650%:  Training average Loss: 0.669923\n",
      "Epoch 36_42.451%:  Training average Loss: 0.670587\n",
      "Epoch 36_43.252%:  Training average Loss: 0.670928\n",
      "Epoch 36_44.053%:  Training average Loss: 0.670102\n",
      "Epoch 36_44.854%:  Training average Loss: 0.670196\n",
      "Epoch 36_45.655%:  Training average Loss: 0.669942\n",
      "Epoch 36_46.456%:  Training average Loss: 0.670874\n",
      "Epoch 36_47.257%:  Training average Loss: 0.670370\n",
      "Epoch 36_48.058%:  Training average Loss: 0.670421\n",
      "Epoch 36_48.859%:  Training average Loss: 0.670913\n",
      "Epoch 36_49.660%:  Training average Loss: 0.670619\n",
      "Epoch 36_50.461%:  Training average Loss: 0.670592\n",
      "Epoch 36_51.262%:  Training average Loss: 0.670811\n",
      "Epoch 36_52.063%:  Training average Loss: 0.669945\n",
      "Epoch 36_52.864%:  Training average Loss: 0.669979\n",
      "Epoch 36_53.665%:  Training average Loss: 0.670597\n",
      "Epoch 36_54.466%:  Training average Loss: 0.669738\n",
      "Epoch 36_55.267%:  Training average Loss: 0.669782\n",
      "Epoch 36_56.068%:  Training average Loss: 0.669797\n",
      "Epoch 36_56.869%:  Training average Loss: 0.670206\n",
      "Epoch 36_57.670%:  Training average Loss: 0.671013\n",
      "Epoch 36_58.471%:  Training average Loss: 0.670624\n",
      "Epoch 36_59.272%:  Training average Loss: 0.670975\n",
      "Epoch 36_60.073%:  Training average Loss: 0.671100\n",
      "Epoch 36_60.874%:  Training average Loss: 0.670715\n",
      "Epoch 36_61.675%:  Training average Loss: 0.671321\n",
      "Epoch 36_62.475%:  Training average Loss: 0.671176\n",
      "Epoch 36_63.276%:  Training average Loss: 0.672023\n",
      "Epoch 36_64.077%:  Training average Loss: 0.672264\n",
      "Epoch 36_64.878%:  Training average Loss: 0.671768\n",
      "Epoch 36_65.679%:  Training average Loss: 0.671096\n",
      "Epoch 36_66.480%:  Training average Loss: 0.671585\n",
      "Epoch 36_67.281%:  Training average Loss: 0.671948\n",
      "Epoch 36_68.082%:  Training average Loss: 0.672271\n",
      "Epoch 36_68.883%:  Training average Loss: 0.672704\n",
      "Epoch 36_69.684%:  Training average Loss: 0.672724\n",
      "Epoch 36_70.485%:  Training average Loss: 0.672877\n",
      "Epoch 36_71.286%:  Training average Loss: 0.672686\n",
      "Epoch 36_72.087%:  Training average Loss: 0.672583\n",
      "Epoch 36_72.888%:  Training average Loss: 0.673080\n",
      "Epoch 36_73.689%:  Training average Loss: 0.672952\n",
      "Epoch 36_74.490%:  Training average Loss: 0.673131\n",
      "Epoch 36_75.291%:  Training average Loss: 0.673138\n",
      "Epoch 36_76.092%:  Training average Loss: 0.673256\n",
      "Epoch 36_76.893%:  Training average Loss: 0.673457\n",
      "Epoch 36_77.694%:  Training average Loss: 0.673791\n",
      "Epoch 36_78.495%:  Training average Loss: 0.674229\n",
      "Epoch 36_79.296%:  Training average Loss: 0.674598\n",
      "Epoch 36_80.097%:  Training average Loss: 0.674905\n",
      "Epoch 36_80.898%:  Training average Loss: 0.674530\n",
      "Epoch 36_81.699%:  Training average Loss: 0.674881\n",
      "Epoch 36_82.500%:  Training average Loss: 0.674583\n",
      "Epoch 36_83.301%:  Training average Loss: 0.674705\n",
      "Epoch 36_84.102%:  Training average Loss: 0.674923\n",
      "Epoch 36_84.903%:  Training average Loss: 0.675111\n",
      "Epoch 36_85.704%:  Training average Loss: 0.675372\n",
      "Epoch 36_86.504%:  Training average Loss: 0.675607\n",
      "Epoch 36_87.305%:  Training average Loss: 0.675370\n",
      "Epoch 36_88.106%:  Training average Loss: 0.675477\n",
      "Epoch 36_88.907%:  Training average Loss: 0.675606\n",
      "Epoch 36_89.708%:  Training average Loss: 0.675648\n",
      "Epoch 36_90.509%:  Training average Loss: 0.675377\n",
      "Epoch 36_91.310%:  Training average Loss: 0.675762\n",
      "Epoch 36_92.111%:  Training average Loss: 0.675728\n",
      "Epoch 36_92.912%:  Training average Loss: 0.675741\n",
      "Epoch 36_93.713%:  Training average Loss: 0.675901\n",
      "Epoch 36_94.514%:  Training average Loss: 0.675940\n",
      "Epoch 36_95.315%:  Training average Loss: 0.675947\n",
      "Epoch 36_96.116%:  Training average Loss: 0.676317\n",
      "Epoch 36_96.917%:  Training average Loss: 0.676637\n",
      "Epoch 36_97.718%:  Training average Loss: 0.676648\n",
      "Epoch 36_98.519%:  Training average Loss: 0.676922\n",
      "Epoch 36_99.320%:  Training average Loss: 0.677021\n",
      "Epoch 36 :  Verification average Loss: 0.880324, Verification accuracy: 65.552815%,Total Time:5921.766640\n",
      "Epoch 37_0.801%:  Training average Loss: 0.621032\n",
      "Epoch 37_1.602%:  Training average Loss: 0.648675\n",
      "Epoch 37_2.403%:  Training average Loss: 0.666129\n",
      "Epoch 37_3.204%:  Training average Loss: 0.656959\n",
      "Epoch 37_4.005%:  Training average Loss: 0.662628\n",
      "Epoch 37_4.806%:  Training average Loss: 0.661766\n",
      "Epoch 37_5.607%:  Training average Loss: 0.661190\n",
      "Epoch 37_6.408%:  Training average Loss: 0.661251\n",
      "Epoch 37_7.209%:  Training average Loss: 0.659291\n",
      "Epoch 37_8.010%:  Training average Loss: 0.658208\n",
      "Epoch 37_8.811%:  Training average Loss: 0.659842\n",
      "Epoch 37_9.612%:  Training average Loss: 0.661810\n",
      "Epoch 37_10.413%:  Training average Loss: 0.661468\n",
      "Epoch 37_11.214%:  Training average Loss: 0.663575\n",
      "Epoch 37_12.015%:  Training average Loss: 0.664223\n",
      "Epoch 37_12.815%:  Training average Loss: 0.665983\n",
      "Epoch 37_13.616%:  Training average Loss: 0.667021\n",
      "Epoch 37_14.417%:  Training average Loss: 0.669892\n",
      "Epoch 37_15.218%:  Training average Loss: 0.669657\n",
      "Epoch 37_16.019%:  Training average Loss: 0.669680\n",
      "Epoch 37_16.820%:  Training average Loss: 0.671221\n",
      "Epoch 37_17.621%:  Training average Loss: 0.671899\n",
      "Epoch 37_18.422%:  Training average Loss: 0.670728\n",
      "Epoch 37_19.223%:  Training average Loss: 0.671750\n",
      "Epoch 37_20.024%:  Training average Loss: 0.670908\n",
      "Epoch 37_20.825%:  Training average Loss: 0.670940\n",
      "Epoch 37_21.626%:  Training average Loss: 0.670753\n",
      "Epoch 37_22.427%:  Training average Loss: 0.671178\n",
      "Epoch 37_23.228%:  Training average Loss: 0.669696\n",
      "Epoch 37_24.029%:  Training average Loss: 0.669672\n",
      "Epoch 37_24.830%:  Training average Loss: 0.670040\n",
      "Epoch 37_25.631%:  Training average Loss: 0.671192\n",
      "Epoch 37_26.432%:  Training average Loss: 0.670680\n",
      "Epoch 37_27.233%:  Training average Loss: 0.671519\n",
      "Epoch 37_28.034%:  Training average Loss: 0.673000\n",
      "Epoch 37_28.835%:  Training average Loss: 0.674520\n",
      "Epoch 37_29.636%:  Training average Loss: 0.674180\n",
      "Epoch 37_30.437%:  Training average Loss: 0.674427\n",
      "Epoch 37_31.238%:  Training average Loss: 0.674708\n",
      "Epoch 37_32.039%:  Training average Loss: 0.674629\n",
      "Epoch 37_32.840%:  Training average Loss: 0.674884\n",
      "Epoch 37_33.641%:  Training average Loss: 0.674360\n",
      "Epoch 37_34.442%:  Training average Loss: 0.674575\n",
      "Epoch 37_35.243%:  Training average Loss: 0.675103\n",
      "Epoch 37_36.044%:  Training average Loss: 0.674463\n",
      "Epoch 37_36.845%:  Training average Loss: 0.673869\n",
      "Epoch 37_37.645%:  Training average Loss: 0.673770\n",
      "Epoch 37_38.446%:  Training average Loss: 0.673280\n",
      "Epoch 37_39.247%:  Training average Loss: 0.672374\n",
      "Epoch 37_40.048%:  Training average Loss: 0.672460\n",
      "Epoch 37_40.849%:  Training average Loss: 0.673362\n",
      "Epoch 37_41.650%:  Training average Loss: 0.673722\n",
      "Epoch 37_42.451%:  Training average Loss: 0.673780\n",
      "Epoch 37_43.252%:  Training average Loss: 0.673993\n",
      "Epoch 37_44.053%:  Training average Loss: 0.673587\n",
      "Epoch 37_44.854%:  Training average Loss: 0.673904\n",
      "Epoch 37_45.655%:  Training average Loss: 0.673843\n",
      "Epoch 37_46.456%:  Training average Loss: 0.674082\n",
      "Epoch 37_47.257%:  Training average Loss: 0.674117\n",
      "Epoch 37_48.058%:  Training average Loss: 0.674250\n",
      "Epoch 37_48.859%:  Training average Loss: 0.674337\n",
      "Epoch 37_49.660%:  Training average Loss: 0.675718\n",
      "Epoch 37_50.461%:  Training average Loss: 0.675332\n",
      "Epoch 37_51.262%:  Training average Loss: 0.675226\n",
      "Epoch 37_52.063%:  Training average Loss: 0.675891\n",
      "Epoch 37_52.864%:  Training average Loss: 0.675475\n",
      "Epoch 37_53.665%:  Training average Loss: 0.674378\n",
      "Epoch 37_54.466%:  Training average Loss: 0.675171\n",
      "Epoch 37_55.267%:  Training average Loss: 0.674456\n",
      "Epoch 37_56.068%:  Training average Loss: 0.674345\n",
      "Epoch 37_56.869%:  Training average Loss: 0.674855\n",
      "Epoch 37_57.670%:  Training average Loss: 0.674854\n",
      "Epoch 37_58.471%:  Training average Loss: 0.675323\n",
      "Epoch 37_59.272%:  Training average Loss: 0.675434\n",
      "Epoch 37_60.073%:  Training average Loss: 0.675251\n",
      "Epoch 37_60.874%:  Training average Loss: 0.675029\n",
      "Epoch 37_61.675%:  Training average Loss: 0.675097\n",
      "Epoch 37_62.475%:  Training average Loss: 0.675314\n",
      "Epoch 37_63.276%:  Training average Loss: 0.675124\n",
      "Epoch 37_64.077%:  Training average Loss: 0.674954\n",
      "Epoch 37_64.878%:  Training average Loss: 0.675426\n",
      "Epoch 37_65.679%:  Training average Loss: 0.675436\n",
      "Epoch 37_66.480%:  Training average Loss: 0.675511\n",
      "Epoch 37_67.281%:  Training average Loss: 0.675592\n",
      "Epoch 37_68.082%:  Training average Loss: 0.675393\n",
      "Epoch 37_68.883%:  Training average Loss: 0.675569\n",
      "Epoch 37_69.684%:  Training average Loss: 0.675357\n",
      "Epoch 37_70.485%:  Training average Loss: 0.675839\n",
      "Epoch 37_71.286%:  Training average Loss: 0.675447\n",
      "Epoch 37_72.087%:  Training average Loss: 0.675062\n",
      "Epoch 37_72.888%:  Training average Loss: 0.674899\n",
      "Epoch 37_73.689%:  Training average Loss: 0.675207\n",
      "Epoch 37_74.490%:  Training average Loss: 0.675416\n",
      "Epoch 37_75.291%:  Training average Loss: 0.675472\n",
      "Epoch 37_76.092%:  Training average Loss: 0.675369\n",
      "Epoch 37_76.893%:  Training average Loss: 0.675665\n",
      "Epoch 37_77.694%:  Training average Loss: 0.676158\n",
      "Epoch 37_78.495%:  Training average Loss: 0.676673\n",
      "Epoch 37_79.296%:  Training average Loss: 0.676690\n",
      "Epoch 37_80.097%:  Training average Loss: 0.676620\n",
      "Epoch 37_80.898%:  Training average Loss: 0.676291\n",
      "Epoch 37_81.699%:  Training average Loss: 0.676054\n",
      "Epoch 37_82.500%:  Training average Loss: 0.676413\n",
      "Epoch 37_83.301%:  Training average Loss: 0.676081\n",
      "Epoch 37_84.102%:  Training average Loss: 0.676139\n",
      "Epoch 37_84.903%:  Training average Loss: 0.675971\n",
      "Epoch 37_85.704%:  Training average Loss: 0.675857\n",
      "Epoch 37_86.504%:  Training average Loss: 0.676485\n",
      "Epoch 37_87.305%:  Training average Loss: 0.676574\n",
      "Epoch 37_88.106%:  Training average Loss: 0.676555\n",
      "Epoch 37_88.907%:  Training average Loss: 0.676585\n",
      "Epoch 37_89.708%:  Training average Loss: 0.677226\n",
      "Epoch 37_90.509%:  Training average Loss: 0.677189\n",
      "Epoch 37_91.310%:  Training average Loss: 0.677352\n",
      "Epoch 37_92.111%:  Training average Loss: 0.677022\n",
      "Epoch 37_92.912%:  Training average Loss: 0.677324\n",
      "Epoch 37_93.713%:  Training average Loss: 0.677149\n",
      "Epoch 37_94.514%:  Training average Loss: 0.676836\n",
      "Epoch 37_95.315%:  Training average Loss: 0.676891\n",
      "Epoch 37_96.116%:  Training average Loss: 0.676947\n",
      "Epoch 37_96.917%:  Training average Loss: 0.677159\n",
      "Epoch 37_97.718%:  Training average Loss: 0.677512\n",
      "Epoch 37_98.519%:  Training average Loss: 0.677440\n",
      "Epoch 37_99.320%:  Training average Loss: 0.677401\n",
      "Epoch 37 :  Verification average Loss: 0.872616, Verification accuracy: 65.101080%,Total Time:6077.504313\n",
      "Epoch 38_0.801%:  Training average Loss: 0.697921\n",
      "Epoch 38_1.602%:  Training average Loss: 0.706476\n",
      "Epoch 38_2.403%:  Training average Loss: 0.693055\n",
      "Epoch 38_3.204%:  Training average Loss: 0.686092\n",
      "Epoch 38_4.005%:  Training average Loss: 0.679402\n",
      "Epoch 38_4.806%:  Training average Loss: 0.673115\n",
      "Epoch 38_5.607%:  Training average Loss: 0.662034\n",
      "Epoch 38_6.408%:  Training average Loss: 0.660965\n",
      "Epoch 38_7.209%:  Training average Loss: 0.657579\n",
      "Epoch 38_8.010%:  Training average Loss: 0.653200\n",
      "Epoch 38_8.811%:  Training average Loss: 0.653926\n",
      "Epoch 38_9.612%:  Training average Loss: 0.656492\n",
      "Epoch 38_10.413%:  Training average Loss: 0.658176\n",
      "Epoch 38_11.214%:  Training average Loss: 0.659694\n",
      "Epoch 38_12.015%:  Training average Loss: 0.657774\n",
      "Epoch 38_12.815%:  Training average Loss: 0.659091\n",
      "Epoch 38_13.616%:  Training average Loss: 0.656202\n",
      "Epoch 38_14.417%:  Training average Loss: 0.656388\n",
      "Epoch 38_15.218%:  Training average Loss: 0.654600\n",
      "Epoch 38_16.019%:  Training average Loss: 0.655455\n",
      "Epoch 38_16.820%:  Training average Loss: 0.655720\n",
      "Epoch 38_17.621%:  Training average Loss: 0.655921\n",
      "Epoch 38_18.422%:  Training average Loss: 0.655670\n",
      "Epoch 38_19.223%:  Training average Loss: 0.656453\n",
      "Epoch 38_20.024%:  Training average Loss: 0.657093\n",
      "Epoch 38_20.825%:  Training average Loss: 0.656043\n",
      "Epoch 38_21.626%:  Training average Loss: 0.656644\n",
      "Epoch 38_22.427%:  Training average Loss: 0.658055\n",
      "Epoch 38_23.228%:  Training average Loss: 0.657879\n",
      "Epoch 38_24.029%:  Training average Loss: 0.659118\n",
      "Epoch 38_24.830%:  Training average Loss: 0.660200\n",
      "Epoch 38_25.631%:  Training average Loss: 0.661253\n",
      "Epoch 38_26.432%:  Training average Loss: 0.662814\n",
      "Epoch 38_27.233%:  Training average Loss: 0.661689\n",
      "Epoch 38_28.034%:  Training average Loss: 0.661094\n",
      "Epoch 38_28.835%:  Training average Loss: 0.660742\n",
      "Epoch 38_29.636%:  Training average Loss: 0.660592\n",
      "Epoch 38_30.437%:  Training average Loss: 0.660747\n",
      "Epoch 38_31.238%:  Training average Loss: 0.660512\n",
      "Epoch 38_32.039%:  Training average Loss: 0.659779\n",
      "Epoch 38_32.840%:  Training average Loss: 0.659182\n",
      "Epoch 38_33.641%:  Training average Loss: 0.660264\n",
      "Epoch 38_34.442%:  Training average Loss: 0.660703\n",
      "Epoch 38_35.243%:  Training average Loss: 0.661116\n",
      "Epoch 38_36.044%:  Training average Loss: 0.660891\n",
      "Epoch 38_36.845%:  Training average Loss: 0.661385\n",
      "Epoch 38_37.645%:  Training average Loss: 0.661934\n",
      "Epoch 38_38.446%:  Training average Loss: 0.663510\n",
      "Epoch 38_39.247%:  Training average Loss: 0.664219\n",
      "Epoch 38_40.048%:  Training average Loss: 0.664140\n",
      "Epoch 38_40.849%:  Training average Loss: 0.663907\n",
      "Epoch 38_41.650%:  Training average Loss: 0.664099\n",
      "Epoch 38_42.451%:  Training average Loss: 0.664348\n",
      "Epoch 38_43.252%:  Training average Loss: 0.664291\n",
      "Epoch 38_44.053%:  Training average Loss: 0.664792\n",
      "Epoch 38_44.854%:  Training average Loss: 0.665076\n",
      "Epoch 38_45.655%:  Training average Loss: 0.664469\n",
      "Epoch 38_46.456%:  Training average Loss: 0.665062\n",
      "Epoch 38_47.257%:  Training average Loss: 0.664484\n",
      "Epoch 38_48.058%:  Training average Loss: 0.664741\n",
      "Epoch 38_48.859%:  Training average Loss: 0.664978\n",
      "Epoch 38_49.660%:  Training average Loss: 0.665154\n",
      "Epoch 38_50.461%:  Training average Loss: 0.666015\n",
      "Epoch 38_51.262%:  Training average Loss: 0.666578\n",
      "Epoch 38_52.063%:  Training average Loss: 0.666563\n",
      "Epoch 38_52.864%:  Training average Loss: 0.666441\n",
      "Epoch 38_53.665%:  Training average Loss: 0.666915\n",
      "Epoch 38_54.466%:  Training average Loss: 0.666885\n",
      "Epoch 38_55.267%:  Training average Loss: 0.666723\n",
      "Epoch 38_56.068%:  Training average Loss: 0.666583\n",
      "Epoch 38_56.869%:  Training average Loss: 0.666624\n",
      "Epoch 38_57.670%:  Training average Loss: 0.666780\n",
      "Epoch 38_58.471%:  Training average Loss: 0.666462\n",
      "Epoch 38_59.272%:  Training average Loss: 0.666509\n",
      "Epoch 38_60.073%:  Training average Loss: 0.666490\n",
      "Epoch 38_60.874%:  Training average Loss: 0.666640\n",
      "Epoch 38_61.675%:  Training average Loss: 0.666784\n",
      "Epoch 38_62.475%:  Training average Loss: 0.666943\n",
      "Epoch 38_63.276%:  Training average Loss: 0.667063\n",
      "Epoch 38_64.077%:  Training average Loss: 0.667692\n",
      "Epoch 38_64.878%:  Training average Loss: 0.667958\n",
      "Epoch 38_65.679%:  Training average Loss: 0.668341\n",
      "Epoch 38_66.480%:  Training average Loss: 0.668483\n",
      "Epoch 38_67.281%:  Training average Loss: 0.668656\n",
      "Epoch 38_68.082%:  Training average Loss: 0.668987\n",
      "Epoch 38_68.883%:  Training average Loss: 0.669048\n",
      "Epoch 38_69.684%:  Training average Loss: 0.668846\n",
      "Epoch 38_70.485%:  Training average Loss: 0.669311\n",
      "Epoch 38_71.286%:  Training average Loss: 0.669258\n",
      "Epoch 38_72.087%:  Training average Loss: 0.669463\n",
      "Epoch 38_72.888%:  Training average Loss: 0.669810\n",
      "Epoch 38_73.689%:  Training average Loss: 0.670215\n",
      "Epoch 38_74.490%:  Training average Loss: 0.670592\n",
      "Epoch 38_75.291%:  Training average Loss: 0.670595\n",
      "Epoch 38_76.092%:  Training average Loss: 0.670620\n",
      "Epoch 38_76.893%:  Training average Loss: 0.671147\n",
      "Epoch 38_77.694%:  Training average Loss: 0.671068\n",
      "Epoch 38_78.495%:  Training average Loss: 0.671903\n",
      "Epoch 38_79.296%:  Training average Loss: 0.671859\n",
      "Epoch 38_80.097%:  Training average Loss: 0.671686\n",
      "Epoch 38_80.898%:  Training average Loss: 0.671835\n",
      "Epoch 38_81.699%:  Training average Loss: 0.672006\n",
      "Epoch 38_82.500%:  Training average Loss: 0.672033\n",
      "Epoch 38_83.301%:  Training average Loss: 0.672266\n",
      "Epoch 38_84.102%:  Training average Loss: 0.672377\n",
      "Epoch 38_84.903%:  Training average Loss: 0.672692\n",
      "Epoch 38_85.704%:  Training average Loss: 0.672827\n",
      "Epoch 38_86.504%:  Training average Loss: 0.673111\n",
      "Epoch 38_87.305%:  Training average Loss: 0.673332\n",
      "Epoch 38_88.106%:  Training average Loss: 0.673792\n",
      "Epoch 38_88.907%:  Training average Loss: 0.673628\n",
      "Epoch 38_89.708%:  Training average Loss: 0.673900\n",
      "Epoch 38_90.509%:  Training average Loss: 0.673976\n",
      "Epoch 38_91.310%:  Training average Loss: 0.673763\n",
      "Epoch 38_92.111%:  Training average Loss: 0.674264\n",
      "Epoch 38_92.912%:  Training average Loss: 0.674637\n",
      "Epoch 38_93.713%:  Training average Loss: 0.674592\n",
      "Epoch 38_94.514%:  Training average Loss: 0.674681\n",
      "Epoch 38_95.315%:  Training average Loss: 0.674707\n",
      "Epoch 38_96.116%:  Training average Loss: 0.674711\n",
      "Epoch 38_96.917%:  Training average Loss: 0.674442\n",
      "Epoch 38_97.718%:  Training average Loss: 0.674511\n",
      "Epoch 38_98.519%:  Training average Loss: 0.674709\n",
      "Epoch 38_99.320%:  Training average Loss: 0.674661\n",
      "Epoch 38 :  Verification average Loss: 0.892307, Verification accuracy: 65.120302%,Total Time:6233.035805\n",
      "Epoch 39_0.801%:  Training average Loss: 0.653776\n",
      "Epoch 39_1.602%:  Training average Loss: 0.678516\n",
      "Epoch 39_2.403%:  Training average Loss: 0.659428\n",
      "Epoch 39_3.204%:  Training average Loss: 0.663185\n",
      "Epoch 39_4.005%:  Training average Loss: 0.665018\n",
      "Epoch 39_4.806%:  Training average Loss: 0.662117\n",
      "Epoch 39_5.607%:  Training average Loss: 0.663193\n",
      "Epoch 39_6.408%:  Training average Loss: 0.665792\n",
      "Epoch 39_7.209%:  Training average Loss: 0.665536\n",
      "Epoch 39_8.010%:  Training average Loss: 0.665472\n",
      "Epoch 39_8.811%:  Training average Loss: 0.666985\n",
      "Epoch 39_9.612%:  Training average Loss: 0.666122\n",
      "Epoch 39_10.413%:  Training average Loss: 0.663943\n",
      "Epoch 39_11.214%:  Training average Loss: 0.665770\n",
      "Epoch 39_12.015%:  Training average Loss: 0.662391\n",
      "Epoch 39_12.815%:  Training average Loss: 0.660443\n",
      "Epoch 39_13.616%:  Training average Loss: 0.661381\n",
      "Epoch 39_14.417%:  Training average Loss: 0.659302\n",
      "Epoch 39_15.218%:  Training average Loss: 0.658833\n",
      "Epoch 39_16.019%:  Training average Loss: 0.658921\n",
      "Epoch 39_16.820%:  Training average Loss: 0.659534\n",
      "Epoch 39_17.621%:  Training average Loss: 0.658988\n",
      "Epoch 39_18.422%:  Training average Loss: 0.657512\n",
      "Epoch 39_19.223%:  Training average Loss: 0.658168\n",
      "Epoch 39_20.024%:  Training average Loss: 0.658141\n",
      "Epoch 39_20.825%:  Training average Loss: 0.658168\n",
      "Epoch 39_21.626%:  Training average Loss: 0.657567\n",
      "Epoch 39_22.427%:  Training average Loss: 0.657954\n",
      "Epoch 39_23.228%:  Training average Loss: 0.658881\n",
      "Epoch 39_24.029%:  Training average Loss: 0.658293\n",
      "Epoch 39_24.830%:  Training average Loss: 0.657191\n",
      "Epoch 39_25.631%:  Training average Loss: 0.659958\n",
      "Epoch 39_26.432%:  Training average Loss: 0.659867\n",
      "Epoch 39_27.233%:  Training average Loss: 0.659100\n",
      "Epoch 39_28.034%:  Training average Loss: 0.660093\n",
      "Epoch 39_28.835%:  Training average Loss: 0.658753\n",
      "Epoch 39_29.636%:  Training average Loss: 0.659961\n",
      "Epoch 39_30.437%:  Training average Loss: 0.659836\n",
      "Epoch 39_31.238%:  Training average Loss: 0.660140\n",
      "Epoch 39_32.039%:  Training average Loss: 0.660992\n",
      "Epoch 39_32.840%:  Training average Loss: 0.661429\n",
      "Epoch 39_33.641%:  Training average Loss: 0.661135\n",
      "Epoch 39_34.442%:  Training average Loss: 0.660936\n",
      "Epoch 39_35.243%:  Training average Loss: 0.661193\n",
      "Epoch 39_36.044%:  Training average Loss: 0.661072\n",
      "Epoch 39_36.845%:  Training average Loss: 0.661234\n",
      "Epoch 39_37.645%:  Training average Loss: 0.660530\n",
      "Epoch 39_38.446%:  Training average Loss: 0.661227\n",
      "Epoch 39_39.247%:  Training average Loss: 0.661661\n",
      "Epoch 39_40.048%:  Training average Loss: 0.661977\n",
      "Epoch 39_40.849%:  Training average Loss: 0.662590\n",
      "Epoch 39_41.650%:  Training average Loss: 0.662447\n",
      "Epoch 39_42.451%:  Training average Loss: 0.663054\n",
      "Epoch 39_43.252%:  Training average Loss: 0.664217\n",
      "Epoch 39_44.053%:  Training average Loss: 0.664516\n",
      "Epoch 39_44.854%:  Training average Loss: 0.665096\n",
      "Epoch 39_45.655%:  Training average Loss: 0.665642\n",
      "Epoch 39_46.456%:  Training average Loss: 0.666013\n",
      "Epoch 39_47.257%:  Training average Loss: 0.666162\n",
      "Epoch 39_48.058%:  Training average Loss: 0.666002\n",
      "Epoch 39_48.859%:  Training average Loss: 0.666443\n",
      "Epoch 39_49.660%:  Training average Loss: 0.666552\n",
      "Epoch 39_50.461%:  Training average Loss: 0.666677\n",
      "Epoch 39_51.262%:  Training average Loss: 0.666670\n",
      "Epoch 39_52.063%:  Training average Loss: 0.666309\n",
      "Epoch 39_52.864%:  Training average Loss: 0.666580\n",
      "Epoch 39_53.665%:  Training average Loss: 0.666202\n",
      "Epoch 39_54.466%:  Training average Loss: 0.666619\n",
      "Epoch 39_55.267%:  Training average Loss: 0.666305\n",
      "Epoch 39_56.068%:  Training average Loss: 0.666689\n",
      "Epoch 39_56.869%:  Training average Loss: 0.666618\n",
      "Epoch 39_57.670%:  Training average Loss: 0.666701\n",
      "Epoch 39_58.471%:  Training average Loss: 0.666693\n",
      "Epoch 39_59.272%:  Training average Loss: 0.666503\n",
      "Epoch 39_60.073%:  Training average Loss: 0.666846\n",
      "Epoch 39_60.874%:  Training average Loss: 0.667344\n",
      "Epoch 39_61.675%:  Training average Loss: 0.667566\n",
      "Epoch 39_62.475%:  Training average Loss: 0.668024\n",
      "Epoch 39_63.276%:  Training average Loss: 0.668743\n",
      "Epoch 39_64.077%:  Training average Loss: 0.668738\n",
      "Epoch 39_64.878%:  Training average Loss: 0.669005\n",
      "Epoch 39_65.679%:  Training average Loss: 0.668980\n",
      "Epoch 39_66.480%:  Training average Loss: 0.669277\n",
      "Epoch 39_67.281%:  Training average Loss: 0.669577\n",
      "Epoch 39_68.082%:  Training average Loss: 0.669909\n",
      "Epoch 39_68.883%:  Training average Loss: 0.670074\n",
      "Epoch 39_69.684%:  Training average Loss: 0.670327\n",
      "Epoch 39_70.485%:  Training average Loss: 0.670455\n",
      "Epoch 39_71.286%:  Training average Loss: 0.670352\n",
      "Epoch 39_72.087%:  Training average Loss: 0.670914\n",
      "Epoch 39_72.888%:  Training average Loss: 0.671173\n",
      "Epoch 39_73.689%:  Training average Loss: 0.671275\n",
      "Epoch 39_74.490%:  Training average Loss: 0.671300\n",
      "Epoch 39_75.291%:  Training average Loss: 0.672130\n",
      "Epoch 39_76.092%:  Training average Loss: 0.671841\n",
      "Epoch 39_76.893%:  Training average Loss: 0.671423\n",
      "Epoch 39_77.694%:  Training average Loss: 0.671664\n",
      "Epoch 39_78.495%:  Training average Loss: 0.671578\n",
      "Epoch 39_79.296%:  Training average Loss: 0.671699\n",
      "Epoch 39_80.097%:  Training average Loss: 0.671806\n",
      "Epoch 39_80.898%:  Training average Loss: 0.671694\n",
      "Epoch 39_81.699%:  Training average Loss: 0.671745\n",
      "Epoch 39_82.500%:  Training average Loss: 0.671631\n",
      "Epoch 39_83.301%:  Training average Loss: 0.672377\n",
      "Epoch 39_84.102%:  Training average Loss: 0.672746\n",
      "Epoch 39_84.903%:  Training average Loss: 0.672938\n",
      "Epoch 39_85.704%:  Training average Loss: 0.673018\n",
      "Epoch 39_86.504%:  Training average Loss: 0.673385\n",
      "Epoch 39_87.305%:  Training average Loss: 0.673276\n",
      "Epoch 39_88.106%:  Training average Loss: 0.673404\n",
      "Epoch 39_88.907%:  Training average Loss: 0.673522\n",
      "Epoch 39_89.708%:  Training average Loss: 0.673127\n",
      "Epoch 39_90.509%:  Training average Loss: 0.673339\n",
      "Epoch 39_91.310%:  Training average Loss: 0.673664\n",
      "Epoch 39_92.111%:  Training average Loss: 0.673942\n",
      "Epoch 39_92.912%:  Training average Loss: 0.674114\n",
      "Epoch 39_93.713%:  Training average Loss: 0.674434\n",
      "Epoch 39_94.514%:  Training average Loss: 0.674447\n",
      "Epoch 39_95.315%:  Training average Loss: 0.674562\n",
      "Epoch 39_96.116%:  Training average Loss: 0.674843\n",
      "Epoch 39_96.917%:  Training average Loss: 0.675327\n",
      "Epoch 39_97.718%:  Training average Loss: 0.675352\n",
      "Epoch 39_98.519%:  Training average Loss: 0.675100\n",
      "Epoch 39_99.320%:  Training average Loss: 0.675181\n",
      "Epoch 39 :  Verification average Loss: 0.890684, Verification accuracy: 65.245250%,Total Time:6388.574657\n",
      "Epoch 40_0.801%:  Training average Loss: 0.681663\n",
      "Epoch 40_1.602%:  Training average Loss: 0.684035\n",
      "Epoch 40_2.403%:  Training average Loss: 0.674867\n",
      "Epoch 40_3.204%:  Training average Loss: 0.682057\n",
      "Epoch 40_4.005%:  Training average Loss: 0.675961\n",
      "Epoch 40_4.806%:  Training average Loss: 0.664685\n",
      "Epoch 40_5.607%:  Training average Loss: 0.665234\n",
      "Epoch 40_6.408%:  Training average Loss: 0.668166\n",
      "Epoch 40_7.209%:  Training average Loss: 0.667445\n",
      "Epoch 40_8.010%:  Training average Loss: 0.665595\n",
      "Epoch 40_8.811%:  Training average Loss: 0.665354\n",
      "Epoch 40_9.612%:  Training average Loss: 0.666459\n",
      "Epoch 40_10.413%:  Training average Loss: 0.669942\n",
      "Epoch 40_11.214%:  Training average Loss: 0.671824\n",
      "Epoch 40_12.015%:  Training average Loss: 0.671420\n",
      "Epoch 40_12.815%:  Training average Loss: 0.669889\n",
      "Epoch 40_13.616%:  Training average Loss: 0.669179\n",
      "Epoch 40_14.417%:  Training average Loss: 0.671146\n",
      "Epoch 40_15.218%:  Training average Loss: 0.668517\n",
      "Epoch 40_16.019%:  Training average Loss: 0.666358\n",
      "Epoch 40_16.820%:  Training average Loss: 0.666177\n",
      "Epoch 40_17.621%:  Training average Loss: 0.666764\n",
      "Epoch 40_18.422%:  Training average Loss: 0.666064\n",
      "Epoch 40_19.223%:  Training average Loss: 0.666335\n",
      "Epoch 40_20.024%:  Training average Loss: 0.666162\n",
      "Epoch 40_20.825%:  Training average Loss: 0.664369\n",
      "Epoch 40_21.626%:  Training average Loss: 0.663423\n",
      "Epoch 40_22.427%:  Training average Loss: 0.663574\n",
      "Epoch 40_23.228%:  Training average Loss: 0.663500\n",
      "Epoch 40_24.029%:  Training average Loss: 0.662938\n",
      "Epoch 40_24.830%:  Training average Loss: 0.662952\n",
      "Epoch 40_25.631%:  Training average Loss: 0.664702\n",
      "Epoch 40_26.432%:  Training average Loss: 0.665189\n",
      "Epoch 40_27.233%:  Training average Loss: 0.666044\n",
      "Epoch 40_28.034%:  Training average Loss: 0.666104\n",
      "Epoch 40_28.835%:  Training average Loss: 0.666629\n",
      "Epoch 40_29.636%:  Training average Loss: 0.665756\n",
      "Epoch 40_30.437%:  Training average Loss: 0.665696\n",
      "Epoch 40_31.238%:  Training average Loss: 0.665212\n",
      "Epoch 40_32.039%:  Training average Loss: 0.665831\n",
      "Epoch 40_32.840%:  Training average Loss: 0.665609\n",
      "Epoch 40_33.641%:  Training average Loss: 0.666603\n",
      "Epoch 40_34.442%:  Training average Loss: 0.666385\n",
      "Epoch 40_35.243%:  Training average Loss: 0.666938\n",
      "Epoch 40_36.044%:  Training average Loss: 0.666917\n",
      "Epoch 40_36.845%:  Training average Loss: 0.665978\n",
      "Epoch 40_37.645%:  Training average Loss: 0.666345\n",
      "Epoch 40_38.446%:  Training average Loss: 0.666658\n",
      "Epoch 40_39.247%:  Training average Loss: 0.667476\n",
      "Epoch 40_40.048%:  Training average Loss: 0.667594\n",
      "Epoch 40_40.849%:  Training average Loss: 0.667849\n",
      "Epoch 40_41.650%:  Training average Loss: 0.668313\n",
      "Epoch 40_42.451%:  Training average Loss: 0.668166\n",
      "Epoch 40_43.252%:  Training average Loss: 0.668797\n",
      "Epoch 40_44.053%:  Training average Loss: 0.668567\n",
      "Epoch 40_44.854%:  Training average Loss: 0.668258\n",
      "Epoch 40_45.655%:  Training average Loss: 0.668488\n",
      "Epoch 40_46.456%:  Training average Loss: 0.669062\n",
      "Epoch 40_47.257%:  Training average Loss: 0.669470\n",
      "Epoch 40_48.058%:  Training average Loss: 0.669349\n",
      "Epoch 40_48.859%:  Training average Loss: 0.670173\n",
      "Epoch 40_49.660%:  Training average Loss: 0.670327\n",
      "Epoch 40_50.461%:  Training average Loss: 0.670568\n",
      "Epoch 40_51.262%:  Training average Loss: 0.670675\n",
      "Epoch 40_52.063%:  Training average Loss: 0.670222\n",
      "Epoch 40_52.864%:  Training average Loss: 0.670759\n",
      "Epoch 40_53.665%:  Training average Loss: 0.671169\n",
      "Epoch 40_54.466%:  Training average Loss: 0.671252\n",
      "Epoch 40_55.267%:  Training average Loss: 0.670630\n",
      "Epoch 40_56.068%:  Training average Loss: 0.669984\n",
      "Epoch 40_56.869%:  Training average Loss: 0.670251\n",
      "Epoch 40_57.670%:  Training average Loss: 0.670177\n",
      "Epoch 40_58.471%:  Training average Loss: 0.670272\n",
      "Epoch 40_59.272%:  Training average Loss: 0.670434\n",
      "Epoch 40_60.073%:  Training average Loss: 0.670594\n",
      "Epoch 40_60.874%:  Training average Loss: 0.670390\n",
      "Epoch 40_61.675%:  Training average Loss: 0.670797\n",
      "Epoch 40_62.475%:  Training average Loss: 0.670899\n",
      "Epoch 40_63.276%:  Training average Loss: 0.670524\n",
      "Epoch 40_64.077%:  Training average Loss: 0.670356\n",
      "Epoch 40_64.878%:  Training average Loss: 0.670290\n",
      "Epoch 40_65.679%:  Training average Loss: 0.670327\n",
      "Epoch 40_66.480%:  Training average Loss: 0.669685\n",
      "Epoch 40_67.281%:  Training average Loss: 0.670424\n",
      "Epoch 40_68.082%:  Training average Loss: 0.670212\n",
      "Epoch 40_68.883%:  Training average Loss: 0.670508\n",
      "Epoch 40_69.684%:  Training average Loss: 0.670384\n",
      "Epoch 40_70.485%:  Training average Loss: 0.670442\n",
      "Epoch 40_71.286%:  Training average Loss: 0.670783\n",
      "Epoch 40_72.087%:  Training average Loss: 0.670681\n",
      "Epoch 40_72.888%:  Training average Loss: 0.670785\n",
      "Epoch 40_73.689%:  Training average Loss: 0.670754\n",
      "Epoch 40_74.490%:  Training average Loss: 0.670534\n",
      "Epoch 40_75.291%:  Training average Loss: 0.670644\n",
      "Epoch 40_76.092%:  Training average Loss: 0.670728\n",
      "Epoch 40_76.893%:  Training average Loss: 0.671013\n",
      "Epoch 40_77.694%:  Training average Loss: 0.670890\n",
      "Epoch 40_78.495%:  Training average Loss: 0.671190\n",
      "Epoch 40_79.296%:  Training average Loss: 0.671461\n",
      "Epoch 40_80.097%:  Training average Loss: 0.671210\n",
      "Epoch 40_80.898%:  Training average Loss: 0.670903\n",
      "Epoch 40_81.699%:  Training average Loss: 0.670800\n",
      "Epoch 40_82.500%:  Training average Loss: 0.670995\n",
      "Epoch 40_83.301%:  Training average Loss: 0.671341\n",
      "Epoch 40_84.102%:  Training average Loss: 0.671402\n",
      "Epoch 40_84.903%:  Training average Loss: 0.671811\n",
      "Epoch 40_85.704%:  Training average Loss: 0.671688\n",
      "Epoch 40_86.504%:  Training average Loss: 0.671593\n",
      "Epoch 40_87.305%:  Training average Loss: 0.672084\n",
      "Epoch 40_88.106%:  Training average Loss: 0.672192\n",
      "Epoch 40_88.907%:  Training average Loss: 0.672237\n",
      "Epoch 40_89.708%:  Training average Loss: 0.672186\n",
      "Epoch 40_90.509%:  Training average Loss: 0.672559\n",
      "Epoch 40_91.310%:  Training average Loss: 0.672532\n",
      "Epoch 40_92.111%:  Training average Loss: 0.672526\n",
      "Epoch 40_92.912%:  Training average Loss: 0.672387\n",
      "Epoch 40_93.713%:  Training average Loss: 0.672703\n",
      "Epoch 40_94.514%:  Training average Loss: 0.673048\n",
      "Epoch 40_95.315%:  Training average Loss: 0.673227\n",
      "Epoch 40_96.116%:  Training average Loss: 0.673644\n",
      "Epoch 40_96.917%:  Training average Loss: 0.673641\n",
      "Epoch 40_97.718%:  Training average Loss: 0.674026\n",
      "Epoch 40_98.519%:  Training average Loss: 0.673910\n",
      "Epoch 40_99.320%:  Training average Loss: 0.673817\n",
      "Epoch 40 :  Verification average Loss: 0.875815, Verification accuracy: 65.210009%,Total Time:6544.752131\n",
      "Epoch 41_0.801%:  Training average Loss: 0.659770\n",
      "Epoch 41_1.602%:  Training average Loss: 0.656546\n",
      "Epoch 41_2.403%:  Training average Loss: 0.652724\n",
      "Epoch 41_3.204%:  Training average Loss: 0.650124\n",
      "Epoch 41_4.005%:  Training average Loss: 0.657448\n",
      "Epoch 41_4.806%:  Training average Loss: 0.654181\n",
      "Epoch 41_5.607%:  Training average Loss: 0.654801\n",
      "Epoch 41_6.408%:  Training average Loss: 0.652834\n",
      "Epoch 41_7.209%:  Training average Loss: 0.653871\n",
      "Epoch 41_8.010%:  Training average Loss: 0.651528\n",
      "Epoch 41_8.811%:  Training average Loss: 0.649651\n",
      "Epoch 41_9.612%:  Training average Loss: 0.650334\n",
      "Epoch 41_10.413%:  Training average Loss: 0.648061\n",
      "Epoch 41_11.214%:  Training average Loss: 0.646799\n",
      "Epoch 41_12.015%:  Training average Loss: 0.647125\n",
      "Epoch 41_12.815%:  Training average Loss: 0.646478\n",
      "Epoch 41_13.616%:  Training average Loss: 0.647851\n",
      "Epoch 41_14.417%:  Training average Loss: 0.645992\n",
      "Epoch 41_15.218%:  Training average Loss: 0.647698\n",
      "Epoch 41_16.019%:  Training average Loss: 0.650629\n",
      "Epoch 41_16.820%:  Training average Loss: 0.651247\n",
      "Epoch 41_17.621%:  Training average Loss: 0.651579\n",
      "Epoch 41_18.422%:  Training average Loss: 0.650580\n",
      "Epoch 41_19.223%:  Training average Loss: 0.650822\n",
      "Epoch 41_20.024%:  Training average Loss: 0.651053\n",
      "Epoch 41_20.825%:  Training average Loss: 0.651669\n",
      "Epoch 41_21.626%:  Training average Loss: 0.652108\n",
      "Epoch 41_22.427%:  Training average Loss: 0.653425\n",
      "Epoch 41_23.228%:  Training average Loss: 0.654376\n",
      "Epoch 41_24.029%:  Training average Loss: 0.654824\n",
      "Epoch 41_24.830%:  Training average Loss: 0.655488\n",
      "Epoch 41_25.631%:  Training average Loss: 0.655785\n",
      "Epoch 41_26.432%:  Training average Loss: 0.655771\n",
      "Epoch 41_27.233%:  Training average Loss: 0.655344\n",
      "Epoch 41_28.034%:  Training average Loss: 0.656880\n",
      "Epoch 41_28.835%:  Training average Loss: 0.656717\n",
      "Epoch 41_29.636%:  Training average Loss: 0.656365\n",
      "Epoch 41_30.437%:  Training average Loss: 0.657100\n",
      "Epoch 41_31.238%:  Training average Loss: 0.658813\n",
      "Epoch 41_32.039%:  Training average Loss: 0.659430\n",
      "Epoch 41_32.840%:  Training average Loss: 0.658444\n",
      "Epoch 41_33.641%:  Training average Loss: 0.659170\n",
      "Epoch 41_34.442%:  Training average Loss: 0.659345\n",
      "Epoch 41_35.243%:  Training average Loss: 0.658506\n",
      "Epoch 41_36.044%:  Training average Loss: 0.658523\n",
      "Epoch 41_36.845%:  Training average Loss: 0.658123\n",
      "Epoch 41_37.645%:  Training average Loss: 0.658331\n",
      "Epoch 41_38.446%:  Training average Loss: 0.658815\n",
      "Epoch 41_39.247%:  Training average Loss: 0.660326\n",
      "Epoch 41_40.048%:  Training average Loss: 0.661012\n",
      "Epoch 41_40.849%:  Training average Loss: 0.661637\n",
      "Epoch 41_41.650%:  Training average Loss: 0.660974\n",
      "Epoch 41_42.451%:  Training average Loss: 0.660868\n",
      "Epoch 41_43.252%:  Training average Loss: 0.661845\n",
      "Epoch 41_44.053%:  Training average Loss: 0.662601\n",
      "Epoch 41_44.854%:  Training average Loss: 0.662715\n",
      "Epoch 41_45.655%:  Training average Loss: 0.663192\n",
      "Epoch 41_46.456%:  Training average Loss: 0.663766\n",
      "Epoch 41_47.257%:  Training average Loss: 0.663988\n",
      "Epoch 41_48.058%:  Training average Loss: 0.664675\n",
      "Epoch 41_48.859%:  Training average Loss: 0.664252\n",
      "Epoch 41_49.660%:  Training average Loss: 0.664489\n",
      "Epoch 41_50.461%:  Training average Loss: 0.664444\n",
      "Epoch 41_51.262%:  Training average Loss: 0.664904\n",
      "Epoch 41_52.063%:  Training average Loss: 0.665337\n",
      "Epoch 41_52.864%:  Training average Loss: 0.665538\n",
      "Epoch 41_53.665%:  Training average Loss: 0.665189\n",
      "Epoch 41_54.466%:  Training average Loss: 0.665232\n",
      "Epoch 41_55.267%:  Training average Loss: 0.665724\n",
      "Epoch 41_56.068%:  Training average Loss: 0.665417\n",
      "Epoch 41_56.869%:  Training average Loss: 0.666503\n",
      "Epoch 41_57.670%:  Training average Loss: 0.666814\n",
      "Epoch 41_58.471%:  Training average Loss: 0.667127\n",
      "Epoch 41_59.272%:  Training average Loss: 0.667466\n",
      "Epoch 41_60.073%:  Training average Loss: 0.667257\n",
      "Epoch 41_60.874%:  Training average Loss: 0.667835\n",
      "Epoch 41_61.675%:  Training average Loss: 0.668264\n",
      "Epoch 41_62.475%:  Training average Loss: 0.668052\n",
      "Epoch 41_63.276%:  Training average Loss: 0.667652\n",
      "Epoch 41_64.077%:  Training average Loss: 0.668081\n",
      "Epoch 41_64.878%:  Training average Loss: 0.668179\n",
      "Epoch 41_65.679%:  Training average Loss: 0.667806\n",
      "Epoch 41_66.480%:  Training average Loss: 0.668011\n",
      "Epoch 41_67.281%:  Training average Loss: 0.668287\n",
      "Epoch 41_68.082%:  Training average Loss: 0.668207\n",
      "Epoch 41_68.883%:  Training average Loss: 0.668154\n",
      "Epoch 41_69.684%:  Training average Loss: 0.668126\n",
      "Epoch 41_70.485%:  Training average Loss: 0.667614\n",
      "Epoch 41_71.286%:  Training average Loss: 0.667668\n",
      "Epoch 41_72.087%:  Training average Loss: 0.668382\n",
      "Epoch 41_72.888%:  Training average Loss: 0.668594\n",
      "Epoch 41_73.689%:  Training average Loss: 0.668997\n",
      "Epoch 41_74.490%:  Training average Loss: 0.668786\n",
      "Epoch 41_75.291%:  Training average Loss: 0.668984\n",
      "Epoch 41_76.092%:  Training average Loss: 0.669074\n",
      "Epoch 41_76.893%:  Training average Loss: 0.669565\n",
      "Epoch 41_77.694%:  Training average Loss: 0.670140\n",
      "Epoch 41_78.495%:  Training average Loss: 0.669968\n",
      "Epoch 41_79.296%:  Training average Loss: 0.669798\n",
      "Epoch 41_80.097%:  Training average Loss: 0.669859\n",
      "Epoch 41_80.898%:  Training average Loss: 0.669879\n",
      "Epoch 41_81.699%:  Training average Loss: 0.669959\n",
      "Epoch 41_82.500%:  Training average Loss: 0.670243\n",
      "Epoch 41_83.301%:  Training average Loss: 0.670187\n",
      "Epoch 41_84.102%:  Training average Loss: 0.670371\n",
      "Epoch 41_84.903%:  Training average Loss: 0.670245\n",
      "Epoch 41_85.704%:  Training average Loss: 0.670512\n",
      "Epoch 41_86.504%:  Training average Loss: 0.670327\n",
      "Epoch 41_87.305%:  Training average Loss: 0.670289\n",
      "Epoch 41_88.106%:  Training average Loss: 0.670193\n",
      "Epoch 41_88.907%:  Training average Loss: 0.670585\n",
      "Epoch 41_89.708%:  Training average Loss: 0.670707\n",
      "Epoch 41_90.509%:  Training average Loss: 0.670887\n",
      "Epoch 41_91.310%:  Training average Loss: 0.671128\n",
      "Epoch 41_92.111%:  Training average Loss: 0.671023\n",
      "Epoch 41_92.912%:  Training average Loss: 0.671023\n",
      "Epoch 41_93.713%:  Training average Loss: 0.671311\n",
      "Epoch 41_94.514%:  Training average Loss: 0.671313\n",
      "Epoch 41_95.315%:  Training average Loss: 0.671294\n",
      "Epoch 41_96.116%:  Training average Loss: 0.671085\n",
      "Epoch 41_96.917%:  Training average Loss: 0.671365\n",
      "Epoch 41_97.718%:  Training average Loss: 0.671749\n",
      "Epoch 41_98.519%:  Training average Loss: 0.671743\n",
      "Epoch 41_99.320%:  Training average Loss: 0.671756\n",
      "Epoch 41 :  Verification average Loss: 0.895107, Verification accuracy: 65.129914%,Total Time:6700.634455\n",
      "Epoch 42_0.801%:  Training average Loss: 0.665380\n",
      "Epoch 42_1.602%:  Training average Loss: 0.671967\n",
      "Epoch 42_2.403%:  Training average Loss: 0.665348\n",
      "Epoch 42_3.204%:  Training average Loss: 0.664028\n",
      "Epoch 42_4.005%:  Training average Loss: 0.661659\n",
      "Epoch 42_4.806%:  Training average Loss: 0.658211\n",
      "Epoch 42_5.607%:  Training average Loss: 0.653312\n",
      "Epoch 42_6.408%:  Training average Loss: 0.660456\n",
      "Epoch 42_7.209%:  Training average Loss: 0.660896\n",
      "Epoch 42_8.010%:  Training average Loss: 0.656999\n",
      "Epoch 42_8.811%:  Training average Loss: 0.660825\n",
      "Epoch 42_9.612%:  Training average Loss: 0.659104\n",
      "Epoch 42_10.413%:  Training average Loss: 0.659210\n",
      "Epoch 42_11.214%:  Training average Loss: 0.658038\n",
      "Epoch 42_12.015%:  Training average Loss: 0.657735\n",
      "Epoch 42_12.815%:  Training average Loss: 0.659329\n",
      "Epoch 42_13.616%:  Training average Loss: 0.660244\n",
      "Epoch 42_14.417%:  Training average Loss: 0.661623\n",
      "Epoch 42_15.218%:  Training average Loss: 0.660967\n",
      "Epoch 42_16.019%:  Training average Loss: 0.660572\n",
      "Epoch 42_16.820%:  Training average Loss: 0.657987\n",
      "Epoch 42_17.621%:  Training average Loss: 0.658127\n",
      "Epoch 42_18.422%:  Training average Loss: 0.656718\n",
      "Epoch 42_19.223%:  Training average Loss: 0.655839\n",
      "Epoch 42_20.024%:  Training average Loss: 0.657816\n",
      "Epoch 42_20.825%:  Training average Loss: 0.657723\n",
      "Epoch 42_21.626%:  Training average Loss: 0.655662\n",
      "Epoch 42_22.427%:  Training average Loss: 0.656155\n",
      "Epoch 42_23.228%:  Training average Loss: 0.658573\n",
      "Epoch 42_24.029%:  Training average Loss: 0.657574\n",
      "Epoch 42_24.830%:  Training average Loss: 0.657825\n",
      "Epoch 42_25.631%:  Training average Loss: 0.659455\n",
      "Epoch 42_26.432%:  Training average Loss: 0.659516\n",
      "Epoch 42_27.233%:  Training average Loss: 0.659828\n",
      "Epoch 42_28.034%:  Training average Loss: 0.659799\n",
      "Epoch 42_28.835%:  Training average Loss: 0.660848\n",
      "Epoch 42_29.636%:  Training average Loss: 0.661606\n",
      "Epoch 42_30.437%:  Training average Loss: 0.660922\n",
      "Epoch 42_31.238%:  Training average Loss: 0.660278\n",
      "Epoch 42_32.039%:  Training average Loss: 0.661088\n",
      "Epoch 42_32.840%:  Training average Loss: 0.659772\n",
      "Epoch 42_33.641%:  Training average Loss: 0.659754\n",
      "Epoch 42_34.442%:  Training average Loss: 0.660147\n",
      "Epoch 42_35.243%:  Training average Loss: 0.660694\n",
      "Epoch 42_36.044%:  Training average Loss: 0.661102\n",
      "Epoch 42_36.845%:  Training average Loss: 0.660256\n",
      "Epoch 42_37.645%:  Training average Loss: 0.660087\n",
      "Epoch 42_38.446%:  Training average Loss: 0.660144\n",
      "Epoch 42_39.247%:  Training average Loss: 0.660170\n",
      "Epoch 42_40.048%:  Training average Loss: 0.660693\n",
      "Epoch 42_40.849%:  Training average Loss: 0.660264\n",
      "Epoch 42_41.650%:  Training average Loss: 0.661188\n",
      "Epoch 42_42.451%:  Training average Loss: 0.660902\n",
      "Epoch 42_43.252%:  Training average Loss: 0.661042\n",
      "Epoch 42_44.053%:  Training average Loss: 0.661453\n",
      "Epoch 42_44.854%:  Training average Loss: 0.661558\n",
      "Epoch 42_45.655%:  Training average Loss: 0.661540\n",
      "Epoch 42_46.456%:  Training average Loss: 0.662762\n",
      "Epoch 42_47.257%:  Training average Loss: 0.663541\n",
      "Epoch 42_48.058%:  Training average Loss: 0.663095\n",
      "Epoch 42_48.859%:  Training average Loss: 0.663102\n",
      "Epoch 42_49.660%:  Training average Loss: 0.662684\n",
      "Epoch 42_50.461%:  Training average Loss: 0.663134\n",
      "Epoch 42_51.262%:  Training average Loss: 0.664378\n",
      "Epoch 42_52.063%:  Training average Loss: 0.664694\n",
      "Epoch 42_52.864%:  Training average Loss: 0.664857\n",
      "Epoch 42_53.665%:  Training average Loss: 0.665065\n",
      "Epoch 42_54.466%:  Training average Loss: 0.665532\n",
      "Epoch 42_55.267%:  Training average Loss: 0.665075\n",
      "Epoch 42_56.068%:  Training average Loss: 0.665829\n",
      "Epoch 42_56.869%:  Training average Loss: 0.665277\n",
      "Epoch 42_57.670%:  Training average Loss: 0.665352\n",
      "Epoch 42_58.471%:  Training average Loss: 0.665873\n",
      "Epoch 42_59.272%:  Training average Loss: 0.665900\n",
      "Epoch 42_60.073%:  Training average Loss: 0.666340\n",
      "Epoch 42_60.874%:  Training average Loss: 0.666208\n",
      "Epoch 42_61.675%:  Training average Loss: 0.665983\n",
      "Epoch 42_62.475%:  Training average Loss: 0.665913\n",
      "Epoch 42_63.276%:  Training average Loss: 0.666463\n",
      "Epoch 42_64.077%:  Training average Loss: 0.666494\n",
      "Epoch 42_64.878%:  Training average Loss: 0.666426\n",
      "Epoch 42_65.679%:  Training average Loss: 0.666782\n",
      "Epoch 42_66.480%:  Training average Loss: 0.667229\n",
      "Epoch 42_67.281%:  Training average Loss: 0.667400\n",
      "Epoch 42_68.082%:  Training average Loss: 0.667348\n",
      "Epoch 42_68.883%:  Training average Loss: 0.667242\n",
      "Epoch 42_69.684%:  Training average Loss: 0.667750\n",
      "Epoch 42_70.485%:  Training average Loss: 0.667855\n",
      "Epoch 42_71.286%:  Training average Loss: 0.667740\n",
      "Epoch 42_72.087%:  Training average Loss: 0.668126\n",
      "Epoch 42_72.888%:  Training average Loss: 0.668171\n",
      "Epoch 42_73.689%:  Training average Loss: 0.668459\n",
      "Epoch 42_74.490%:  Training average Loss: 0.668417\n",
      "Epoch 42_75.291%:  Training average Loss: 0.667923\n",
      "Epoch 42_76.092%:  Training average Loss: 0.668330\n",
      "Epoch 42_76.893%:  Training average Loss: 0.667931\n",
      "Epoch 42_77.694%:  Training average Loss: 0.667424\n",
      "Epoch 42_78.495%:  Training average Loss: 0.667648\n",
      "Epoch 42_79.296%:  Training average Loss: 0.667873\n",
      "Epoch 42_80.097%:  Training average Loss: 0.668016\n",
      "Epoch 42_80.898%:  Training average Loss: 0.668452\n",
      "Epoch 42_81.699%:  Training average Loss: 0.668034\n",
      "Epoch 42_82.500%:  Training average Loss: 0.668128\n",
      "Epoch 42_83.301%:  Training average Loss: 0.668499\n",
      "Epoch 42_84.102%:  Training average Loss: 0.668708\n",
      "Epoch 42_84.903%:  Training average Loss: 0.669189\n",
      "Epoch 42_85.704%:  Training average Loss: 0.669180\n",
      "Epoch 42_86.504%:  Training average Loss: 0.669369\n",
      "Epoch 42_87.305%:  Training average Loss: 0.669263\n",
      "Epoch 42_88.106%:  Training average Loss: 0.669085\n",
      "Epoch 42_88.907%:  Training average Loss: 0.669407\n",
      "Epoch 42_89.708%:  Training average Loss: 0.669177\n",
      "Epoch 42_90.509%:  Training average Loss: 0.669371\n",
      "Epoch 42_91.310%:  Training average Loss: 0.669254\n",
      "Epoch 42_92.111%:  Training average Loss: 0.669488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 29\u001B[0m\n\u001B[0;32m     25\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     27\u001B[0m correct \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mmax(out, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m1\u001B[39m]  \u001B[38;5;66;03m#get the indices\u001B[39;00m\n\u001B[0;32m     28\u001B[0m            \u001B[38;5;241m.\u001B[39mview(batch_label\u001B[38;5;241m.\u001B[39msize()) \u001B[38;5;241m==\u001B[39m batch_label)\u001B[38;5;241m.\u001B[39msum()\n\u001B[1;32m---> 29\u001B[0m total_correct \u001B[38;5;241m=\u001B[39m total_correct \u001B[38;5;241m+\u001B[39m \u001B[43mcorrect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m steps\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m%.3f\u001B[39;00m\u001B[38;5;132;01m%%\u001B[39;00m\u001B[38;5;124m:  Training average Loss: \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     33\u001B[0m               \u001B[38;5;241m%\u001B[39m(i, steps \u001B[38;5;241m*\u001B[39m train_iterator\u001B[38;5;241m.\u001B[39mbatch_size\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_iterator\u001B[38;5;241m.\u001B[39mdataset),total_loss\u001B[38;5;241m/\u001B[39msteps))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "best_accuracy=0.0\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    #训练\n",
    "    for batch in train_iterator:\n",
    "        steps+=1\n",
    "        #print(steps)\n",
    "        optimizer.zero_grad() #  梯度缓存清零\n",
    "\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)    #[batch_size, label_num]\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1]  #get the indices\n",
    "                   .view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        if steps%100==0:\n",
    "            print(\"Epoch %d_%.3f%%:  Training average Loss: %f\"\n",
    "                      %(i, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps))\n",
    "\n",
    "    #验证\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(test_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    for batch in test_iterator:\n",
    "        steps+=1\n",
    "        batch_text=batch.Phrase\n",
    "        batch_label=batch.Sentiment\n",
    "        out=model(batch_text)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "        print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "          %(i, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time))\n",
    "\n",
    "        if best_accuracy < total_correct/total_data_num :\n",
    "            best_accuracy =total_correct/total_data_num\n",
    "            torch.save(model,'model_dict/model_lstm/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            print('Model is saved in model_dict/model_lstm/epoch_%d_accuracy_%f'%(i,total_correct/total_data_num))\n",
    "            #torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}