{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "from Preprocessor import Preprocessor\n",
    "from Train import run_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# def preprocess(inputdir,\n",
    "#                embeddings_file,\n",
    "#                targetdir,\n",
    "#                lowercase=False,\n",
    "#                ignore_punctuation=False,\n",
    "#                num_words=None,\n",
    "#                stopwords=[],\n",
    "#                labeldict={},\n",
    "#                bos=None,\n",
    "#                eos=None):\n",
    "#     if not os.path.exists(targetdir): os.mkdir(targetdir)\n",
    "#\n",
    "#     #获取数据集\n",
    "#     train_file = \"\"\n",
    "#     dev_file = \"\"\n",
    "#     test_file = \"\"\n",
    "#     for file in os.listdir(inputdir):\n",
    "#         if fnmatch.fnmatch(file,\"*_train.txt\"):\n",
    "#             train_file = file\n",
    "#         elif fnmatch.fnmatch(file,\"*_dev.txt\"):\n",
    "#             dev_file = file\n",
    "#         elif fnmatch.fnmatch(file,\"*_test.txt\"):\n",
    "#             test_file = file\n",
    "#\n",
    "#     #数据预处理\n",
    "#     preprocessor = Preprocessor(lowercase=lowercase,\n",
    "#                                 ignore_punctuation=ignore_punctuation,\n",
    "#                                 num_words=num_words,\n",
    "#                                 stopwords=stopwords,\n",
    "#                                 labeldict=labeldict,\n",
    "#                                 bos=bos,\n",
    "#                                 eos=eos)\n",
    "#     print(\"=============训练数据预处理中=================\")\n",
    "#     print(\"读取数据\")\n",
    "#     data = preprocessor.read_data(os.path.join(inputdir,train_file))\n",
    "#     print(\"计算词向量\")\n",
    "#     preprocessor.build_worddict(data)\n",
    "#     with open(os.path.join(targetdir,\"worddict.pkl\"),\"wb\") as pkl_file:\n",
    "#         pickle.dump(preprocessor.worddict,pkl_file)\n",
    "#     print(\"词向量转换\")\n",
    "#     transformed_data = preprocessor.transform_to_indices(data)\n",
    "#     with open(os.path.join(targetdir,\"train_data.pkl\"),\"wb\") as pkl_file:\n",
    "#         pickle.dump(transformed_data,pkl_file)\n",
    "#\n",
    "#     print(\"============验证集预处理中===================\")\n",
    "#     print(\"读取数据\")\n",
    "#     data = preprocessor.read_data(os.path.join(inputdir,dev_file))\n",
    "#     print(\"词向量转换\")\n",
    "#     transformed_data = preprocessor.transform_to_indices(data)\n",
    "#     with open(os.path.join(targetdir,\"dev_data.pkl\"),\"wb\") as pkl_file:\n",
    "#         pickle.dump(transformed_data,pkl_file)\n",
    "#\n",
    "#     print(\"===========测试集预处理中=================\")\n",
    "#     print(\"读取数据\")\n",
    "#     data = preprocessor.read_data(os.path.join(inputdir,test_file))\n",
    "#     print(\"词向量转换\")\n",
    "#     transformed_data = preprocessor.transform_to_indices(data)\n",
    "#     with open(os.path.join(targetdir,\"test_data.pkl\"),\"wb\") as pkl_file:\n",
    "#         pickle.dump(transformed_data,pkl_file)\n",
    "#\n",
    "#     print(\"===========embedding预处理============\")\n",
    "#     embed_matrix = preprocessor.build_embedding_matrix(embeddings_file)\n",
    "#     with open(os.path.join(targetdir,\"embeddings.pkl\"),\"wb\") as pkl_file:\n",
    "#         pickle.dump(embed_matrix, pkl_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# #数据预处理\n",
    "# with open('preprocess.json','r') as f:\n",
    "#     pre_config = json.load(f)\n",
    "#\n",
    "# pre_args = argparse.Namespace(**pre_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# preprocess(inputdir=pre_args.data_dir,\n",
    "#            embeddings_file=pre_args.embeddings_file,\n",
    "#            targetdir=pre_args.target_dir,\n",
    "#            lowercase=pre_args.lowercase,\n",
    "#            num_words=pre_args.num_words,\n",
    "#            stopwords=pre_args.stopwords,\n",
    "#            labeldict=pre_args.labeldict,\n",
    "#            bos=pre_args.bos,\n",
    "#            eos=pre_args.eos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#数据训练\n",
    "with open(\"./train.json\",\"r\") as f:\n",
    "    train_config = json.load(f)\n",
    "\n",
    "train_args = argparse.Namespace(**train_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================  Preparing for training  ====================\n",
      "\t* 加载训练数据...\n",
      "\t* 加载验证集...\n",
      "\t* 构建模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Codes\\python\\nlpbeginner\\task3-text-matching\\Utils.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* Validation loss before training: 1.1031, accuracy: 32.8592%\n",
      "\n",
      " ==================== Training ESIM model on device: cuda:0 ====================\n",
      "* Training epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.0600s, loss: 0.6876: 100%|██████████| 17168/17168 [19:23<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Training time: 1163.4441s, loss = 0.6876, accuracy: 70.6863%\n",
      "* Validation for epoch 1:\n",
      "-> Valid. time: 4.8840s, loss: 0.4624, accuracy: 82.2292%\n",
      "\n",
      "* Training epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. batch proc. time: 0.0594s, loss: 0.5396:  31%|███       | 5330/17168 [05:57<13:14, 14.91it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mrun_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m          \u001B[49m\u001B[43mvalid_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalid_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m          \u001B[49m\u001B[43membeddings_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m          \u001B[49m\u001B[43mtarget_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m          \u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m          \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m          \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m          \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m          \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatience\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_gradient_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m          \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Codes\\python\\nlpbeginner\\task3-text-matching\\Train.py:190\u001B[0m, in \u001B[0;36mrun_train\u001B[1;34m(train_file, valid_file, embeddings_file, target_dir, hidden_size, dropout, num_classes, epochs, batch_size, lr, patience, max_grad_norm, checkpoint)\u001B[0m\n\u001B[0;32m    187\u001B[0m epochs_count\u001B[38;5;241m.\u001B[39mappend(epoch)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m* Training epoch \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch))\n\u001B[1;32m--> 190\u001B[0m epoch_time, epoch_loss, epoch_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    197\u001B[0m train_losses\u001B[38;5;241m.\u001B[39mappend(epoch_loss)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-> Training time: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124ms, loss = \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m, accuracy: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    199\u001B[0m       \u001B[38;5;241m.\u001B[39mformat(epoch_time, epoch_loss, (epoch_accuracy\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m)))\n",
      "File \u001B[1;32mD:\\Codes\\python\\nlpbeginner\\task3-text-matching\\Train.py:41\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, optimizer, criterion, epoch_number, max_gradient_norm)\u001B[0m\n\u001B[0;32m     36\u001B[0m logits, probs \u001B[38;5;241m=\u001B[39m model(premises,\n\u001B[0;32m     37\u001B[0m                       premises_lengths,\n\u001B[0;32m     38\u001B[0m                       hypotheses,\n\u001B[0;32m     39\u001B[0m                       hypotheses_lengths)\n\u001B[0;32m     40\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(logits, labels)\n\u001B[1;32m---> 41\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m nn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_gradient_norm)\n\u001B[0;32m     44\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\torch102\\lib\\site-packages\\torch\\tensor.py:198\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackward\u001B[39m(\u001B[38;5;28mself\u001B[39m, gradient\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, create_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    171\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \n\u001B[0;32m    173\u001B[0m \u001B[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;124;03m            products. Defaults to ``False``.\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 198\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\torch102\\lib\\site-packages\\torch\\autograd\\__init__.py:98\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     96\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m---> 98\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (_cudnn_rnn_backward_input at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:931)\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "run_train(train_file=train_args.train_data,\n",
    "          valid_file=train_args.valid_data,\n",
    "          embeddings_file=train_args.embeddings,\n",
    "          target_dir=train_args.target_dir,\n",
    "          hidden_size=train_args.hidden_size,\n",
    "          dropout=train_args.dropout,\n",
    "          num_classes=train_args.num_classes,\n",
    "          epochs=train_args.epochs,\n",
    "          batch_size=train_args.batch_size,\n",
    "          lr=train_args.lr,\n",
    "          patience=train_args.patience,\n",
    "          max_grad_norm=train_args.max_gradient_norm,\n",
    "          )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}